{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMLS II",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBv6K6yRzzFmMsx67rkhI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donyeun/AMLSII_19-20_SN18154195/blob/master/AMLS_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skg_yPYJE3V_",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTTXAniWmfMi",
        "colab_type": "code",
        "outputId": "93fe2654-5dcf-41de-efc8-c3e42096c307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe4VG6osyCiF",
        "colab_type": "text"
      },
      "source": [
        "# Libraries and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5LCvzjssSJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HZ83t_QZq1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = {\n",
        "       'paths': {\n",
        "           'train_folder_task_a': '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/dataset/training/2017_English_final/GOLD/Subtask_A',\n",
        "           'train_additional_dataset_filenames': ['livejournal-2014test-A.tsv', 'sms-2013test-A.tsv'],\n",
        "           'test_file_task_a': '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/dataset/testing/SemEval2017-task4-test/SemEval2017-task4-test.subtask-A.english.txt',\n",
        "\n",
        "           'train_folder_task_b': '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/dataset/training/2017_English_final/GOLD/Subtasks_BD',\n",
        "           'test_file_task_b': '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/dataset/testing/SemEval2017-task4-test/SemEval2017-task4-test.subtask-BD.english.txt',\n",
        "       },\n",
        "       'task_a': {\n",
        "           'use_additional_dataset': False,\n",
        "           'dataset': {\n",
        "               'clean' : {\n",
        "                   'training_filepath': '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/dataset/clean/task_a_training.csv'\n",
        "               }\n",
        "           }\n",
        "       },\n",
        "       'preprocessing': {\n",
        "           'tokenization': {\n",
        "               'remove_twitter_handle': True,\n",
        "               'preserve_case': False\n",
        "           }\n",
        "       },\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuEK2LfkrPYV",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset and Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EVaQRfoFLyT",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess Additional Dataset\n",
        "In task A, there are some additional datasets that were given from the competition apart from the standard twitter corpus, which is sms and livejournal corpus. We can use both of these additional datasets, after we preprocess the formatting so that it matches the rest of the twitter datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUAJontBFp21",
        "colab_type": "code",
        "outputId": "26d3410e-19ff-4311-c34d-83230ddd1233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "task_a_train_additional_dfs = {}\n",
        "\n",
        "for filename in cfg['paths']['train_additional_dataset_filenames']:\n",
        "  # read additional corpora\n",
        "  task_a_train_additional_dfs[filename] = pd.read_csv(os.path.join(cfg['paths']['train_folder_task_a'], filename), sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "  \n",
        "  if filename == 'livejournal-2014test-A.tsv':\n",
        "    # remove the 0-th column from livejournal corpora, as it is unnecessary\n",
        "    task_a_train_additional_dfs[filename] = task_a_train_additional_dfs[filename].drop(columns=[0])\n",
        "  elif filename == 'sms-2013test-A.tsv':\n",
        "    # remove the 1st column from sms corpora, as it is unnecessary\n",
        "    task_a_train_additional_dfs[filename] = task_a_train_additional_dfs[filename].drop(columns=[1])\n",
        "  \n",
        "  # reset the column index to make it incremental\n",
        "  task_a_train_additional_dfs[filename].columns = range(task_a_train_additional_dfs[filename].shape[1])\n",
        "  \n",
        "  print(task_a_train_additional_dfs[filename])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             0         1                                                  2\n",
            "0     LJ111111  negative  I know I missed something here , but what does...\n",
            "1     LJ111113   neutral  What do you think of Beside Ourselves as a tit...\n",
            "2     LJ111114  positive                    :D I intend to be one someday .\n",
            "3     LJ111117  negative  LLLINKKK LLLINKKK IIIMAGEEELLLINKKK The choice...\n",
            "4     LJ111119   neutral                     LLLINKKK Some more mountains .\n",
            "...        ...       ...                                                ...\n",
            "1137  LJ113616  positive                     Maybe it was - his - fantasy ?\n",
            "1138  LJ113618  negative  It was ok , but they always just seem so nervo...\n",
            "1139  LJ113621  positive  It is streamable from YepRoc -- matter of fact...\n",
            "1140  LJ113623  positive  comment telling me who you are , or how you fo...\n",
            "1141  LJ113625   neutral  im on myspace ... ill try and find you and add...\n",
            "\n",
            "[1142 rows x 3 columns]\n",
            "          0         1                                                  2\n",
            "0     10936   neutral  Yes i am going from school have class till 5 c...\n",
            "1     11051   neutral  can u tape the match for me?  i\\u2019ll rush o...\n",
            "2     10966   neutral  Too many people at my house my relatives are h...\n",
            "3     11211  negative  Yea I have spoken to him liao. Indeed he is up...\n",
            "4     11350  positive  Haha... I want to see. E macdonalds here cheap...\n",
            "...     ...       ...                                                ...\n",
            "2089  10038  negative  Oki... Think i\\u2019m confused... I only know ...\n",
            "2090  11799   neutral  Yup... Ok i go home look at the timings then i...\n",
            "2091  11945   neutral            Here got lots of hair dresser fr china.\n",
            "2092  10154   neutral  no alh  we are not discussing fromt he viewpoi...\n",
            "2093  11428   neutral  Not dat i dun wan sign up but i wan only for a...\n",
            "\n",
            "[2094 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH283206E6BF",
        "colab_type": "text"
      },
      "source": [
        "## Make Datasets As Pandas' DataFrames and Cleaning Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgC4YYhHrtdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_txt_files_as_one_dataframe(folderpath, filename_keywords_list, additional_dataset_dfs=None):\n",
        "  dataset_per_file_dfs = {}\n",
        "  dataset_df = pd.DataFrame()\n",
        "  filenames = os.listdir(folderpath)\n",
        "  \n",
        "  # open txt files (in tsv formatting)\n",
        "  for filename in filenames:\n",
        "    # if the filename contains a keyword in the filename_keywords_list, then open the txt file\n",
        "    # this is to avoid opening unnecessary txt such as readme.txt file.\n",
        "    if any(keyword in filename for keyword in filename_keywords_list):\n",
        "      dataset_per_file_dfs[filename] = pd.read_csv(os.path.join(folderpath, filename), sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
        "      print(dataset_per_file_dfs[filename].shape, filename)\n",
        "\n",
        "  # combine the original dataset with additional dataset (if any)\n",
        "  if additional_dataset_dfs is not None:\n",
        "    for key, value in additional_dataset_dfs.items():\n",
        "      dataset_per_file_dfs[key] = value\n",
        "\n",
        "  # append all the files as one dataframe\n",
        "  for key, value in dataset_per_file_dfs.items():\n",
        "    # print(key, '\\t', i, '\\t', dataset_per_file_df[key].shape[0])\n",
        "    dataset_df = dataset_df.append(dataset_per_file_dfs[key], ignore_index=True)\n",
        "  return dataset_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXBTf96GQeBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_dataframe_format(df, new_column_name_list, drop_column_list=[]):\n",
        "  # drop unnecessary column\n",
        "  df = df.drop(columns=drop_column_list)\n",
        "  \n",
        "  # rename column\n",
        "  df.columns = new_column_name_list\n",
        "\n",
        "  # remove row in dataframe if the 'text' or 'sentiment' column value is missing\n",
        "  df = df.dropna(subset=['sentiment', 'text'], how='any').reset_index(drop=True)\n",
        "  \n",
        "  # remove row if the sentiment is not 'positive', 'negative' or 'neutral'\n",
        "  # this happens in the dataset, for example, there are some rows\n",
        "  # where its sentiments are 'off topic'\n",
        "  valid_sentiments = ['positive', 'negative', 'neutral']\n",
        "  df = df[df['sentiment'].isin(valid_sentiments)].reset_index(drop=True)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znGSEFNntmVa",
        "colab_type": "code",
        "outputId": "99d714f6-abc3-4ff3-fac8-457873057ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# read dataset files and append it as one pandas dataframe\n",
        "if cfg['task_a']['use_additional_dataset']:\n",
        "  task_a_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_a'], ['twitter'], task_a_train_additional_dfs)\n",
        "else:\n",
        "  task_a_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_a'], ['twitter'])\n",
        "\n",
        "task_a_train_df = clean_dataframe_format(task_a_train_df, ['id', 'sentiment', 'text'], drop_column_list=[3])\n",
        "\n",
        "# # save dataset as csv file\n",
        "# with pd.option_context('display.max_columns', None):  # more options can be specified also\n",
        "#     print(task_a_train_df)\n",
        "\n",
        "# print(task_a_train_df.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 3) twitter-2016devtest-A.txt\n",
            "(1999, 3) twitter-2016dev-A.txt\n",
            "(6000, 3) twitter-2016train-A.txt\n",
            "(1654, 3) twitter-2013dev-A.txt\n",
            "(3547, 3) twitter-2013test-A.txt\n",
            "(9684, 3) twitter-2013train-A.txt\n",
            "(1853, 3) twitter-2014test-A.txt\n",
            "(2390, 3) twitter-2015test-A.txt\n",
            "(489, 3) twitter-2015train-A.txt\n",
            "(86, 3) twitter-2014sarcasm-A.txt\n",
            "(20633, 4) twitter-2016test-A.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Aem1fwuZB8",
        "colab_type": "code",
        "outputId": "75e9fd28-9ab7-4227-e679-18f4734d13d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "task_b_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_b'], ['twitter'])\n",
        "task_b_train_df = clean_dataframe_format(task_b_train_df, ['id', 'topic','sentiment', 'text'], drop_column_list=[4])\n",
        "task_b_train_df.info()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4346, 4) twitter-2016train-BD.txt\n",
            "(1325, 4) twitter-2016dev-BD.txt\n",
            "(1417, 4) twitter-2016devtest-BD.txt\n",
            "(489, 4) twitter-2015train-BD.txt\n",
            "(10552, 5) twitter-2016test-BD.txt\n",
            "(2383, 5) twitter-2015testBD.txt\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20508 entries, 0 to 20507\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         20508 non-null  object\n",
            " 1   topic      20508 non-null  object\n",
            " 2   sentiment  20508 non-null  object\n",
            " 3   text       20508 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 641.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5QJj-YTXg5Z",
        "colab_type": "code",
        "outputId": "cfad788c-124c-4fd0-b714-540b324ff6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(task_b_train_df[\n",
        "                (task_b_train_df['sentiment'] != 'negative') &\n",
        "                (task_b_train_df['sentiment'] != 'positive') &\n",
        "                (task_b_train_df['sentiment'] != 'neutral')\n",
        "                ]['sentiment'])\n",
        "\n",
        "# task_b_train_df\n",
        "\n",
        "# task_b_train_df[task_b_train_df['text'].str.len()< 40]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Series([], Name: sentiment, dtype: object)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iApuaK7wj9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "10fbbf20-a69c-464a-dc13-8a8429edf2f0"
      },
      "source": [
        "task_a_test_df = pd.read_csv(cfg['paths']['test_file_task_a'], sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "task_a_test_df = clean_dataframe_format(task_a_test_df, ['id', 'sentiment', 'text'])\n",
        "task_a_test_df"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>801989080477154944</td>\n",
              "      <td>neutral</td>\n",
              "      <td>#ArianaGrande Ari By Ariana Grande 80% Full ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>801989272341453952</td>\n",
              "      <td>positive</td>\n",
              "      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>801990978424962944</td>\n",
              "      <td>positive</td>\n",
              "      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>801996232553963008</td>\n",
              "      <td>positive</td>\n",
              "      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>801998343442407040</td>\n",
              "      <td>neutral</td>\n",
              "      <td>SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12279</th>\n",
              "      <td>805699615781625856</td>\n",
              "      <td>positive</td>\n",
              "      <td>@dansen17 update: Zac Efron kissing a puppy ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12280</th>\n",
              "      <td>805701709356003328</td>\n",
              "      <td>neutral</td>\n",
              "      <td>#zac efron sex pic skins michelle sex https://...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12281</th>\n",
              "      <td>805701818357579776</td>\n",
              "      <td>neutral</td>\n",
              "      <td>First Look at Neighbors 2 with Zac Efron Shirt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12282</th>\n",
              "      <td>805703557081075712</td>\n",
              "      <td>neutral</td>\n",
              "      <td>zac efron poses nude #lovely libra porn https:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12283</th>\n",
              "      <td>805704324105940992</td>\n",
              "      <td>neutral</td>\n",
              "      <td>#Fashion #Style The Paperboy (NEW Blu-ray Disc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12284 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ...                                               text\n",
              "0      801989080477154944  ...  #ArianaGrande Ari By Ariana Grande 80% Full ht...\n",
              "1      801989272341453952  ...  Ariana Grande KIIS FM Yours Truly CD listening...\n",
              "2      801990978424962944  ...  Ariana Grande White House Easter Egg Roll in W...\n",
              "3      801996232553963008  ...  #CD #Musics Ariana Grande Sweet Like Candy 3.4...\n",
              "4      801998343442407040  ...  SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...\n",
              "...                   ...  ...                                                ...\n",
              "12279  805699615781625856  ...  @dansen17 update: Zac Efron kissing a puppy ht...\n",
              "12280  805701709356003328  ...  #zac efron sex pic skins michelle sex https://...\n",
              "12281  805701818357579776  ...  First Look at Neighbors 2 with Zac Efron Shirt...\n",
              "12282  805703557081075712  ...  zac efron poses nude #lovely libra porn https:...\n",
              "12283  805704324105940992  ...  #Fashion #Style The Paperboy (NEW Blu-ray Disc...\n",
              "\n",
              "[12284 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqE-Y6XJBeA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "859d74e9-3703-41fd-fc71-78a12f9889d6"
      },
      "source": [
        "task_b_test_df = pd.read_csv(cfg['paths']['test_file_task_b'], sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "task_b_test_df = clean_dataframe_format(task_b_test_df, ['id', 'topic', 'sentiment', 'text'])\n",
        "task_b_test_df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>topic</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>801989272341453952</td>\n",
              "      <td>#ArianaGrande</td>\n",
              "      <td>positive</td>\n",
              "      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>801990978424962944</td>\n",
              "      <td>#ArianaGrande</td>\n",
              "      <td>positive</td>\n",
              "      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>801996232553963008</td>\n",
              "      <td>#ArianaGrande</td>\n",
              "      <td>positive</td>\n",
              "      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>801998343442407040</td>\n",
              "      <td>#ArianaGrande</td>\n",
              "      <td>positive</td>\n",
              "      <td>SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>802001659970744064</td>\n",
              "      <td>#ArianaGrande</td>\n",
              "      <td>positive</td>\n",
              "      <td>Hairspray Live! Previews at the Macy's Thanksg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6180</th>\n",
              "      <td>805696468959002624</td>\n",
              "      <td>zac efron</td>\n",
              "      <td>positive</td>\n",
              "      <td>Abby *talking about the Hamilton soundtrack\": ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6181</th>\n",
              "      <td>805699412257181697</td>\n",
              "      <td>zac efron</td>\n",
              "      <td>positive</td>\n",
              "      <td>can we like get zac efron or justin bieber for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6182</th>\n",
              "      <td>805699615781625856</td>\n",
              "      <td>zac efron</td>\n",
              "      <td>positive</td>\n",
              "      <td>@dansen17 update: Zac Efron kissing a puppy ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6183</th>\n",
              "      <td>805701818357579776</td>\n",
              "      <td>zac efron</td>\n",
              "      <td>positive</td>\n",
              "      <td>First Look at Neighbors 2 with Zac Efron Shirt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6184</th>\n",
              "      <td>805703557081075712</td>\n",
              "      <td>zac efron</td>\n",
              "      <td>positive</td>\n",
              "      <td>zac efron poses nude #lovely libra porn https:...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6185 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ...                                               text\n",
              "0     801989272341453952  ...  Ariana Grande KIIS FM Yours Truly CD listening...\n",
              "1     801990978424962944  ...  Ariana Grande White House Easter Egg Roll in W...\n",
              "2     801996232553963008  ...  #CD #Musics Ariana Grande Sweet Like Candy 3.4...\n",
              "3     801998343442407040  ...  SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...\n",
              "4     802001659970744064  ...  Hairspray Live! Previews at the Macy's Thanksg...\n",
              "...                  ...  ...                                                ...\n",
              "6180  805696468959002624  ...  Abby *talking about the Hamilton soundtrack\": ...\n",
              "6181  805699412257181697  ...  can we like get zac efron or justin bieber for...\n",
              "6182  805699615781625856  ...  @dansen17 update: Zac Efron kissing a puppy ht...\n",
              "6183  805701818357579776  ...  First Look at Neighbors 2 with Zac Efron Shirt...\n",
              "6184  805703557081075712  ...  zac efron poses nude #lovely libra porn https:...\n",
              "\n",
              "[6185 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OirqNXwXfQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking and compare the frequency with what's written in the paper\n",
        "# # save dataset as csv file\n",
        "# task_a_train_df.to_csv('coba_train_a.csv', sep='\\t')\n",
        "\n",
        "# assert task_a_train_df[task_a_train_df['sentiment'] == 'positive'].shape[0] == 19902  #SALAH\n",
        "assert task_a_train_df[task_a_train_df['sentiment'] == 'negative'].shape[0] == 7840\n",
        "assert task_a_train_df[task_a_train_df['sentiment'] == 'neutral'].shape[0] == 22591\n",
        "\n",
        "assert task_a_test_df[task_a_test_df['sentiment'] == 'positive'].shape[0] == 2375\n",
        "assert task_a_test_df[task_a_test_df['sentiment'] == 'negative'].shape[0] == 3972\n",
        "assert task_a_test_df[task_a_test_df['sentiment'] == 'neutral'].shape[0] == 5937\n",
        "\n",
        "assert len(task_b_train_df['topic'].unique()) == 373\n",
        "assert task_b_train_df[task_b_train_df['sentiment'] == 'positive'].shape[0] == 14951\n",
        "assert task_b_train_df[task_b_train_df['sentiment'] == 'negative'].shape[0] == 4013\n",
        "assert task_b_train_df[task_b_train_df['sentiment'] == 'neutral'].shape[0] == 1544\n",
        "\n",
        "\n",
        "assert len(task_b_test_df['topic'].unique()) == 125\n",
        "assert task_b_test_df[task_b_test_df['sentiment'] == 'positive'].shape[0] == 2463\n",
        "assert task_b_test_df[task_b_test_df['sentiment'] == 'negative'].shape[0] == 3722"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KGklVjRx50N",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwjXcI3Gx7xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_df(df, tokenizer, input_column_name='text', output_column_name='tokenized'):\n",
        "  df[output_column_name] = df.apply(\n",
        "    lambda row: tokenize_text(\n",
        "        row[input_column_name],\n",
        "        tokenizer\n",
        "    ),\n",
        "    axis=1)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paxv3Xsw0fkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "feb8456b-2aa1-4f64-cf7f-f7388cb67d1e"
      },
      "source": [
        "tokenizer = nltk.tokenize.TweetTokenizer(\n",
        "    strip_handles = cfg['preprocessing']['tokenization']['remove_twitter_handle'],\n",
        "    preserve_case = cfg['preprocessing']['tokenization']['preserve_case']\n",
        ")\n",
        "\n",
        "task_a_train_df = tokenize_df(task_a_train_df, tokenizer)\n",
        "task_b_train_df = tokenize_df(task_b_train_df, tokenizer)\n",
        "task_a_test_df = tokenize_df(task_a_test_df, tokenizer)\n",
        "task_b_test_df = tokenize_df(task_b_test_df, tokenizer)\n",
        "\n",
        "\n",
        "# print sample result\n",
        "task_b_test_df.head(10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2712ebb24db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtask_a_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_a_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtask_b_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_b_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtask_a_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_a_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9af6332a04ae>\u001b[0m in \u001b[0;36mtokenize_df\u001b[0;34m(df, tokenizer, input_column_name, output_column_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     ),\n\u001b[0;32m----> 7\u001b[0;31m     axis=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-9af6332a04ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_column_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_column_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   df[output_column_name] = df.apply(\n\u001b[0;32m----> 3\u001b[0;31m     lambda row: tokenize_text(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFnxpFTZEe7E",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUrnPEtN76nc",
        "colab_type": "text"
      },
      "source": [
        "## Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXTNP_1975_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_wordcloud(text, title, max_words=200):\n",
        "  wc = WordCloud(\n",
        "      max_words=max_words,\n",
        "      background_color='white',\n",
        "      # ranks_only= frequency\n",
        "  ).generate(text)\n",
        "\n",
        "  print(wc.words_)\n",
        "\n",
        "  fig = plt.figure(1, figsize=(12, 12))\n",
        "  fig.suptitle(title)\n",
        "  plt.imshow(wc)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MWVlPVeEit3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = task_b_train_df[task_b_train_df['sentiment'] == 'positive']['text'][4]\n",
        "print(df)\n",
        "# print(df.shape)\n",
        "# show_wordcloud(str(df.values), 'judul', max_words=20000)\n",
        "\n",
        "# print(df.shape)\n",
        "word_string = \"you verse wrote book stand titlea i you you you kampung\"\n",
        "print(type(word_string))\n",
        "show_wordcloud(word_string, 'judul', max_words=20000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGMBtM98uTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_a_train_df['sentiment'].value_counts().plot(kind='bar',figsize=(7,4));\n",
        "plt.title('Number of tweets');\n",
        "plt.xlabel('Sentiment');\n",
        "plt.ylabel('Total tweets');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7YGmqNd2WrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5oxkBMoH5hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt4arEBt4ntG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gauu8B_i9XHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_b_train_df[task_b_train_df['text'].str.contains('not')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWrBCXtSH4P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwFx7A43Aju4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = task_b_train_df[task_b_train_df['sentiment'] == 'positive']['topic'].value_counts(sort=False).to_frame(name='positive')\n",
        "negative = task_b_train_df[task_b_train_df['sentiment'] == 'negative']['topic'].value_counts(sort=False).to_frame(name='negative')\n",
        "neutral = task_b_train_df[task_b_train_df['sentiment'] == 'neutral']['topic'].value_counts(sort=False).to_frame(name='neutral')\n",
        "# negative = task_b_train_df['topic'][task_b_train_df['sentiment'] == 'negative'].value_counts(sort=False)\n",
        "# neutral = task_b_train_df['topic'][task_b_train_df['sentiment'] == 'neutral'].value_counts(sort=False)\n",
        "joined = positive.join(negative).join(neutral)\n",
        "\n",
        "joined = joined[(joined['positive'] > 0) & (joined['negative']>0)]\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    print(joined)\n",
        "\n",
        "plt.bar(range(joined.shape[0]), joined['positive'], label='positive', color='blue')\n",
        "plt.bar(range(joined.shape[0]), joined['negative'], label='negative', color='red', bottom=joined['positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm_M0CXUAsAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save dataset as csv file\n",
        "# task_a_train_df.to_csv('coba_train_a', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFgYcYx08ULD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nS8VAw9h6ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwJSV9cfh9Q6",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KvskijQ9H9",
        "colab_type": "text"
      },
      "source": [
        "## Build Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP7SH78Hh_Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import csv\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, dataset_df):\n",
        "    self.samples = []\n",
        "\n",
        "    for idx, row in dataset_df.iterrows():\n",
        "      if idx == 101:\n",
        "        break\n",
        "      self.samples.append( (row['text'], row['sentiment']) )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text, sentiment = self.samples[idx]\n",
        "    return text, sentiment\n",
        "\n",
        "  \n",
        "\n",
        "task_a_trainset = TextDataset(task_a_train_df)\n",
        "dataloader = DataLoader(\n",
        "                task_a_trainset,\n",
        "                batch_size=50,\n",
        "                num_workers=2\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDdUbMpWPSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i, batch in enumerate(dataloader):\n",
        "#   for sample in batch:\n",
        "#     print(batch)\n",
        "#   print(\">>>>>>>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpIHw_3YdSAn",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TsmpHiydeyr",
        "colab_type": "text"
      },
      "source": [
        "three layers:\n",
        "1. embedding layer (transform one-hot encoding vector into a dense embedding vector)\n",
        "2. RNN\n",
        "3. linear layer (output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_SPYGZdWuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "    self.fully_connected = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, text):\n",
        "    # text dimention is [sentence len, batch size]\n",
        "    \n",
        "    # embedded dimention is [sentence len, batch size, embedding_dim]\n",
        "    embedded = self.embedding(text)\n",
        "\n",
        "    # output dimention is [sentence len, batch size, hidden dim]\n",
        "    # hidden dimension is [1, batch size, hidden dim]\n",
        "    output, hidden = self.rnn(embedded)\n",
        "\n",
        "    assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "    \n",
        "    return self.fully_connected(hidden.squeze(0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP20cupMd1v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}