{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_model","provenance":[{"file_id":"1v1-LOhQs98ZpD3ZxgDhdEgHW_FYX4XP-","timestamp":1588573604119},{"file_id":"https://github.com/donyeun/AMLSII_19-20_SN18154195/blob/master/AMLS_II.ipynb","timestamp":1587959115442}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyMcGrXbELoCgeWcRt88HI1q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sb8ChAixgkvE","colab_type":"text"},"source":["# **SemEval 2017 Task 4 - Training Phase**\n","---\n","\n","\n","This is the second piece of code within the series of SemEval 2017 Task 4 challenge.\n","\n","In this notebook, we will perform hyperparameter tuning in order to obtain the best model architecture."]},{"cell_type":"code","metadata":{"id":"oTTXAniWmfMi","colab_type":"code","outputId":"354910be-173c-45bb-d194-a4a5bdfd86fc","executionInfo":{"status":"ok","timestamp":1588574080490,"user_tz":-60,"elapsed":1862,"user":{"displayName":"Dony Arisandy Wiranata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giy4RjCBJgU9A8zhT2B-AlcY_LP7-W-c59IBcPEoGQ=s64","userId":"05975001259031186418"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pe4VG6osyCiF","colab_type":"text"},"source":["# Libraries and Variables\n","\n","* Reading `config.yaml` which contains all ML parameters as well as filepaths\n","* Import all dependencies and libraries\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SkxFAcT8itpN","colab_type":"code","colab":{}},"source":["# load configuration file that store all the constant and parameters settings\n","import yaml\n","CONFIG_YAML_FILEPATH = '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/config.yaml'\n","with open(CONFIG_YAML_FILEPATH, 'r') as file:\n","  cfg = yaml.safe_load(file)\n","\n","TASK_NUMBER = cfg['task_number']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5LCvzjssSJf","colab_type":"code","outputId":"4ab59c4f-e2ea-4f72-9bdc-5901f3e5e115","executionInfo":{"status":"ok","timestamp":1588574084179,"user_tz":-60,"elapsed":5523,"user":{"displayName":"Dony Arisandy Wiranata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giy4RjCBJgU9A8zhT2B-AlcY_LP7-W-c59IBcPEoGQ=s64","userId":"05975001259031186418"}},"colab":{"base_uri":"https://localhost:8080/","height":955}},"source":["# install additional dependencies\n","! pip install -r {cfg['paths']['requirements']}\n","\n","import pandas as pd\n","import os\n","import torch\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import nltk\n","import csv\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","from torchtext.data import TabularDataset, Field, LabelField, BucketIterator\n","from sklearn.preprocessing import LabelEncoder\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","from ekphrasis.classes.preprocessor import TextPreProcessor"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.2.0rc3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (1.0.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.2.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (3.2.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 5)) (4.38.0)\n","Requirement already satisfied: ekphrasis in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (0.5.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 7)) (3.2.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 8)) (3.13)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 9)) (1.5.0+cu101)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 10)) (1.5.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.12.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.10.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.6.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.28.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.34.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.9.0)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.18.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.12.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (2.8.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.6.0.post3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.2.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (46.1.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.7.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (2.4.7)\n","Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from ekphrasis->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (2.0.3)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ekphrasis->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (0.4.3)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from ekphrasis->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (5.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 9)) (0.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 10)) (7.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.1.1)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (0.1.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jZwbZbXbVweZ","colab_type":"text"},"source":["## Task Logistics"]},{"cell_type":"code","metadata":{"id":"uyq_2EdrVNYL","colab_type":"code","colab":{}},"source":["# GENERAL TRAIN-TEST VARS\n","if TASK_NUMBER == 'A':\n","  train_csv_filepath = cfg['paths']['cleaned_train_a']\n","  test_csv_filepath = cfg['paths']['cleaned_test_a']\n","  saved_model_filepath = cfg['paths']['task_a_model']\n","elif TASK_NUMBER == 'B':\n","  train_csv_filepath = cfg['paths']['cleaned_train_b']\n","  test_csv_filepath = cfg['paths']['cleaned_test_b']\n","  saved_model_filepath = cfg['paths']['task_b_model']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwJSV9cfh9Q6","colab_type":"text"},"source":["# List of Hyperparameter Trials"]},{"cell_type":"code","metadata":{"id":"icXEiVRXVXW6","colab_type":"code","colab":{}},"source":["params = {\n","    'vocab': {\n","        'batch_size' : [32, 64],\n","\t      'max_vocab_size' : [50000, 100000],\n","  \t    'pretrained_embedding': [{\n","            'url' : 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip',\n","            'filepath': './crawl-300d-2M.vec',\n","            'embedding_dim': 300,\n","        },{\n","            'url' : 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n","            'filepath': './glove.twitter.27B.200d.txt',\n","            'embedding_dim' : 200,\n","        },],\n","        'preprocessing' : [{\n","            'normalize' : ['url', 'email', 'percent', 'money', 'phone', 'user',\n","                'time', 'date', 'number'],\n","            'annotate' : ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'],\n","            'spell_correct_elong' : True,\n","            'to_lowercase': True,    \n","        },\n","        ]\n","    },\n","    'nn': { \n","          'hidden_dim': [128, 256],\n","          'n_layers': [2, 3],\n","          'is_bidirectional': [True],\n","          'dropout': [0.5, 0.8],\n","          'n_epochs': [20],\n","    }\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0KvskijQ9H9","colab_type":"text"},"source":["## Build Torch Dataset\n"]},{"cell_type":"code","metadata":{"id":"h-_FIEeLyTFM","colab_type":"code","colab":{}},"source":["# read csv file as torchtext's TabularDataset\n","def csv_to_tabular_dataset(filepath, fields):\n","  tabular_daset = TabularDataset(\n","      path=filepath,\n","      fields = fields,\n","      format='tsv',\n","      skip_header=True\n","  )\n","  return tabular_daset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpIHw_3YdSAn","colab_type":"text"},"source":["## RNN-LSTM"]},{"cell_type":"markdown","metadata":{"id":"2TsmpHiydeyr","colab_type":"text"},"source":["three layers:\n","1. embedding layer (transform one-hot encoding vector into a dense embedding vector)\n","2. RNN\n","3. linear layer (output)"]},{"cell_type":"code","metadata":{"id":"Xo_SPYGZdWuq","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import time\n","import requests, zipfile, io\n","from pathlib import Path\n","from torchtext.vocab import Vectors\n","import torch.optim as optim\n","import dill\n","\n","class RNN(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n","               bidirectional, dropout, pad_idx):\n","    super().__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","    # self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","    self.rnn = nn.LSTM(embedding_dim,\n","                       hidden_dim,\n","                       num_layers=n_layers,\n","                       bidirectional=bidirectional,\n","                       dropout=dropout,\n","                       )\n","    self.fully_connected = nn.Linear(hidden_dim*2, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, text, text_length):\n","    # text dimention is [sentence len, batch size]\n","    \n","    # embedded dimention is [sentence len, batch size, embedding_dim]\n","    embedded = self.dropout(self.embedding(text))\n","\n","    # pack the sequence\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n","    # print('packed_embedded:')\n","    # print(packed_embedded)\n","    packet_output, (hidden, cell) = self.rnn(packed_embedded)\n","\n","    # unpack the sequence\n","    output, output_length = nn.utils.rnn.pad_packed_sequence(packet_output)\n","\n","    # output dimention is [sentence len, batch size, hidden dim * num directions]\n","    # output over padding tokens are zero tensors\n","\n","    # hidden dimension is [num layers*num directions, batch size, hidden dim]\n","    # cell dimension is also [num layers*num directions, batch size, hidden dim]\n","\n","    #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","    #and apply dropout\n","\n","    # hidden dimension is [batch size, hid dim*num directions]\n","    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n","    \n","    return self.fully_connected(hidden)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWbAXYOsr68C","colab_type":"code","colab":{}},"source":["def count_model_params(model):\n","  # number of parameters that are trainable\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5cEu9fAwixJ","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n","def calculate_performance(y, preds):\n","  # get the index of the max probability \n","  Y_pred = preds.argmax(dim = 1, keepdim = True).squeeze(1)\n","  Y_pred = Y_pred.detach().cpu().clone().numpy()\n","  \n","  Y_true = y.detach().cpu().clone().numpy()\n","\n","  acc = accuracy_score(Y_true, Y_pred)\n","  rec = recall_score(Y_true, Y_pred, average='macro')\n","  prec= precision_score(Y_true, Y_pred, average='macro')\n","  f1  = f1_score(Y_true, Y_pred, average='macro')\n","  return acc, rec, prec, f1, Y_true, Y_pred\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rm3uSwWoY6_7","colab_type":"text"},"source":["# Train the Model"]},{"cell_type":"code","metadata":{"id":"b0wGJ4rww7Fj","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_rec = 0\n","    epoch_prec = 0\n","    epoch_f1 = 0\n","\n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","                \n","        text, text_length = batch.text\n","        predictions = model(text, text_length).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.sentiment.long())\n","        \n","        acc, rec, prec, f1, Y_true, Y_pred = calculate_performance(batch.sentiment, predictions)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        epoch_rec += rec.item()\n","        epoch_prec += prec.item()\n","        epoch_f1 += f1.item()\n","\n","        avg_loss = epoch_loss / len(iterator)\n","        avg_acc = epoch_acc / len(iterator)\n","        avg_rec = epoch_rec / len(iterator)\n","        avg_prec = epoch_prec / len(iterator)\n","        avg_f1 = epoch_f1 / len(iterator)\n","\n","    return avg_loss, avg_acc, avg_rec, avg_prec, avg_f1, Y_true, Y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41SqIzKvJUdF","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, criterion):   \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_rec = 0\n","    epoch_prec = 0\n","    epoch_f1 = 0\n","    \n","    Y_true = []\n","    Y_pred = []\n","    model.eval()\n","\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            text, text_length = batch.text\n","            predictions = model(text, text_length).squeeze(1)\n","            actuals = batch.sentiment\n"," \n","            loss = criterion(predictions, batch.sentiment.long())\n","            \n","            acc, rec, prec, f1, y_true, y_pred = calculate_performance(batch.sentiment, predictions)\n","            Y_true = np.concatenate([Y_true,y_true])\n","            Y_pred = np.concatenate([Y_pred,y_pred])\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            epoch_rec += rec.item()\n","            epoch_prec += prec.item()\n","            epoch_f1 += f1.item()\n","\n","            avg_loss = epoch_loss / len(iterator)\n","            avg_acc = epoch_acc / len(iterator)\n","            avg_rec = epoch_rec / len(iterator)\n","            avg_prec = epoch_prec / len(iterator)\n","            avg_f1 = epoch_f1 / len(iterator)\n","    return avg_loss, avg_acc, avg_rec, avg_prec, avg_f1, Y_true, Y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0JpAUV-xYY2","colab_type":"code","colab":{}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xORR7LVxiro","colab_type":"code","colab":{}},"source":["def build_torch_dataset(csv_filepath, fields, vocab_params, is_training_data=True):\n","\tbatch_size = vocab_params['batch_size']\n","\tmax_vocab_size = vocab_params['max_vocab_size']\n","\tpretrained_embedding_filepath = vocab_params['pretrained_embedding']['filepath']\n","\tpretrained_embedding_url = vocab_params['pretrained_embedding']['url']\n","\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"," \n","\tdata = csv_to_tabular_dataset(\n","\t  csv_filepath,\n","\t  fields = fields,\n","\t)\n","\tif not is_training_data:\n","\t\t# split into batches\n","\t\tempty, empty, test_iterator = BucketIterator.splits(\n","\t\t\t\t(None, None, data), \n","\t\t\t\tbatch_sizes = (None, None, batch_size),\n","\t\t\t\tsort_key=lambda x: len(x.text),\n","\t\t\t\tsort_within_batch = True,\n","\t\t\t\tdevice = device)\n","\t\treturn test_iterator\n","\telse:\t\t\n","\t\t# split train data as traid:validation\n","\t\ttrain_data, valid_data = data.split(split_ratio=0.8)\n","\n","\t\t# split into batches\n","\t\ttrain_iterator, valid_iterator = BucketIterator.splits(\n","\t\t\t\t(train_data, valid_data), \n","\t\t\t\tbatch_sizes = (batch_size, batch_size),\n","\t\t\t\tsort_key=lambda x: len(x.text),\n","\t\t\t\tsort_within_batch = True,\n","\t\t\t\tdevice = device)\n","\n","\t\t# build vocabulary\n","\t\t# check if we need to download vector file\n","\t\tif not Path(pretrained_embedding_filepath).is_file():\n","\t\t\t# download vector file (in .zip)\n","\t\t\tr = requests.get(pretrained_embedding_url)\n","\t\t\tz = zipfile.ZipFile(io.BytesIO(r.content))\n","\t\t\t# unzip the file\n","\t\t\tz.extractall()\n","\n","\t\tROW_NUM.build_vocab(train_data)\n","\t\tSENTIMENT.build_vocab(train_data)\n","\t\tTEXT.build_vocab(train_data,\n","\t\t\t\t\t\t\t\t\t\tmax_size = max_vocab_size,\n","\t\t\t\t\t\t\t\t\t\tvectors = Vectors(pretrained_embedding_filepath),\n","\t\t\t\t\t\t\t\t\t\tunk_init = torch.Tensor.normal_\n","\t\t\t\t\t\t\t\t\t\t)\n","\n","\t\treturn ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s95VV1S8WMaR","colab_type":"code","colab":{}},"source":["def tune_hyperparams(config, ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator, model_filepath, best_valid_loss):\n","\tUNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\tPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","\t# build the RNN object\n","\tmodel = RNN(\n","\t\tlen(TEXT.vocab),\n","\t\tconfig['embedding_dim'],\n","\t\tconfig['hidden_dim'],\n","\t\tlen(SENTIMENT.vocab),\n","\t\tconfig['n_layers'],\n","\t\tconfig['is_bidirectional'],\n","\t\tconfig['dropout'],\n","\t\tPAD_IDX,\n","\t)\n","\n","\t# print the model and its number of params\n","\tprint(model.parameters)\n","\tprint(count_model_params(model))\n","\tprint(model)\n","\tfor p in model.parameters():\n","\t  print(p.numel())\n","\n","\t# replace the initial weights of the embedding layer with the pretrained embeddings\n","\tpretrained_embeddings = TEXT.vocab.vectors\n","\tmodel.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","  # initialise UNK and PAD tokens to zeros\n","\tmodel.embedding.weight.data[UNK_IDX] = torch.zeros(config['embedding_dim'])\n","\tmodel.embedding.weight.data[PAD_IDX] = torch.zeros(config['embedding_dim'])\n","\n","\toptimiser = optim.Adam(model.parameters())\n","\tcriterion = nn.CrossEntropyLoss()\n","\n","\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\tmodel = model.to(device)\n","\tcriterion = criterion.to(device)\n","\n","\t# --- EARLY STOPPING\n","\tuse_early_stopping = True\n","\tearly_stopping_patience = 2\n","\n","\tfor epoch in range(config['n_epochs']):\n","\t\tstart_time = time.time()\n","\t\ttrain_loss, train_acc, train_rec, train_prec, train_f1, train_Y_true, train_Y_pred = train(model, train_iterator, optimiser, criterion)\n","\t\tvalid_loss, valid_acc, valid_rec, valid_prec, valid_f1, valid_Y_true, valid_Y_pred = evaluate(model, valid_iterator, criterion)\n","\t\tend_time = time.time()\n","\n","\t\tepoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","\t\tif valid_loss < best_valid_loss:\n","\t\t\tbest_valid_loss = valid_loss\n","\t\t\t# save the model as a file\n","\t\t\tcheckpoint = {\n","\t\t\t\t\t'model' : model,\n","\t\t\t\t\t'state_dict' : model.state_dict(),\n","\t\t\t\t\t'optimiser' : optimiser.state_dict(),\n","\t\t\t}\n","\t\t\ttorch.save(checkpoint, model_filepath)\n","\t \t\t# save the Field(s)\n","\t\t\twith open(cfg['paths']['fields']['row_num'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(ROW_NUM,f)\n","\t\t\twith open(cfg['paths']['fields']['text'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(TEXT,f)\n","\t\t\twith open(cfg['paths']['fields']['sentiment'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(SENTIMENT,f)\n","\t\telse:\n","\t\t\tearly_stopping_patience -= 1   \n","\t\tprint(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","\t\tprint(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n","\t\tprint(f'\\tTrain Acc : {train_acc*100:.2f}% | Val. Acc : {valid_acc*100:.2f}%')\n","\t\tprint(f'\\tTrain Rec : {train_rec*100:.2f}% | Val. Rec : {valid_rec*100:.2f}%')\n","\t\tprint(f'\\tTrain Prec: {train_prec*100:.2f}% | Val. Prec: {valid_prec*100:.2f}%')\n","\t\tprint(f'\\tTrain F1  : {train_f1*100:.2f}% | Val. F1  : {valid_f1*100:.2f}%')\n","\n","\t\t# check if early stopping is needed\n","\t\tif use_early_stopping:\n","\t\t\tif early_stopping_patience < 0:\n","\t\t\t\tprint('Early stopping!' )\n","\t\t\t\tbreak\n","\n","\t# after completing all epochs, visualise the word vectors\n","\tvectors = model.embedding.weight.data\n","\tlabels = [l for l in TEXT.vocab.itos]\n","\n","\tprint('best valid loss: ', best_valid_loss)\n","\treturn best_valid_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGU30dGY3t1Z","colab_type":"code","outputId":"4206c818-f577-4a3a-a854-1782ac79ffcc","executionInfo":{"status":"ok","timestamp":1588577228903,"user_tz":-60,"elapsed":3150191,"user":{"displayName":"Dony Arisandy Wiranata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giy4RjCBJgU9A8zhT2B-AlcY_LP7-W-c59IBcPEoGQ=s64","userId":"05975001259031186418"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def generate_text_processor(preprocessing_params):\n","  text_processor = TextPreProcessor(\n","    # normalized these terms (ex: \"google.com\" into \"<url>\")\n","    normalize = preprocessing_params['normalize'],\n","\n","    # annotate these terms (ex: \"#win\" into [\"<hashtag>\", \"win\", \"</hashtag>\"])\n","    annotate = preprocessing_params['annotate'],\n","    fix_html=True,\n","    segmenter = 'twitter',\n","    corrector='twitter',\n","    unpack_hashtags=True,\n","    unpack_contractions=True,\n","    spell_correct_elong=preprocessing_params['spell_correct_elong'],\n","    tokenizer = SocialTokenizer(lowercase=preprocessing_params['to_lowercase']).tokenize,\n","    dicts = [emoticons]\n","  )\n","  return text_processor\n","\n","def custom_tokenizer(example):\n","  return text_processor.pre_process_doc(example)\n","\n","from itertools import product\n","def get_cartesian_product(d):\n","  # get all possible combination of a dictionary containing lists\n","  return [dict(zip(d, v)) for v in product(*d.values())]\n","\n","\n","# RUN EXPERIMENTS / MAIN\n","vocab_params_trials = get_cartesian_product(params['vocab'])\n","nn_params_trials = get_cartesian_product(params['nn'])\n","best_valid_loss = float('inf')\n","\n","for vocab_params in vocab_params_trials:\n","  # build torch dataset and build the vocab.\n","  # this should be done only once during hyperparam searching\n","  print('-'*25  )\n","  print(vocab_params)\n","\n","  # text_processor will be used inside the custom_tokenizer\n","  text_processor = generate_text_processor(vocab_params['preprocessing'])\n","  TEXT = Field(tokenize=custom_tokenizer,\n","              include_lengths=True)\n","  SENTIMENT = LabelField(dtype = torch.int)\n","  ROW_NUM = Field()\n"," \n","\n","  fields = [('row_num', ROW_NUM), ('text', TEXT), ('sentiment', SENTIMENT)]\n","  ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator = build_torch_dataset(train_csv_filepath, fields, vocab_params, True)\n","\n","  for nn_params in nn_params_trials:\n","    # execute the training process\n","      nn_params['embedding_dim'] = vocab_params['pretrained_embedding']['embedding_dim']\n","      print('#'*25)\n","      print(nn_params)\n","  \n","      best_valid_loss = tune_hyperparams(nn_params, ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator, saved_model_filepath, best_valid_loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-------------------------\n","{'batch_size': 32, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Word statistics files not found!\n","Downloading... done!\n","Unpacking... done!\n","Reading twitter - 1grams ...\n","generating cache file for faster loading...\n","reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n","Reading twitter - 2grams ...\n","generating cache file for faster loading...\n","reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n","Reading twitter - 1grams ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/1999995 [00:00<?, ?it/s]Skipping token b'1999995' with 1-dimensional vector [b'300']; likely a header\n","100%|█████████▉| 1999919/1999995 [04:33<00:00, 8550.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.339 | Val. Loss: 0.283\n","\tTrain Acc : 86.00% | Val. Acc : 89.04%\n","\tTrain Rec : 73.64% | Val. Rec : 80.67%\n","\tTrain Prec: 78.07% | Val. Prec: 83.19%\n","\tTrain F1  : 73.75% | Val. F1  : 80.99%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.246 | Val. Loss: 0.268\n","\tTrain Acc : 89.88% | Val. Acc : 89.51%\n","\tTrain Rec : 83.32% | Val. Rec : 81.20%\n","\tTrain Prec: 85.70% | Val. Prec: 85.51%\n","\tTrain F1  : 83.26% | Val. F1  : 81.91%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.188 | Val. Loss: 0.314\n","\tTrain Acc : 92.53% | Val. Acc : 88.80%\n","\tTrain Rec : 87.80% | Val. Rec : 79.60%\n","\tTrain Prec: 89.60% | Val. Prec: 84.01%\n","\tTrain F1  : 87.78% | Val. F1  : 80.38%\n","Epoch: 04 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.145 | Val. Loss: 0.289\n","\tTrain Acc : 94.41% | Val. Acc : 88.32%\n","\tTrain Rec : 90.30% | Val. Rec : 80.92%\n","\tTrain Prec: 92.02% | Val. Prec: 83.05%\n","\tTrain F1  : 90.49% | Val. F1  : 80.66%\n","Epoch: 05 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.110 | Val. Loss: 0.336\n","\tTrain Acc : 95.77% | Val. Acc : 87.51%\n","\tTrain Rec : 93.22% | Val. Rec : 77.86%\n","\tTrain Prec: 94.02% | Val. Prec: 82.65%\n","\tTrain F1  : 93.05% | Val. F1  : 78.56%\n","Early stopping!\n","best valid loss:  0.2677929479034007\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.402 | Val. Loss: 0.321\n","\tTrain Acc : 83.05% | Val. Acc : 86.81%\n","\tTrain Rec : 66.99% | Val. Rec : 70.89%\n","\tTrain Prec: 70.10% | Val. Prec: 83.88%\n","\tTrain F1  : 66.32% | Val. F1  : 73.24%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.308 | Val. Loss: 0.314\n","\tTrain Acc : 87.34% | Val. Acc : 89.12%\n","\tTrain Rec : 77.54% | Val. Rec : 78.91%\n","\tTrain Prec: 81.89% | Val. Prec: 84.91%\n","\tTrain F1  : 78.06% | Val. F1  : 80.28%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.275 | Val. Loss: 0.326\n","\tTrain Acc : 88.78% | Val. Acc : 89.27%\n","\tTrain Rec : 80.59% | Val. Rec : 78.28%\n","\tTrain Prec: 84.22% | Val. Prec: 87.13%\n","\tTrain F1  : 80.95% | Val. F1  : 80.35%\n","Early stopping!\n","best valid loss:  0.2677929479034007\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.340 | Val. Loss: 0.275\n","\tTrain Acc : 85.52% | Val. Acc : 89.14%\n","\tTrain Rec : 73.39% | Val. Rec : 78.78%\n","\tTrain Prec: 76.44% | Val. Prec: 86.02%\n","\tTrain F1  : 73.21% | Val. F1  : 80.48%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.244 | Val. Loss: 0.261\n","\tTrain Acc : 89.86% | Val. Acc : 88.88%\n","\tTrain Rec : 83.05% | Val. Rec : 82.15%\n","\tTrain Prec: 85.79% | Val. Prec: 83.96%\n","\tTrain F1  : 83.18% | Val. F1  : 81.91%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.192 | Val. Loss: 0.311\n","\tTrain Acc : 92.35% | Val. Acc : 88.93%\n","\tTrain Rec : 87.69% | Val. Rec : 79.67%\n","\tTrain Prec: 89.29% | Val. Prec: 84.44%\n","\tTrain F1  : 87.54% | Val. F1  : 80.60%\n","Epoch: 04 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.149 | Val. Loss: 0.291\n","\tTrain Acc : 94.29% | Val. Acc : 88.59%\n","\tTrain Rec : 90.51% | Val. Rec : 78.70%\n","\tTrain Prec: 91.96% | Val. Prec: 85.28%\n","\tTrain F1  : 90.52% | Val. F1  : 79.86%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.403 | Val. Loss: 0.316\n","\tTrain Acc : 82.63% | Val. Acc : 88.20%\n","\tTrain Rec : 66.57% | Val. Rec : 76.81%\n","\tTrain Prec: 69.97% | Val. Prec: 83.41%\n","\tTrain F1  : 66.09% | Val. F1  : 78.33%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.315 | Val. Loss: 0.310\n","\tTrain Acc : 86.98% | Val. Acc : 89.27%\n","\tTrain Rec : 77.26% | Val. Rec : 80.31%\n","\tTrain Prec: 82.03% | Val. Prec: 84.99%\n","\tTrain F1  : 77.90% | Val. F1  : 81.12%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.273 | Val. Loss: 0.306\n","\tTrain Acc : 88.76% | Val. Acc : 88.64%\n","\tTrain Rec : 80.48% | Val. Rec : 76.72%\n","\tTrain Prec: 84.19% | Val. Prec: 86.68%\n","\tTrain F1  : 80.96% | Val. F1  : 78.91%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.334 | Val. Loss: 0.278\n","\tTrain Acc : 85.80% | Val. Acc : 88.77%\n","\tTrain Rec : 73.71% | Val. Rec : 78.12%\n","\tTrain Prec: 78.50% | Val. Prec: 85.78%\n","\tTrain F1  : 73.89% | Val. F1  : 79.62%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.243 | Val. Loss: 0.279\n","\tTrain Acc : 89.82% | Val. Acc : 89.01%\n","\tTrain Rec : 82.91% | Val. Rec : 79.65%\n","\tTrain Prec: 85.79% | Val. Prec: 85.97%\n","\tTrain F1  : 83.10% | Val. F1  : 80.87%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.186 | Val. Loss: 0.303\n","\tTrain Acc : 92.49% | Val. Acc : 88.48%\n","\tTrain Rec : 87.64% | Val. Rec : 81.22%\n","\tTrain Prec: 89.27% | Val. Prec: 83.38%\n","\tTrain F1  : 87.61% | Val. F1  : 80.98%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.397 | Val. Loss: 0.289\n","\tTrain Acc : 83.20% | Val. Acc : 88.12%\n","\tTrain Rec : 67.63% | Val. Rec : 75.62%\n","\tTrain Prec: 72.49% | Val. Prec: 83.95%\n","\tTrain F1  : 67.41% | Val. F1  : 77.64%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.314 | Val. Loss: 0.315\n","\tTrain Acc : 86.80% | Val. Acc : 88.09%\n","\tTrain Rec : 76.90% | Val. Rec : 74.17%\n","\tTrain Prec: 81.62% | Val. Prec: 85.65%\n","\tTrain F1  : 77.32% | Val. F1  : 76.76%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.281 | Val. Loss: 0.266\n","\tTrain Acc : 88.49% | Val. Acc : 89.51%\n","\tTrain Rec : 80.12% | Val. Rec : 79.36%\n","\tTrain Prec: 83.91% | Val. Prec: 86.13%\n","\tTrain F1  : 80.49% | Val. F1  : 80.93%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.347 | Val. Loss: 0.290\n","\tTrain Acc : 85.46% | Val. Acc : 88.38%\n","\tTrain Rec : 73.49% | Val. Rec : 78.06%\n","\tTrain Prec: 77.58% | Val. Prec: 83.97%\n","\tTrain F1  : 73.24% | Val. F1  : 79.33%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.253 | Val. Loss: 0.277\n","\tTrain Acc : 89.72% | Val. Acc : 88.93%\n","\tTrain Rec : 82.88% | Val. Rec : 77.10%\n","\tTrain Prec: 85.40% | Val. Prec: 87.36%\n","\tTrain F1  : 82.91% | Val. F1  : 79.40%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.196 | Val. Loss: 0.296\n","\tTrain Acc : 92.22% | Val. Acc : 88.43%\n","\tTrain Rec : 87.35% | Val. Rec : 80.45%\n","\tTrain Prec: 89.20% | Val. Prec: 82.63%\n","\tTrain F1  : 87.39% | Val. F1  : 80.42%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.396 | Val. Loss: 0.352\n","\tTrain Acc : 83.59% | Val. Acc : 86.94%\n","\tTrain Rec : 68.00% | Val. Rec : 72.60%\n","\tTrain Prec: 73.09% | Val. Prec: 83.88%\n","\tTrain F1  : 68.03% | Val. F1  : 74.83%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.315 | Val. Loss: 0.274\n","\tTrain Acc : 86.86% | Val. Acc : 88.75%\n","\tTrain Rec : 76.75% | Val. Rec : 76.92%\n","\tTrain Prec: 81.88% | Val. Prec: 85.01%\n","\tTrain F1  : 77.38% | Val. F1  : 78.90%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.295 | Val. Loss: 0.291\n","\tTrain Acc : 88.17% | Val. Acc : 87.75%\n","\tTrain Rec : 79.43% | Val. Rec : 82.63%\n","\tTrain Prec: 83.36% | Val. Prec: 80.41%\n","\tTrain F1  : 79.84% | Val. F1  : 80.41%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 0/1193514 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 1062/1193514 [00:00<01:52, 10611.46it/s]\u001b[A\n","  0%|          | 2126/1193514 [00:00<01:52, 10619.81it/s]\u001b[A\n","  0%|          | 3215/1193514 [00:00<01:51, 10698.57it/s]\u001b[A\n","  0%|          | 4218/1193514 [00:00<01:53, 10487.93it/s]\u001b[A\n","  0%|          | 5364/1193514 [00:00<01:50, 10761.08it/s]\u001b[A\n","  1%|          | 6335/1193514 [00:00<01:53, 10420.22it/s]\u001b[A\n","  1%|          | 7398/1193514 [00:00<01:53, 10478.87it/s]\u001b[A\n","  1%|          | 8537/1193514 [00:00<01:50, 10735.73it/s]\u001b[A\n","  1%|          | 9676/1193514 [00:00<01:48, 10923.27it/s]\u001b[A\n","  1%|          | 10816/1193514 [00:01<01:46, 11060.88it/s]\u001b[A\n","  1%|          | 11956/1193514 [00:01<01:45, 11158.68it/s]\u001b[A\n","  1%|          | 13054/1193514 [00:01<01:47, 10939.46it/s]\u001b[A\n","  1%|          | 14150/1193514 [00:01<01:47, 10944.14it/s]\u001b[A\n","  1%|▏         | 15236/1193514 [00:01<01:51, 10581.60it/s]\u001b[A\n","  1%|▏         | 16347/1193514 [00:01<01:49, 10732.91it/s]\u001b[A\n","  1%|▏         | 17450/1193514 [00:01<01:48, 10819.59it/s]\u001b[A\n","  2%|▏         | 18537/1193514 [00:01<01:48, 10832.70it/s]\u001b[A\n","  2%|▏         | 19649/1193514 [00:01<01:47, 10916.63it/s]\u001b[A\n","  2%|▏         | 20743/1193514 [00:01<01:47, 10922.65it/s]\u001b[A\n","  2%|▏         | 21860/1193514 [00:02<01:46, 10995.40it/s]\u001b[A\n","  2%|▏         | 22960/1193514 [00:02<01:46, 10963.99it/s]\u001b[A\n","  2%|▏         | 24057/1193514 [00:02<01:50, 10578.84it/s]\u001b[A\n","  2%|▏         | 25155/1193514 [00:02<01:49, 10694.06it/s]\u001b[A\n","  2%|▏         | 26298/1193514 [00:02<01:47, 10903.61it/s]\u001b[A\n","  2%|▏         | 27431/1193514 [00:02<01:45, 11026.11it/s]\u001b[A\n","  2%|▏         | 28548/1193514 [00:02<01:45, 11067.71it/s]\u001b[A\n","  2%|▏         | 29657/1193514 [00:02<01:46, 10966.42it/s]\u001b[A\n","  3%|▎         | 30760/1193514 [00:02<01:45, 10983.09it/s]\u001b[A\n","  3%|▎         | 31860/1193514 [00:02<01:48, 10680.74it/s]\u001b[A\n","  3%|▎         | 32931/1193514 [00:03<01:49, 10613.62it/s]\u001b[A\n","  3%|▎         | 34071/1193514 [00:03<01:46, 10837.17it/s]\u001b[A\n","  3%|▎         | 35158/1193514 [00:03<01:47, 10802.45it/s]\u001b[A\n","  3%|▎         | 36240/1193514 [00:03<01:49, 10610.89it/s]\u001b[A\n","  3%|▎         | 37349/1193514 [00:03<01:47, 10748.19it/s]\u001b[A\n","  3%|▎         | 38470/1193514 [00:03<01:46, 10880.99it/s]\u001b[A\n","  3%|▎         | 39594/1193514 [00:03<01:45, 10984.72it/s]\u001b[A\n","  3%|▎         | 40694/1193514 [00:03<01:45, 10921.05it/s]\u001b[A\n","  4%|▎         | 41792/1193514 [00:03<01:45, 10938.56it/s]\u001b[A\n","  4%|▎         | 42887/1193514 [00:03<01:45, 10871.91it/s]\u001b[A\n","  4%|▎         | 44009/1193514 [00:04<01:44, 10972.14it/s]\u001b[A\n","  4%|▍         | 45137/1193514 [00:04<01:43, 11061.60it/s]\u001b[A\n","  4%|▍         | 46244/1193514 [00:04<01:45, 10860.28it/s]\u001b[A\n","  4%|▍         | 47333/1193514 [00:04<01:45, 10867.42it/s]\u001b[A\n","  4%|▍         | 48463/1193514 [00:04<01:44, 10992.94it/s]\u001b[A\n","  4%|▍         | 49564/1193514 [00:04<01:44, 10945.98it/s]\u001b[A\n","  4%|▍         | 50692/1193514 [00:04<01:43, 11041.64it/s]\u001b[A\n","  4%|▍         | 51820/1193514 [00:04<01:42, 11111.55it/s]\u001b[A\n","  4%|▍         | 52932/1193514 [00:04<01:45, 10813.33it/s]\u001b[A\n","  5%|▍         | 54016/1193514 [00:04<01:45, 10766.01it/s]\u001b[A\n","  5%|▍         | 55095/1193514 [00:05<01:46, 10731.93it/s]\u001b[A\n","  5%|▍         | 56188/1193514 [00:05<01:45, 10790.25it/s]\u001b[A\n","  5%|▍         | 57268/1193514 [00:05<01:45, 10790.65it/s]\u001b[A\n","  5%|▍         | 58396/1193514 [00:05<01:43, 10929.69it/s]\u001b[A\n","  5%|▍         | 59499/1193514 [00:05<01:43, 10958.58it/s]\u001b[A\n","  5%|▌         | 60612/1193514 [00:05<01:42, 11008.30it/s]\u001b[A\n","  5%|▌         | 61765/1193514 [00:05<01:41, 11157.79it/s]\u001b[A\n","  5%|▌         | 62896/1193514 [00:05<01:40, 11202.38it/s]\u001b[A\n","  5%|▌         | 64017/1193514 [00:05<01:41, 11181.65it/s]\u001b[A\n","  5%|▌         | 65147/1193514 [00:05<01:40, 11215.59it/s]\u001b[A\n","  6%|▌         | 66269/1193514 [00:06<01:41, 11141.41it/s]\u001b[A\n","  6%|▌         | 67384/1193514 [00:06<01:45, 10625.81it/s]\u001b[A\n","  6%|▌         | 68452/1193514 [00:06<01:47, 10491.69it/s]\u001b[A\n","  6%|▌         | 69506/1193514 [00:06<01:48, 10349.60it/s]\u001b[A\n","  6%|▌         | 70618/1193514 [00:06<01:46, 10567.14it/s]\u001b[A\n","  6%|▌         | 71738/1193514 [00:06<01:44, 10748.07it/s]\u001b[A\n","  6%|▌         | 72816/1193514 [00:06<01:47, 10402.69it/s]\u001b[A\n","  6%|▌         | 73861/1193514 [00:06<01:50, 10112.44it/s]\u001b[A\n","  6%|▋         | 74878/1193514 [00:06<01:52, 9977.68it/s] \u001b[A\n","  6%|▋         | 75880/1193514 [00:07<01:57, 9526.95it/s]\u001b[A\n","  6%|▋         | 76840/1193514 [00:07<01:56, 9548.64it/s]\u001b[A\n","  7%|▋         | 77808/1193514 [00:07<01:56, 9585.34it/s]\u001b[A\n","  7%|▋         | 78771/1193514 [00:07<01:57, 9500.80it/s]\u001b[A\n","  7%|▋         | 79826/1193514 [00:07<01:53, 9792.15it/s]\u001b[A\n","  7%|▋         | 80907/1193514 [00:07<01:50, 10074.94it/s]\u001b[A\n","  7%|▋         | 81992/1193514 [00:07<01:47, 10294.57it/s]\u001b[A\n","  7%|▋         | 83090/1193514 [00:07<01:45, 10488.86it/s]\u001b[A\n","  7%|▋         | 84209/1193514 [00:07<01:43, 10686.92it/s]\u001b[A\n","  7%|▋         | 85356/1193514 [00:07<01:41, 10910.20it/s]\u001b[A\n","  7%|▋         | 86451/1193514 [00:08<01:41, 10876.12it/s]\u001b[A\n","  7%|▋         | 87573/1193514 [00:08<01:40, 10976.39it/s]\u001b[A\n","  7%|▋         | 88673/1193514 [00:08<01:43, 10688.75it/s]\u001b[A\n","  8%|▊         | 89746/1193514 [00:08<01:44, 10574.81it/s]\u001b[A\n","  8%|▊         | 90851/1193514 [00:08<01:42, 10711.55it/s]\u001b[A\n","  8%|▊         | 91935/1193514 [00:08<01:42, 10748.47it/s]\u001b[A\n","  8%|▊         | 93012/1193514 [00:08<01:46, 10357.49it/s]\u001b[A\n","  8%|▊         | 94052/1193514 [00:08<01:50, 9964.60it/s] \u001b[A\n","  8%|▊         | 95055/1193514 [00:08<01:51, 9864.28it/s]\u001b[A\n","  8%|▊         | 96143/1193514 [00:08<01:48, 10146.97it/s]\u001b[A\n","  8%|▊         | 97163/1193514 [00:09<01:50, 9966.68it/s] \u001b[A\n","  8%|▊         | 98281/1193514 [00:09<01:46, 10299.40it/s]\u001b[A\n","  8%|▊         | 99372/1193514 [00:09<01:44, 10472.31it/s]\u001b[A\n","  8%|▊         | 100445/1193514 [00:09<01:43, 10545.91it/s]\u001b[A\n","  9%|▊         | 101556/1193514 [00:09<01:41, 10706.92it/s]\u001b[A\n","  9%|▊         | 102687/1193514 [00:09<01:40, 10880.30it/s]\u001b[A\n","  9%|▊         | 103778/1193514 [00:09<01:40, 10797.46it/s]\u001b[A\n","  9%|▉         | 104860/1193514 [00:09<01:44, 10393.39it/s]\u001b[A\n","  9%|▉         | 105994/1193514 [00:09<01:42, 10658.11it/s]\u001b[A\n","  9%|▉         | 107065/1193514 [00:10<01:44, 10419.57it/s]\u001b[A\n","  9%|▉         | 108112/1193514 [00:10<01:46, 10173.97it/s]\u001b[A\n","  9%|▉         | 109134/1193514 [00:10<01:47, 10101.02it/s]\u001b[A\n","  9%|▉         | 110221/1193514 [00:10<01:44, 10318.29it/s]\u001b[A\n","  9%|▉         | 111303/1193514 [00:10<01:43, 10461.92it/s]\u001b[A\n","  9%|▉         | 112353/1193514 [00:10<01:45, 10222.62it/s]\u001b[A\n"," 10%|▉         | 113414/1193514 [00:10<01:44, 10335.05it/s]\u001b[A\n"," 10%|▉         | 114524/1193514 [00:10<01:42, 10551.44it/s]\u001b[A\n"," 10%|▉         | 115583/1193514 [00:10<01:42, 10561.29it/s]\u001b[A\n"," 10%|▉         | 116672/1193514 [00:10<01:41, 10657.23it/s]\u001b[A\n"," 10%|▉         | 117780/1193514 [00:11<01:39, 10779.26it/s]\u001b[A\n"," 10%|▉         | 118860/1193514 [00:11<01:44, 10258.63it/s]\u001b[A\n"," 10%|█         | 119893/1193514 [00:11<01:45, 10140.40it/s]\u001b[A\n"," 10%|█         | 120984/1193514 [00:11<01:43, 10359.02it/s]\u001b[A\n"," 10%|█         | 122108/1193514 [00:11<01:41, 10607.97it/s]\u001b[A\n"," 10%|█         | 123237/1193514 [00:11<01:39, 10803.51it/s]\u001b[A\n"," 10%|█         | 124322/1193514 [00:11<01:41, 10535.87it/s]\u001b[A\n"," 11%|█         | 125423/1193514 [00:11<01:40, 10671.37it/s]\u001b[A\n"," 11%|█         | 126494/1193514 [00:11<01:41, 10560.12it/s]\u001b[A\n"," 11%|█         | 127577/1193514 [00:11<01:40, 10638.03it/s]\u001b[A\n"," 11%|█         | 128684/1193514 [00:12<01:38, 10763.91it/s]\u001b[A\n"," 11%|█         | 129791/1193514 [00:12<01:38, 10852.55it/s]\u001b[A\n"," 11%|█         | 130899/1193514 [00:12<01:37, 10916.27it/s]\u001b[A\n"," 11%|█         | 131992/1193514 [00:12<01:37, 10890.34it/s]\u001b[A\n"," 11%|█         | 133094/1193514 [00:12<01:37, 10928.11it/s]\u001b[A\n"," 11%|█         | 134188/1193514 [00:12<01:37, 10890.56it/s]\u001b[A\n"," 11%|█▏        | 135290/1193514 [00:12<01:36, 10926.46it/s]\u001b[A\n"," 11%|█▏        | 136383/1193514 [00:12<01:37, 10802.64it/s]\u001b[A\n"," 12%|█▏        | 137483/1193514 [00:12<01:37, 10859.83it/s]\u001b[A\n"," 12%|█▏        | 138570/1193514 [00:13<01:39, 10566.96it/s]\u001b[A\n"," 12%|█▏        | 139702/1193514 [00:13<01:37, 10781.59it/s]\u001b[A\n"," 12%|█▏        | 140833/1193514 [00:13<01:36, 10931.62it/s]\u001b[A\n"," 12%|█▏        | 141950/1193514 [00:13<01:35, 11000.93it/s]\u001b[A\n"," 12%|█▏        | 143052/1193514 [00:13<01:36, 10836.55it/s]\u001b[A\n"," 12%|█▏        | 144138/1193514 [00:13<01:40, 10439.33it/s]\u001b[A\n"," 12%|█▏        | 145263/1193514 [00:13<01:38, 10668.70it/s]\u001b[A\n"," 12%|█▏        | 146384/1193514 [00:13<01:36, 10825.35it/s]\u001b[A\n"," 12%|█▏        | 147478/1193514 [00:13<01:36, 10859.22it/s]\u001b[A\n"," 12%|█▏        | 148585/1193514 [00:13<01:35, 10921.57it/s]\u001b[A\n"," 13%|█▎        | 149679/1193514 [00:14<01:35, 10914.34it/s]\u001b[A\n"," 13%|█▎        | 150808/1193514 [00:14<01:34, 11023.31it/s]\u001b[A\n"," 13%|█▎        | 151912/1193514 [00:14<01:37, 10729.43it/s]\u001b[A\n"," 13%|█▎        | 153057/1193514 [00:14<01:35, 10935.52it/s]\u001b[A\n"," 13%|█▎        | 154154/1193514 [00:14<01:39, 10439.58it/s]\u001b[A\n"," 13%|█▎        | 155256/1193514 [00:14<01:37, 10606.20it/s]\u001b[A\n"," 13%|█▎        | 156362/1193514 [00:14<01:36, 10736.45it/s]\u001b[A\n"," 13%|█▎        | 157440/1193514 [00:14<01:39, 10460.57it/s]\u001b[A\n"," 13%|█▎        | 158491/1193514 [00:14<01:42, 10096.46it/s]\u001b[A\n"," 13%|█▎        | 159507/1193514 [00:14<01:43, 9947.34it/s] \u001b[A\n"," 13%|█▎        | 160507/1193514 [00:15<01:45, 9808.74it/s]\u001b[A\n"," 14%|█▎        | 161601/1193514 [00:15<01:41, 10120.81it/s]\u001b[A\n"," 14%|█▎        | 162749/1193514 [00:15<01:38, 10490.68it/s]\u001b[A\n"," 14%|█▎        | 163822/1193514 [00:15<01:37, 10560.85it/s]\u001b[A\n"," 14%|█▍        | 164907/1193514 [00:15<01:36, 10643.46it/s]\u001b[A\n"," 14%|█▍        | 166047/1193514 [00:15<01:34, 10859.56it/s]\u001b[A\n"," 14%|█▍        | 167168/1193514 [00:15<01:33, 10960.35it/s]\u001b[A\n"," 14%|█▍        | 168303/1193514 [00:15<01:32, 11074.23it/s]\u001b[A\n"," 14%|█▍        | 169413/1193514 [00:15<01:34, 10817.59it/s]\u001b[A\n"," 14%|█▍        | 170540/1193514 [00:15<01:33, 10948.02it/s]\u001b[A\n"," 14%|█▍        | 171653/1193514 [00:16<01:32, 11001.22it/s]\u001b[A\n"," 14%|█▍        | 172802/1193514 [00:16<01:31, 11141.38it/s]\u001b[A\n"," 15%|█▍        | 173918/1193514 [00:16<01:33, 10884.14it/s]\u001b[A\n"," 15%|█▍        | 175009/1193514 [00:16<01:35, 10677.36it/s]\u001b[A\n"," 15%|█▍        | 176106/1193514 [00:16<01:34, 10761.85it/s]\u001b[A\n"," 15%|█▍        | 177224/1193514 [00:16<01:33, 10883.39it/s]\u001b[A\n"," 15%|█▍        | 178365/1193514 [00:16<01:31, 11035.01it/s]\u001b[A\n"," 15%|█▌        | 179505/1193514 [00:16<01:31, 11140.32it/s]\u001b[A\n"," 15%|█▌        | 180621/1193514 [00:16<01:37, 10425.37it/s]\u001b[A\n"," 15%|█▌        | 181745/1193514 [00:17<01:34, 10653.81it/s]\u001b[A\n"," 15%|█▌        | 182875/1193514 [00:17<01:33, 10838.39it/s]\u001b[A\n"," 15%|█▌        | 183991/1193514 [00:17<01:32, 10932.60it/s]\u001b[A\n"," 16%|█▌        | 185090/1193514 [00:17<01:35, 10565.52it/s]\u001b[A\n"," 16%|█▌        | 186171/1193514 [00:17<01:34, 10636.93it/s]\u001b[A\n"," 16%|█▌        | 187289/1193514 [00:17<01:33, 10792.88it/s]\u001b[A\n"," 16%|█▌        | 188383/1193514 [00:17<01:32, 10835.19it/s]\u001b[A\n"," 16%|█▌        | 189498/1193514 [00:17<01:31, 10926.50it/s]\u001b[A\n"," 16%|█▌        | 190633/1193514 [00:17<01:30, 11048.30it/s]\u001b[A\n"," 16%|█▌        | 191740/1193514 [00:17<01:32, 10793.39it/s]\u001b[A\n"," 16%|█▌        | 192822/1193514 [00:18<01:33, 10661.81it/s]\u001b[A\n"," 16%|█▌        | 193931/1193514 [00:18<01:32, 10784.15it/s]\u001b[A\n"," 16%|█▋        | 195012/1193514 [00:18<01:33, 10736.07it/s]\u001b[A\n"," 16%|█▋        | 196141/1193514 [00:18<01:31, 10894.89it/s]\u001b[A\n"," 17%|█▋        | 197269/1193514 [00:18<01:30, 11002.13it/s]\u001b[A\n"," 17%|█▋        | 198371/1193514 [00:18<01:31, 10929.22it/s]\u001b[A\n"," 17%|█▋        | 199505/1193514 [00:18<01:29, 11047.21it/s]\u001b[A\n"," 17%|█▋        | 200659/1193514 [00:18<01:28, 11189.73it/s]\u001b[A\n"," 17%|█▋        | 201788/1193514 [00:18<01:28, 11217.52it/s]\u001b[A\n"," 17%|█▋        | 202911/1193514 [00:18<01:29, 11084.92it/s]\u001b[A\n"," 17%|█▋        | 204021/1193514 [00:19<01:29, 11087.61it/s]\u001b[A\n"," 17%|█▋        | 205174/1193514 [00:19<01:28, 11214.73it/s]\u001b[A\n"," 17%|█▋        | 206297/1193514 [00:19<01:28, 11157.44it/s]\u001b[A\n"," 17%|█▋        | 207433/1193514 [00:19<01:27, 11216.42it/s]\u001b[A\n"," 17%|█▋        | 208556/1193514 [00:19<01:27, 11215.47it/s]\u001b[A\n"," 18%|█▊        | 209678/1193514 [00:19<01:28, 11078.49it/s]\u001b[A\n"," 18%|█▊        | 210787/1193514 [00:19<01:30, 10816.69it/s]\u001b[A\n"," 18%|█▊        | 211900/1193514 [00:19<01:29, 10906.96it/s]\u001b[A\n"," 18%|█▊        | 213022/1193514 [00:19<01:29, 10998.88it/s]\u001b[A\n"," 18%|█▊        | 214124/1193514 [00:19<01:30, 10781.00it/s]\u001b[A\n"," 18%|█▊        | 215204/1193514 [00:20<01:34, 10394.98it/s]\u001b[A\n"," 18%|█▊        | 216248/1193514 [00:20<01:36, 10107.62it/s]\u001b[A\n"," 18%|█▊        | 217367/1193514 [00:20<01:33, 10408.60it/s]\u001b[A\n"," 18%|█▊        | 218503/1193514 [00:20<01:31, 10675.22it/s]\u001b[A\n"," 18%|█▊        | 219644/1193514 [00:20<01:29, 10882.91it/s]\u001b[A\n"," 18%|█▊        | 220767/1193514 [00:20<01:28, 10983.27it/s]\u001b[A\n"," 19%|█▊        | 221913/1193514 [00:20<01:27, 11121.27it/s]\u001b[A\n"," 19%|█▊        | 223065/1193514 [00:20<01:26, 11235.62it/s]\u001b[A\n"," 19%|█▉        | 224191/1193514 [00:20<01:30, 10673.55it/s]\u001b[A\n"," 19%|█▉        | 225315/1193514 [00:21<01:29, 10835.19it/s]\u001b[A\n"," 19%|█▉        | 226476/1193514 [00:21<01:27, 11055.91it/s]\u001b[A\n"," 19%|█▉        | 227617/1193514 [00:21<01:26, 11156.07it/s]\u001b[A\n"," 19%|█▉        | 228737/1193514 [00:21<01:27, 11059.95it/s]\u001b[A\n"," 19%|█▉        | 229871/1193514 [00:21<01:26, 11141.60it/s]\u001b[A\n"," 19%|█▉        | 230997/1193514 [00:21<01:26, 11174.34it/s]\u001b[A\n"," 19%|█▉        | 232116/1193514 [00:21<01:27, 11041.00it/s]\u001b[A\n"," 20%|█▉        | 233222/1193514 [00:21<01:27, 11030.92it/s]\u001b[A\n"," 20%|█▉        | 234370/1193514 [00:21<01:25, 11159.06it/s]\u001b[A\n"," 20%|█▉        | 235487/1193514 [00:21<01:25, 11140.43it/s]\u001b[A\n"," 20%|█▉        | 236622/1193514 [00:22<01:25, 11198.62it/s]\u001b[A\n"," 20%|█▉        | 237776/1193514 [00:22<01:24, 11298.06it/s]\u001b[A\n"," 20%|██        | 238913/1193514 [00:22<01:24, 11318.39it/s]\u001b[A\n"," 20%|██        | 240046/1193514 [00:22<01:24, 11249.61it/s]\u001b[A\n"," 20%|██        | 241173/1193514 [00:22<01:24, 11255.69it/s]\u001b[A\n"," 20%|██        | 242309/1193514 [00:22<01:24, 11285.61it/s]\u001b[A\n"," 20%|██        | 243438/1193514 [00:22<01:24, 11231.78it/s]\u001b[A\n"," 20%|██        | 244562/1193514 [00:22<01:27, 10807.69it/s]\u001b[A\n"," 21%|██        | 245689/1193514 [00:22<01:26, 10940.84it/s]\u001b[A\n"," 21%|██        | 246786/1193514 [00:22<01:27, 10864.47it/s]\u001b[A\n"," 21%|██        | 247942/1193514 [00:23<01:25, 11061.54it/s]\u001b[A\n"," 21%|██        | 249084/1193514 [00:23<01:24, 11164.23it/s]\u001b[A\n"," 21%|██        | 250203/1193514 [00:23<01:24, 11154.45it/s]\u001b[A\n"," 21%|██        | 251331/1193514 [00:23<01:24, 11190.17it/s]\u001b[A\n"," 21%|██        | 252471/1193514 [00:23<01:23, 11251.49it/s]\u001b[A\n"," 21%|██▏       | 253625/1193514 [00:23<01:22, 11335.03it/s]\u001b[A\n"," 21%|██▏       | 254760/1193514 [00:23<01:23, 11292.90it/s]\u001b[A\n"," 21%|██▏       | 255890/1193514 [00:23<01:23, 11210.03it/s]\u001b[A\n"," 22%|██▏       | 257042/1193514 [00:23<01:22, 11298.97it/s]\u001b[A\n"," 22%|██▏       | 258173/1193514 [00:23<01:24, 11095.97it/s]\u001b[A\n"," 22%|██▏       | 259327/1193514 [00:24<01:23, 11223.16it/s]\u001b[A\n"," 22%|██▏       | 260493/1193514 [00:24<01:22, 11350.41it/s]\u001b[A\n"," 22%|██▏       | 261630/1193514 [00:24<01:23, 11188.15it/s]\u001b[A\n"," 22%|██▏       | 262751/1193514 [00:24<01:24, 11024.18it/s]\u001b[A\n"," 22%|██▏       | 263895/1193514 [00:24<01:23, 11144.59it/s]\u001b[A\n"," 22%|██▏       | 265011/1193514 [00:24<01:23, 11123.30it/s]\u001b[A\n"," 22%|██▏       | 266125/1193514 [00:24<01:23, 11055.40it/s]\u001b[A\n"," 22%|██▏       | 267269/1193514 [00:24<01:22, 11167.21it/s]\u001b[A\n"," 22%|██▏       | 268387/1193514 [00:24<01:23, 11048.24it/s]\u001b[A\n"," 23%|██▎       | 269510/1193514 [00:24<01:23, 11099.69it/s]\u001b[A\n"," 23%|██▎       | 270638/1193514 [00:25<01:22, 11151.61it/s]\u001b[A\n"," 23%|██▎       | 271791/1193514 [00:25<01:21, 11261.24it/s]\u001b[A\n"," 23%|██▎       | 272918/1193514 [00:25<01:23, 10983.41it/s]\u001b[A\n"," 23%|██▎       | 274019/1193514 [00:25<01:26, 10686.57it/s]\u001b[A\n"," 23%|██▎       | 275091/1193514 [00:25<01:29, 10262.70it/s]\u001b[A\n"," 23%|██▎       | 276124/1193514 [00:25<01:31, 10003.16it/s]\u001b[A\n"," 23%|██▎       | 277130/1193514 [00:25<01:32, 9903.35it/s] \u001b[A\n"," 23%|██▎       | 278125/1193514 [00:25<01:32, 9854.62it/s]\u001b[A\n"," 23%|██▎       | 279114/1193514 [00:25<01:33, 9746.87it/s]\u001b[A\n"," 23%|██▎       | 280091/1193514 [00:26<01:34, 9670.66it/s]\u001b[A\n"," 24%|██▎       | 281060/1193514 [00:26<01:34, 9640.13it/s]\u001b[A\n"," 24%|██▎       | 282038/1193514 [00:26<01:34, 9677.66it/s]\u001b[A\n"," 24%|██▎       | 283099/1193514 [00:26<01:31, 9938.72it/s]\u001b[A\n"," 24%|██▍       | 284096/1193514 [00:26<01:33, 9723.96it/s]\u001b[A\n"," 24%|██▍       | 285178/1193514 [00:26<01:30, 10026.78it/s]\u001b[A\n"," 24%|██▍       | 286242/1193514 [00:26<01:28, 10202.87it/s]\u001b[A\n"," 24%|██▍       | 287288/1193514 [00:26<01:28, 10276.44it/s]\u001b[A\n"," 24%|██▍       | 288414/1193514 [00:26<01:25, 10552.33it/s]\u001b[A\n"," 24%|██▍       | 289506/1193514 [00:26<01:24, 10658.21it/s]\u001b[A\n"," 24%|██▍       | 290581/1193514 [00:27<01:24, 10685.22it/s]\u001b[A\n"," 24%|██▍       | 291691/1193514 [00:27<01:23, 10804.53it/s]\u001b[A\n"," 25%|██▍       | 292825/1193514 [00:27<01:22, 10958.96it/s]\u001b[A\n"," 25%|██▍       | 293967/1193514 [00:27<01:21, 11092.68it/s]\u001b[A\n"," 25%|██▍       | 295078/1193514 [00:27<01:21, 11038.98it/s]\u001b[A\n"," 25%|██▍       | 296209/1193514 [00:27<01:20, 11118.83it/s]\u001b[A\n"," 25%|██▍       | 297322/1193514 [00:27<01:21, 11043.30it/s]\u001b[A\n"," 25%|██▌       | 298428/1193514 [00:27<01:24, 10567.48it/s]\u001b[A\n"," 25%|██▌       | 299490/1193514 [00:27<01:27, 10246.75it/s]\u001b[A\n"," 25%|██▌       | 300521/1193514 [00:27<01:31, 9762.82it/s] \u001b[A\n"," 25%|██▌       | 301590/1193514 [00:28<01:28, 10022.81it/s]\u001b[A\n"," 25%|██▌       | 302718/1193514 [00:28<01:25, 10367.33it/s]\u001b[A\n"," 25%|██▌       | 303877/1193514 [00:28<01:23, 10705.86it/s]\u001b[A\n"," 26%|██▌       | 304994/1193514 [00:28<01:21, 10839.39it/s]\u001b[A\n"," 26%|██▌       | 306115/1193514 [00:28<01:21, 10946.90it/s]\u001b[A\n"," 26%|██▌       | 307259/1193514 [00:28<01:19, 11090.05it/s]\u001b[A\n"," 26%|██▌       | 308372/1193514 [00:28<01:20, 11050.95it/s]\u001b[A\n"," 26%|██▌       | 309480/1193514 [00:28<01:22, 10712.36it/s]\u001b[A\n"," 26%|██▌       | 310556/1193514 [00:28<01:25, 10370.07it/s]\u001b[A\n"," 26%|██▌       | 311599/1193514 [00:29<01:28, 9934.09it/s] \u001b[A\n"," 26%|██▌       | 312601/1193514 [00:29<01:29, 9861.19it/s]\u001b[A\n"," 26%|██▋       | 313672/1193514 [00:29<01:27, 10098.43it/s]\u001b[A\n"," 26%|██▋       | 314724/1193514 [00:29<01:25, 10221.23it/s]\u001b[A\n"," 26%|██▋       | 315833/1193514 [00:29<01:23, 10465.14it/s]\u001b[A\n"," 27%|██▋       | 316884/1193514 [00:29<01:23, 10468.17it/s]\u001b[A\n"," 27%|██▋       | 317993/1193514 [00:29<01:22, 10646.53it/s]\u001b[A\n"," 27%|██▋       | 319093/1193514 [00:29<01:21, 10748.95it/s]\u001b[A\n"," 27%|██▋       | 320260/1193514 [00:29<01:19, 11009.49it/s]\u001b[A\n"," 27%|██▋       | 321428/1193514 [00:29<01:17, 11200.25it/s]\u001b[A\n"," 27%|██▋       | 322552/1193514 [00:30<01:20, 10829.69it/s]\u001b[A\n"," 27%|██▋       | 323690/1193514 [00:30<01:19, 10987.54it/s]\u001b[A\n"," 27%|██▋       | 324810/1193514 [00:30<01:18, 11048.73it/s]\u001b[A\n"," 27%|██▋       | 325968/1193514 [00:30<01:17, 11200.47it/s]\u001b[A\n"," 27%|██▋       | 327101/1193514 [00:30<01:17, 11238.54it/s]\u001b[A\n"," 28%|██▊       | 328227/1193514 [00:30<01:17, 11149.57it/s]\u001b[A\n"," 28%|██▊       | 329344/1193514 [00:30<01:20, 10678.67it/s]\u001b[A\n"," 28%|██▊       | 330418/1193514 [00:30<01:23, 10294.76it/s]\u001b[A\n"," 28%|██▊       | 331455/1193514 [00:30<01:25, 10113.99it/s]\u001b[A\n"," 28%|██▊       | 332472/1193514 [00:30<01:26, 9966.99it/s] \u001b[A\n"," 28%|██▊       | 333473/1193514 [00:31<01:29, 9640.94it/s]\u001b[A\n"," 28%|██▊       | 334443/1193514 [00:31<01:29, 9598.00it/s]\u001b[A\n"," 28%|██▊       | 335407/1193514 [00:31<01:30, 9511.65it/s]\u001b[A\n"," 28%|██▊       | 336374/1193514 [00:31<01:29, 9555.46it/s]\u001b[A\n"," 28%|██▊       | 337511/1193514 [00:31<01:25, 10035.49it/s]\u001b[A\n"," 28%|██▊       | 338641/1193514 [00:31<01:22, 10381.13it/s]\u001b[A\n"," 28%|██▊       | 339688/1193514 [00:31<01:23, 10247.19it/s]\u001b[A\n"," 29%|██▊       | 340720/1193514 [00:31<01:25, 9985.66it/s] \u001b[A\n"," 29%|██▊       | 341725/1193514 [00:31<01:26, 9895.18it/s]\u001b[A\n"," 29%|██▊       | 342719/1193514 [00:32<01:27, 9701.65it/s]\u001b[A\n"," 29%|██▉       | 343694/1193514 [00:32<01:27, 9695.14it/s]\u001b[A\n"," 29%|██▉       | 344667/1193514 [00:32<01:27, 9692.74it/s]\u001b[A\n"," 29%|██▉       | 345639/1193514 [00:32<01:27, 9650.87it/s]\u001b[A\n"," 29%|██▉       | 346606/1193514 [00:32<01:27, 9641.78it/s]\u001b[A\n"," 29%|██▉       | 347572/1193514 [00:32<01:28, 9566.84it/s]\u001b[A\n"," 29%|██▉       | 348530/1193514 [00:32<01:28, 9513.83it/s]\u001b[A\n"," 29%|██▉       | 349483/1193514 [00:32<01:29, 9389.69it/s]\u001b[A\n"," 29%|██▉       | 350423/1193514 [00:32<01:30, 9330.11it/s]\u001b[A\n"," 29%|██▉       | 351374/1193514 [00:32<01:29, 9381.29it/s]\u001b[A\n"," 30%|██▉       | 352313/1193514 [00:33<01:30, 9340.91it/s]\u001b[A\n"," 30%|██▉       | 353258/1193514 [00:33<01:29, 9371.75it/s]\u001b[A\n"," 30%|██▉       | 354282/1193514 [00:33<01:27, 9615.91it/s]\u001b[A\n"," 30%|██▉       | 355398/1193514 [00:33<01:23, 10031.04it/s]\u001b[A\n"," 30%|██▉       | 356499/1193514 [00:33<01:21, 10304.89it/s]\u001b[A\n"," 30%|██▉       | 357640/1193514 [00:33<01:18, 10612.75it/s]\u001b[A\n"," 30%|███       | 358730/1193514 [00:33<01:18, 10696.16it/s]\u001b[A\n"," 30%|███       | 359812/1193514 [00:33<01:17, 10731.28it/s]\u001b[A\n"," 30%|███       | 360906/1193514 [00:33<01:17, 10793.04it/s]\u001b[A\n"," 30%|███       | 362031/1193514 [00:33<01:16, 10923.96it/s]\u001b[A\n"," 30%|███       | 363126/1193514 [00:34<01:16, 10899.68it/s]\u001b[A\n"," 31%|███       | 364224/1193514 [00:34<01:15, 10922.19it/s]\u001b[A\n"," 31%|███       | 365318/1193514 [00:34<01:18, 10519.83it/s]\u001b[A\n"," 31%|███       | 366431/1193514 [00:34<01:17, 10694.10it/s]\u001b[A\n"," 31%|███       | 367504/1193514 [00:34<01:19, 10357.57it/s]\u001b[A\n"," 31%|███       | 368633/1193514 [00:34<01:17, 10618.56it/s]\u001b[A\n"," 31%|███       | 369722/1193514 [00:34<01:17, 10697.21it/s]\u001b[A\n"," 31%|███       | 370842/1193514 [00:34<01:15, 10840.75it/s]\u001b[A\n"," 31%|███       | 371938/1193514 [00:34<01:15, 10875.69it/s]\u001b[A\n"," 31%|███▏      | 373098/1193514 [00:34<01:14, 11082.73it/s]\u001b[A\n"," 31%|███▏      | 374209/1193514 [00:35<01:15, 10863.32it/s]\u001b[A\n"," 31%|███▏      | 375299/1193514 [00:35<01:17, 10492.52it/s]\u001b[A\n"," 32%|███▏      | 376388/1193514 [00:35<01:17, 10606.57it/s]\u001b[A\n"," 32%|███▏      | 377517/1193514 [00:35<01:15, 10802.03it/s]\u001b[A\n"," 32%|███▏      | 378601/1193514 [00:35<01:16, 10624.93it/s]\u001b[A\n"," 32%|███▏      | 379694/1193514 [00:35<01:15, 10713.62it/s]\u001b[A\n"," 32%|███▏      | 380796/1193514 [00:35<01:15, 10801.86it/s]\u001b[A\n"," 32%|███▏      | 381878/1193514 [00:35<01:15, 10682.48it/s]\u001b[A\n"," 32%|███▏      | 382948/1193514 [00:35<01:15, 10681.33it/s]\u001b[A\n"," 32%|███▏      | 384018/1193514 [00:36<01:16, 10630.70it/s]\u001b[A\n"," 32%|███▏      | 385086/1193514 [00:36<01:15, 10641.66it/s]\u001b[A\n"," 32%|███▏      | 386204/1193514 [00:36<01:14, 10795.59it/s]\u001b[A\n"," 32%|███▏      | 387311/1193514 [00:36<01:14, 10875.11it/s]\u001b[A\n"," 33%|███▎      | 388400/1193514 [00:36<01:16, 10591.29it/s]\u001b[A\n"," 33%|███▎      | 389462/1193514 [00:36<01:21, 9843.04it/s] \u001b[A\n"," 33%|███▎      | 390585/1193514 [00:36<01:18, 10221.08it/s]\u001b[A\n"," 33%|███▎      | 391620/1193514 [00:36<01:20, 10009.36it/s]\u001b[A\n"," 33%|███▎      | 392631/1193514 [00:36<01:21, 9820.41it/s] \u001b[A\n"," 33%|███▎      | 393727/1193514 [00:36<01:18, 10134.82it/s]\u001b[A\n"," 33%|███▎      | 394847/1193514 [00:37<01:16, 10429.94it/s]\u001b[A\n"," 33%|███▎      | 395937/1193514 [00:37<01:15, 10566.42it/s]\u001b[A\n"," 33%|███▎      | 397046/1193514 [00:37<01:14, 10716.54it/s]\u001b[A\n"," 33%|███▎      | 398123/1193514 [00:37<01:17, 10321.20it/s]\u001b[A\n"," 33%|███▎      | 399162/1193514 [00:37<01:16, 10340.77it/s]\u001b[A\n"," 34%|███▎      | 400282/1193514 [00:37<01:14, 10584.06it/s]\u001b[A\n"," 34%|███▎      | 401395/1193514 [00:37<01:13, 10740.56it/s]\u001b[A\n"," 34%|███▎      | 402530/1193514 [00:37<01:12, 10916.07it/s]\u001b[A\n"," 34%|███▍      | 403625/1193514 [00:37<01:13, 10731.58it/s]\u001b[A\n"," 34%|███▍      | 404737/1193514 [00:37<01:12, 10844.81it/s]\u001b[A\n"," 34%|███▍      | 405824/1193514 [00:38<01:13, 10748.14it/s]\u001b[A\n"," 34%|███▍      | 406915/1193514 [00:38<01:12, 10794.58it/s]\u001b[A\n"," 34%|███▍      | 407996/1193514 [00:38<01:13, 10639.66it/s]\u001b[A\n"," 34%|███▍      | 409062/1193514 [00:38<01:14, 10563.87it/s]\u001b[A\n"," 34%|███▍      | 410124/1193514 [00:38<01:14, 10579.68it/s]\u001b[A\n"," 34%|███▍      | 411220/1193514 [00:38<01:13, 10688.40it/s]\u001b[A\n"," 35%|███▍      | 412368/1193514 [00:38<01:11, 10912.20it/s]\u001b[A\n"," 35%|███▍      | 413479/1193514 [00:38<01:11, 10970.17it/s]\u001b[A\n"," 35%|███▍      | 414578/1193514 [00:38<01:12, 10712.18it/s]\u001b[A\n"," 35%|███▍      | 415652/1193514 [00:38<01:12, 10696.53it/s]\u001b[A\n"," 35%|███▍      | 416724/1193514 [00:39<01:12, 10659.99it/s]\u001b[A\n"," 35%|███▌      | 417809/1193514 [00:39<01:12, 10714.93it/s]\u001b[A\n"," 35%|███▌      | 418910/1193514 [00:39<01:11, 10800.87it/s]\u001b[A\n"," 35%|███▌      | 419991/1193514 [00:39<01:13, 10517.30it/s]\u001b[A\n"," 35%|███▌      | 421045/1193514 [00:39<01:16, 10153.03it/s]\u001b[A\n"," 35%|███▌      | 422065/1193514 [00:39<01:18, 9851.03it/s] \u001b[A\n"," 35%|███▌      | 423056/1193514 [00:39<01:19, 9739.53it/s]\u001b[A\n"," 36%|███▌      | 424034/1193514 [00:39<01:20, 9609.46it/s]\u001b[A\n"," 36%|███▌      | 424998/1193514 [00:39<01:22, 9274.88it/s]\u001b[A\n"," 36%|███▌      | 425931/1193514 [00:40<01:24, 9064.15it/s]\u001b[A\n"," 36%|███▌      | 426855/1193514 [00:40<01:24, 9111.89it/s]\u001b[A\n"," 36%|███▌      | 427950/1193514 [00:40<01:19, 9593.17it/s]\u001b[A\n"," 36%|███▌      | 429070/1193514 [00:40<01:16, 10023.87it/s]\u001b[A\n"," 36%|███▌      | 430133/1193514 [00:40<01:14, 10193.10it/s]\u001b[A\n"," 36%|███▌      | 431161/1193514 [00:40<01:16, 9971.88it/s] \u001b[A\n"," 36%|███▌      | 432193/1193514 [00:40<01:15, 10073.54it/s]\u001b[A\n"," 36%|███▋      | 433322/1193514 [00:40<01:13, 10408.30it/s]\u001b[A\n"," 36%|███▋      | 434458/1193514 [00:40<01:11, 10675.39it/s]\u001b[A\n"," 36%|███▋      | 435532/1193514 [00:40<01:12, 10475.19it/s]\u001b[A\n"," 37%|███▋      | 436585/1193514 [00:41<01:15, 10074.75it/s]\u001b[A\n"," 37%|███▋      | 437600/1193514 [00:41<01:19, 9519.56it/s] \u001b[A\n"," 37%|███▋      | 438669/1193514 [00:41<01:16, 9840.63it/s]\u001b[A\n"," 37%|███▋      | 439788/1193514 [00:41<01:13, 10208.93it/s]\u001b[A\n"," 37%|███▋      | 440895/1193514 [00:41<01:12, 10452.74it/s]\u001b[A\n"," 37%|███▋      | 441950/1193514 [00:41<01:15, 10013.55it/s]\u001b[A\n"," 37%|███▋      | 443043/1193514 [00:41<01:13, 10271.64it/s]\u001b[A\n"," 37%|███▋      | 444080/1193514 [00:41<01:15, 9985.74it/s] \u001b[A\n"," 37%|███▋      | 445087/1193514 [00:41<01:16, 9755.22it/s]\u001b[A\n"," 37%|███▋      | 446070/1193514 [00:42<01:17, 9665.74it/s]\u001b[A\n"," 37%|███▋      | 447042/1193514 [00:42<01:18, 9486.77it/s]\u001b[A\n"," 38%|███▊      | 448168/1193514 [00:42<01:14, 9955.11it/s]\u001b[A\n"," 38%|███▊      | 449173/1193514 [00:42<01:14, 9929.01it/s]\u001b[A\n"," 38%|███▊      | 450303/1193514 [00:42<01:12, 10300.23it/s]\u001b[A\n"," 38%|███▊      | 451441/1193514 [00:42<01:09, 10601.30it/s]\u001b[A\n"," 38%|███▊      | 452547/1193514 [00:42<01:09, 10731.63it/s]\u001b[A\n"," 38%|███▊      | 453673/1193514 [00:42<01:07, 10883.50it/s]\u001b[A\n"," 38%|███▊      | 454812/1193514 [00:42<01:06, 11030.45it/s]\u001b[A\n"," 38%|███▊      | 455919/1193514 [00:42<01:07, 10952.43it/s]\u001b[A\n"," 38%|███▊      | 457068/1193514 [00:43<01:06, 11106.01it/s]\u001b[A\n"," 38%|███▊      | 458182/1193514 [00:43<01:07, 10948.00it/s]\u001b[A\n"," 38%|███▊      | 459280/1193514 [00:43<01:07, 10955.58it/s]\u001b[A\n"," 39%|███▊      | 460378/1193514 [00:43<01:09, 10611.90it/s]\u001b[A\n"," 39%|███▊      | 461543/1193514 [00:43<01:07, 10903.32it/s]\u001b[A\n"," 39%|███▉      | 462714/1193514 [00:43<01:05, 11132.80it/s]\u001b[A\n"," 39%|███▉      | 463832/1193514 [00:43<01:06, 10924.72it/s]\u001b[A\n"," 39%|███▉      | 464971/1193514 [00:43<01:05, 11059.40it/s]\u001b[A\n"," 39%|███▉      | 466081/1193514 [00:43<01:06, 11008.71it/s]\u001b[A\n"," 39%|███▉      | 467208/1193514 [00:43<01:05, 11085.14it/s]\u001b[A\n"," 39%|███▉      | 468319/1193514 [00:44<01:08, 10554.15it/s]\u001b[A\n"," 39%|███▉      | 469381/1193514 [00:44<01:09, 10489.62it/s]\u001b[A\n"," 39%|███▉      | 470518/1193514 [00:44<01:07, 10738.78it/s]\u001b[A\n"," 40%|███▉      | 471597/1193514 [00:44<01:08, 10481.48it/s]\u001b[A\n"," 40%|███▉      | 472650/1193514 [00:44<01:10, 10220.94it/s]\u001b[A\n"," 40%|███▉      | 473677/1193514 [00:44<01:11, 10076.13it/s]\u001b[A\n"," 40%|███▉      | 474689/1193514 [00:44<01:12, 9870.45it/s] \u001b[A\n"," 40%|███▉      | 475680/1193514 [00:44<01:14, 9593.06it/s]\u001b[A\n"," 40%|███▉      | 476644/1193514 [00:44<01:14, 9590.98it/s]\u001b[A\n"," 40%|████      | 477607/1193514 [00:45<01:16, 9378.43it/s]\u001b[A\n"," 40%|████      | 478573/1193514 [00:45<01:15, 9459.97it/s]\u001b[A\n"," 40%|████      | 479522/1193514 [00:45<01:15, 9447.05it/s]\u001b[A\n"," 40%|████      | 480614/1193514 [00:45<01:12, 9844.38it/s]\u001b[A\n"," 40%|████      | 481725/1193514 [00:45<01:09, 10191.21it/s]\u001b[A\n"," 40%|████      | 482852/1193514 [00:45<01:07, 10492.26it/s]\u001b[A\n"," 41%|████      | 483988/1193514 [00:45<01:06, 10737.77it/s]\u001b[A\n"," 41%|████      | 485084/1193514 [00:45<01:05, 10803.44it/s]\u001b[A\n"," 41%|████      | 486169/1193514 [00:45<01:07, 10472.26it/s]\u001b[A\n"," 41%|████      | 487222/1193514 [00:45<01:07, 10422.28it/s]\u001b[A\n"," 41%|████      | 488314/1193514 [00:46<01:06, 10564.11it/s]\u001b[A\n"," 41%|████      | 489452/1193514 [00:46<01:05, 10795.94it/s]\u001b[A\n"," 41%|████      | 490536/1193514 [00:46<01:06, 10581.67it/s]\u001b[A\n"," 41%|████      | 491598/1193514 [00:46<01:06, 10525.34it/s]\u001b[A\n"," 41%|████▏     | 492725/1193514 [00:46<01:05, 10737.26it/s]\u001b[A\n"," 41%|████▏     | 493889/1193514 [00:46<01:03, 10990.63it/s]\u001b[A\n"," 41%|████▏     | 494992/1193514 [00:46<01:05, 10613.61it/s]\u001b[A\n"," 42%|████▏     | 496059/1193514 [00:46<01:07, 10305.58it/s]\u001b[A\n"," 42%|████▏     | 497175/1193514 [00:46<01:06, 10544.91it/s]\u001b[A\n"," 42%|████▏     | 498316/1193514 [00:47<01:04, 10789.04it/s]\u001b[A\n"," 42%|████▏     | 499411/1193514 [00:47<01:04, 10835.13it/s]\u001b[A\n"," 42%|████▏     | 500499/1193514 [00:47<01:04, 10670.22it/s]\u001b[A\n"," 42%|████▏     | 501579/1193514 [00:47<01:04, 10706.38it/s]\u001b[A\n"," 42%|████▏     | 502652/1193514 [00:47<01:05, 10593.73it/s]\u001b[A\n"," 42%|████▏     | 503753/1193514 [00:47<01:04, 10714.22it/s]\u001b[A\n"," 42%|████▏     | 504890/1193514 [00:47<01:03, 10902.82it/s]\u001b[A\n"," 42%|████▏     | 506012/1193514 [00:47<01:02, 10993.84it/s]\u001b[A\n"," 42%|████▏     | 507113/1193514 [00:47<01:02, 10973.53it/s]\u001b[A\n"," 43%|████▎     | 508246/1193514 [00:47<01:01, 11076.98it/s]\u001b[A\n"," 43%|████▎     | 509385/1193514 [00:48<01:01, 11165.74it/s]\u001b[A\n"," 43%|████▎     | 510503/1193514 [00:48<01:01, 11031.45it/s]\u001b[A\n"," 43%|████▎     | 511610/1193514 [00:48<01:01, 11040.67it/s]\u001b[A\n"," 43%|████▎     | 512752/1193514 [00:48<01:01, 11146.79it/s]\u001b[A\n"," 43%|████▎     | 513884/1193514 [00:48<01:00, 11197.30it/s]\u001b[A\n"," 43%|████▎     | 515005/1193514 [00:48<01:03, 10636.22it/s]\u001b[A\n"," 43%|████▎     | 516143/1193514 [00:48<01:02, 10848.22it/s]\u001b[A\n"," 43%|████▎     | 517234/1193514 [00:48<01:04, 10531.87it/s]\u001b[A\n"," 43%|████▎     | 518344/1193514 [00:48<01:03, 10693.48it/s]\u001b[A\n"," 44%|████▎     | 519462/1193514 [00:48<01:02, 10832.90it/s]\u001b[A\n"," 44%|████▎     | 520593/1193514 [00:49<01:01, 10967.82it/s]\u001b[A\n"," 44%|████▎     | 521693/1193514 [00:49<01:02, 10747.46it/s]\u001b[A\n"," 44%|████▍     | 522771/1193514 [00:49<01:02, 10713.64it/s]\u001b[A\n"," 44%|████▍     | 523845/1193514 [00:49<01:03, 10474.07it/s]\u001b[A\n"," 44%|████▍     | 524896/1193514 [00:49<01:05, 10212.91it/s]\u001b[A\n"," 44%|████▍     | 525921/1193514 [00:49<01:07, 9963.58it/s] \u001b[A\n"," 44%|████▍     | 526922/1193514 [00:49<01:07, 9871.76it/s]\u001b[A\n"," 44%|████▍     | 527912/1193514 [00:49<01:07, 9859.34it/s]\u001b[A\n"," 44%|████▍     | 528903/1193514 [00:49<01:07, 9868.95it/s]\u001b[A\n"," 44%|████▍     | 530026/1193514 [00:49<01:04, 10239.96it/s]\u001b[A\n"," 44%|████▍     | 531055/1193514 [00:50<01:06, 9957.60it/s] \u001b[A\n"," 45%|████▍     | 532056/1193514 [00:50<01:07, 9784.32it/s]\u001b[A\n"," 45%|████▍     | 533145/1193514 [00:50<01:05, 10091.57it/s]\u001b[A\n"," 45%|████▍     | 534188/1193514 [00:50<01:04, 10189.69it/s]\u001b[A\n"," 45%|████▍     | 535312/1193514 [00:50<01:02, 10482.41it/s]\u001b[A\n"," 45%|████▍     | 536417/1193514 [00:50<01:01, 10643.49it/s]\u001b[A\n"," 45%|████▌     | 537486/1193514 [00:50<01:03, 10395.51it/s]\u001b[A\n"," 45%|████▌     | 538594/1193514 [00:50<01:01, 10590.61it/s]\u001b[A\n"," 45%|████▌     | 539751/1193514 [00:50<01:00, 10866.20it/s]\u001b[A\n"," 45%|████▌     | 540843/1193514 [00:51<01:01, 10579.73it/s]\u001b[A\n"," 45%|████▌     | 541906/1193514 [00:51<01:04, 10095.77it/s]\u001b[A\n"," 45%|████▌     | 543014/1193514 [00:51<01:02, 10371.51it/s]\u001b[A\n"," 46%|████▌     | 544158/1193514 [00:51<01:00, 10669.00it/s]\u001b[A\n"," 46%|████▌     | 545286/1193514 [00:51<00:59, 10843.47it/s]\u001b[A\n"," 46%|████▌     | 546392/1193514 [00:51<00:59, 10905.74it/s]\u001b[A\n"," 46%|████▌     | 547536/1193514 [00:51<00:58, 11060.60it/s]\u001b[A\n"," 46%|████▌     | 548677/1193514 [00:51<00:57, 11160.95it/s]\u001b[A\n"," 46%|████▌     | 549819/1193514 [00:51<00:57, 11236.00it/s]\u001b[A\n"," 46%|████▌     | 550976/1193514 [00:51<00:56, 11334.12it/s]\u001b[A\n"," 46%|████▋     | 552111/1193514 [00:52<00:57, 11125.30it/s]\u001b[A\n"," 46%|████▋     | 553226/1193514 [00:52<00:57, 11074.45it/s]\u001b[A\n"," 46%|████▋     | 554335/1193514 [00:52<00:57, 11068.62it/s]\u001b[A\n"," 47%|████▋     | 555468/1193514 [00:52<00:57, 11144.09it/s]\u001b[A\n"," 47%|████▋     | 556592/1193514 [00:52<00:57, 11170.88it/s]\u001b[A\n"," 47%|████▋     | 557710/1193514 [00:52<00:57, 11142.25it/s]\u001b[A\n"," 47%|████▋     | 558841/1193514 [00:52<00:56, 11188.54it/s]\u001b[A\n"," 47%|████▋     | 559961/1193514 [00:52<00:56, 11186.09it/s]\u001b[A\n"," 47%|████▋     | 561080/1193514 [00:52<00:57, 10973.06it/s]\u001b[A\n"," 47%|████▋     | 562236/1193514 [00:52<00:56, 11141.46it/s]\u001b[A\n"," 47%|████▋     | 563374/1193514 [00:53<00:56, 11210.91it/s]\u001b[A\n"," 47%|████▋     | 564497/1193514 [00:53<00:58, 10730.64it/s]\u001b[A\n"," 47%|████▋     | 565576/1193514 [00:53<00:58, 10670.89it/s]\u001b[A\n"," 47%|████▋     | 566696/1193514 [00:53<00:57, 10823.53it/s]\u001b[A\n"," 48%|████▊     | 567828/1193514 [00:53<00:57, 10966.76it/s]\u001b[A\n"," 48%|████▊     | 568962/1193514 [00:53<00:56, 11075.15it/s]\u001b[A\n"," 48%|████▊     | 570072/1193514 [00:53<00:56, 11070.49it/s]\u001b[A\n"," 48%|████▊     | 571181/1193514 [00:53<00:56, 11074.45it/s]\u001b[A\n"," 48%|████▊     | 572309/1193514 [00:53<00:55, 11133.76it/s]\u001b[A\n"," 48%|████▊     | 573425/1193514 [00:53<00:55, 11140.27it/s]\u001b[A\n"," 48%|████▊     | 574582/1193514 [00:54<00:54, 11265.77it/s]\u001b[A\n"," 48%|████▊     | 575710/1193514 [00:54<00:56, 11008.45it/s]\u001b[A\n"," 48%|████▊     | 576813/1193514 [00:54<00:56, 10855.29it/s]\u001b[A\n"," 48%|████▊     | 577941/1193514 [00:54<00:56, 10978.81it/s]\u001b[A\n"," 49%|████▊     | 579057/1193514 [00:54<00:55, 11031.99it/s]\u001b[A\n"," 49%|████▊     | 580182/1193514 [00:54<00:55, 11094.71it/s]\u001b[A\n"," 49%|████▊     | 581293/1193514 [00:54<00:55, 11066.87it/s]\u001b[A\n"," 49%|████▉     | 582401/1193514 [00:54<00:55, 10996.26it/s]\u001b[A\n"," 49%|████▉     | 583523/1193514 [00:54<00:55, 11060.73it/s]\u001b[A\n"," 49%|████▉     | 584654/1193514 [00:54<00:54, 11133.67it/s]\u001b[A\n"," 49%|████▉     | 585786/1193514 [00:55<00:54, 11187.31it/s]\u001b[A\n"," 49%|████▉     | 586906/1193514 [00:55<00:55, 10972.29it/s]\u001b[A\n"," 49%|████▉     | 588005/1193514 [00:55<00:56, 10634.26it/s]\u001b[A\n"," 49%|████▉     | 589072/1193514 [00:55<00:57, 10580.91it/s]\u001b[A\n"," 49%|████▉     | 590205/1193514 [00:55<00:55, 10791.28it/s]\u001b[A\n"," 50%|████▉     | 591349/1193514 [00:55<00:54, 10976.51it/s]\u001b[A\n"," 50%|████▉     | 592488/1193514 [00:55<00:54, 11095.97it/s]\u001b[A\n"," 50%|████▉     | 593604/1193514 [00:55<00:53, 11115.00it/s]\u001b[A\n"," 50%|████▉     | 594732/1193514 [00:55<00:53, 11163.95it/s]\u001b[A\n"," 50%|████▉     | 595878/1193514 [00:56<00:53, 11250.71it/s]\u001b[A\n"," 50%|█████     | 597010/1193514 [00:56<00:52, 11271.33it/s]\u001b[A\n"," 50%|█████     | 598138/1193514 [00:56<00:54, 10946.21it/s]\u001b[A\n"," 50%|█████     | 599236/1193514 [00:56<00:54, 10950.15it/s]\u001b[A\n"," 50%|█████     | 600333/1193514 [00:56<00:54, 10939.35it/s]\u001b[A\n"," 50%|█████     | 601429/1193514 [00:56<00:54, 10830.19it/s]\u001b[A\n"," 50%|█████     | 602543/1193514 [00:56<00:54, 10918.98it/s]\u001b[A\n"," 51%|█████     | 603678/1193514 [00:56<00:53, 11042.32it/s]\u001b[A\n"," 51%|█████     | 604784/1193514 [00:56<00:55, 10678.06it/s]\u001b[A\n"," 51%|█████     | 605877/1193514 [00:56<00:54, 10751.45it/s]\u001b[A\n"," 51%|█████     | 606971/1193514 [00:57<00:54, 10806.53it/s]\u001b[A\n"," 51%|█████     | 608116/1193514 [00:57<00:53, 10989.49it/s]\u001b[A\n"," 51%|█████     | 609217/1193514 [00:57<00:53, 10992.50it/s]\u001b[A\n"," 51%|█████     | 610318/1193514 [00:57<00:53, 10937.08it/s]\u001b[A\n"," 51%|█████     | 611413/1193514 [00:57<00:53, 10912.44it/s]\u001b[A\n"," 51%|█████▏    | 612573/1193514 [00:57<00:52, 11109.19it/s]\u001b[A\n"," 51%|█████▏    | 613691/1193514 [00:57<00:52, 11130.33it/s]\u001b[A\n"," 52%|█████▏    | 614855/1193514 [00:57<00:51, 11275.72it/s]\u001b[A\n"," 52%|█████▏    | 615984/1193514 [00:57<00:51, 11261.43it/s]\u001b[A\n"," 52%|█████▏    | 617111/1193514 [00:57<00:51, 11261.78it/s]\u001b[A\n"," 52%|█████▏    | 618267/1193514 [00:58<00:50, 11348.90it/s]\u001b[A\n"," 52%|█████▏    | 619430/1193514 [00:58<00:50, 11429.14it/s]\u001b[A\n"," 52%|█████▏    | 620574/1193514 [00:58<00:50, 11391.40it/s]\u001b[A\n"," 52%|█████▏    | 621714/1193514 [00:58<00:50, 11312.41it/s]\u001b[A\n"," 52%|█████▏    | 622846/1193514 [00:58<00:50, 11260.92it/s]\u001b[A\n"," 52%|█████▏    | 623976/1193514 [00:58<00:50, 11271.70it/s]\u001b[A\n"," 52%|█████▏    | 625117/1193514 [00:58<00:50, 11310.54it/s]\u001b[A\n"," 52%|█████▏    | 626254/1193514 [00:58<00:50, 11327.94it/s]\u001b[A\n"," 53%|█████▎    | 627387/1193514 [00:58<00:50, 11204.51it/s]\u001b[A\n"," 53%|█████▎    | 628508/1193514 [00:58<00:50, 11080.57it/s]\u001b[A\n"," 53%|█████▎    | 629670/1193514 [00:59<00:50, 11234.79it/s]\u001b[A\n"," 53%|█████▎    | 630837/1193514 [00:59<00:49, 11360.45it/s]\u001b[A\n"," 53%|█████▎    | 631975/1193514 [00:59<00:50, 11181.98it/s]\u001b[A\n"," 53%|█████▎    | 633095/1193514 [00:59<00:53, 10517.49it/s]\u001b[A\n"," 53%|█████▎    | 634156/1193514 [00:59<00:54, 10261.36it/s]\u001b[A\n"," 53%|█████▎    | 635190/1193514 [00:59<00:55, 10135.66it/s]\u001b[A\n"," 53%|█████▎    | 636210/1193514 [00:59<00:55, 9979.30it/s] \u001b[A\n"," 53%|█████▎    | 637213/1193514 [00:59<00:56, 9823.29it/s]\u001b[A\n"," 53%|█████▎    | 638200/1193514 [00:59<00:57, 9718.81it/s]\u001b[A\n"," 54%|█████▎    | 639175/1193514 [01:00<00:57, 9661.38it/s]\u001b[A\n"," 54%|█████▎    | 640295/1193514 [01:00<00:54, 10071.62it/s]\u001b[A\n"," 54%|█████▎    | 641357/1193514 [01:00<00:53, 10228.17it/s]\u001b[A\n"," 54%|█████▍    | 642449/1193514 [01:00<00:52, 10424.99it/s]\u001b[A\n"," 54%|█████▍    | 643543/1193514 [01:00<00:52, 10573.52it/s]\u001b[A\n"," 54%|█████▍    | 644659/1193514 [01:00<00:51, 10740.74it/s]\u001b[A\n"," 54%|█████▍    | 645784/1193514 [01:00<00:50, 10886.12it/s]\u001b[A\n"," 54%|█████▍    | 646895/1193514 [01:00<00:49, 10951.86it/s]\u001b[A\n"," 54%|█████▍    | 648020/1193514 [01:00<00:49, 11038.69it/s]\u001b[A\n"," 54%|█████▍    | 649126/1193514 [01:00<00:51, 10583.75it/s]\u001b[A\n"," 54%|█████▍    | 650261/1193514 [01:01<00:50, 10800.31it/s]\u001b[A\n"," 55%|█████▍    | 651346/1193514 [01:01<00:50, 10637.04it/s]\u001b[A\n"," 55%|█████▍    | 652450/1193514 [01:01<00:50, 10754.03it/s]\u001b[A\n"," 55%|█████▍    | 653553/1193514 [01:01<00:49, 10833.98it/s]\u001b[A\n"," 55%|█████▍    | 654639/1193514 [01:01<00:50, 10694.61it/s]\u001b[A\n"," 55%|█████▍    | 655711/1193514 [01:01<00:51, 10395.79it/s]\u001b[A\n"," 55%|█████▌    | 656853/1193514 [01:01<00:50, 10682.40it/s]\u001b[A\n"," 55%|█████▌    | 657949/1193514 [01:01<00:49, 10764.10it/s]\u001b[A\n"," 55%|█████▌    | 659060/1193514 [01:01<00:49, 10862.67it/s]\u001b[A\n"," 55%|█████▌    | 660149/1193514 [01:01<00:50, 10526.31it/s]\u001b[A\n"," 55%|█████▌    | 661206/1193514 [01:02<00:52, 10217.52it/s]\u001b[A\n"," 55%|█████▌    | 662233/1193514 [01:02<00:52, 10143.75it/s]\u001b[A\n"," 56%|█████▌    | 663357/1193514 [01:02<00:50, 10448.66it/s]\u001b[A\n"," 56%|█████▌    | 664414/1193514 [01:02<00:50, 10482.93it/s]\u001b[A\n"," 56%|█████▌    | 665534/1193514 [01:02<00:49, 10687.77it/s]\u001b[A\n"," 56%|█████▌    | 666652/1193514 [01:02<00:48, 10829.62it/s]\u001b[A\n"," 56%|█████▌    | 667800/1193514 [01:02<00:47, 11015.98it/s]\u001b[A\n"," 56%|█████▌    | 668905/1193514 [01:02<00:49, 10553.64it/s]\u001b[A\n"," 56%|█████▌    | 669967/1193514 [01:02<00:50, 10289.61it/s]\u001b[A\n"," 56%|█████▌    | 671123/1193514 [01:02<00:49, 10639.39it/s]\u001b[A\n"," 56%|█████▋    | 672277/1193514 [01:03<00:47, 10894.11it/s]\u001b[A\n"," 56%|█████▋    | 673399/1193514 [01:03<00:47, 10988.65it/s]\u001b[A\n"," 57%|█████▋    | 674503/1193514 [01:03<00:47, 10945.88it/s]\u001b[A\n"," 57%|█████▋    | 675601/1193514 [01:03<00:47, 10914.00it/s]\u001b[A\n"," 57%|█████▋    | 676758/1193514 [01:03<00:46, 11100.42it/s]\u001b[A\n"," 57%|█████▋    | 677871/1193514 [01:03<00:47, 10886.21it/s]\u001b[A\n"," 57%|█████▋    | 678963/1193514 [01:03<00:47, 10864.86it/s]\u001b[A\n"," 57%|█████▋    | 680118/1193514 [01:03<00:46, 11061.19it/s]\u001b[A\n"," 57%|█████▋    | 681255/1193514 [01:03<00:45, 11150.06it/s]\u001b[A\n"," 57%|█████▋    | 682374/1193514 [01:04<00:45, 11159.46it/s]\u001b[A\n"," 57%|█████▋    | 683492/1193514 [01:04<00:46, 11073.12it/s]\u001b[A\n"," 57%|█████▋    | 684609/1193514 [01:04<00:45, 11100.65it/s]\u001b[A\n"," 57%|█████▋    | 685720/1193514 [01:04<00:48, 10447.05it/s]\u001b[A\n"," 58%|█████▊    | 686774/1193514 [01:04<00:50, 10134.24it/s]\u001b[A\n"," 58%|█████▊    | 687796/1193514 [01:04<00:50, 9967.63it/s] \u001b[A\n"," 58%|█████▊    | 688799/1193514 [01:04<00:51, 9868.20it/s]\u001b[A\n"," 58%|█████▊    | 689791/1193514 [01:04<00:51, 9809.40it/s]\u001b[A\n"," 58%|█████▊    | 690908/1193514 [01:04<00:49, 10180.27it/s]\u001b[A\n"," 58%|█████▊    | 692059/1193514 [01:04<00:47, 10544.67it/s]\u001b[A\n"," 58%|█████▊    | 693203/1193514 [01:05<00:46, 10797.09it/s]\u001b[A\n"," 58%|█████▊    | 694318/1193514 [01:05<00:45, 10898.88it/s]\u001b[A\n"," 58%|█████▊    | 695445/1193514 [01:05<00:45, 11007.32it/s]\u001b[A\n"," 58%|█████▊    | 696550/1193514 [01:05<00:47, 10480.42it/s]\u001b[A\n"," 58%|█████▊    | 697680/1193514 [01:05<00:46, 10711.54it/s]\u001b[A\n"," 59%|█████▊    | 698824/1193514 [01:05<00:45, 10920.01it/s]\u001b[A\n"," 59%|█████▊    | 699963/1193514 [01:05<00:44, 11056.85it/s]\u001b[A\n"," 59%|█████▊    | 701074/1193514 [01:05<00:45, 10854.48it/s]\u001b[A\n"," 59%|█████▉    | 702226/1193514 [01:05<00:44, 11044.24it/s]\u001b[A\n"," 59%|█████▉    | 703371/1193514 [01:05<00:43, 11162.49it/s]\u001b[A\n"," 59%|█████▉    | 704505/1193514 [01:06<00:43, 11213.60it/s]\u001b[A\n"," 59%|█████▉    | 705634/1193514 [01:06<00:43, 11234.88it/s]\u001b[A\n"," 59%|█████▉    | 706759/1193514 [01:06<00:43, 11147.61it/s]\u001b[A\n"," 59%|█████▉    | 707875/1193514 [01:06<00:45, 10730.66it/s]\u001b[A\n"," 59%|█████▉    | 709021/1193514 [01:06<00:44, 10937.39it/s]\u001b[A\n"," 59%|█████▉    | 710139/1193514 [01:06<00:43, 11008.60it/s]\u001b[A\n"," 60%|█████▉    | 711251/1193514 [01:06<00:43, 11040.46it/s]\u001b[A\n"," 60%|█████▉    | 712362/1193514 [01:06<00:43, 11060.83it/s]\u001b[A\n"," 60%|█████▉    | 713480/1193514 [01:06<00:43, 11095.32it/s]\u001b[A\n"," 60%|█████▉    | 714650/1193514 [01:06<00:42, 11270.01it/s]\u001b[A\n"," 60%|█████▉    | 715779/1193514 [01:07<00:42, 11171.63it/s]\u001b[A\n"," 60%|██████    | 716935/1193514 [01:07<00:42, 11285.04it/s]\u001b[A\n"," 60%|██████    | 718065/1193514 [01:07<00:42, 11147.29it/s]\u001b[A\n"," 60%|██████    | 719181/1193514 [01:07<00:43, 10910.52it/s]\u001b[A\n"," 60%|██████    | 720312/1193514 [01:07<00:42, 11026.37it/s]\u001b[A\n"," 60%|██████    | 721451/1193514 [01:07<00:42, 11132.18it/s]\u001b[A\n"," 61%|██████    | 722575/1193514 [01:07<00:42, 11163.51it/s]\u001b[A\n"," 61%|██████    | 723693/1193514 [01:07<00:42, 11098.76it/s]\u001b[A\n"," 61%|██████    | 724821/1193514 [01:07<00:42, 11150.63it/s]\u001b[A\n"," 61%|██████    | 725978/1193514 [01:08<00:41, 11270.75it/s]\u001b[A\n"," 61%|██████    | 727128/1193514 [01:08<00:41, 11337.89it/s]\u001b[A\n"," 61%|██████    | 728282/1193514 [01:08<00:40, 11396.10it/s]\u001b[A\n"," 61%|██████    | 729423/1193514 [01:08<00:41, 11235.71it/s]\u001b[A\n"," 61%|██████    | 730548/1193514 [01:08<00:42, 10814.84it/s]\u001b[A\n"," 61%|██████▏   | 731686/1193514 [01:08<00:42, 10974.86it/s]\u001b[A\n"," 61%|██████▏   | 732831/1193514 [01:08<00:41, 11112.77it/s]\u001b[A\n"," 61%|██████▏   | 733946/1193514 [01:08<00:43, 10653.47it/s]\u001b[A\n"," 62%|██████▏   | 735018/1193514 [01:08<00:45, 10185.72it/s]\u001b[A\n"," 62%|██████▏   | 736046/1193514 [01:08<00:45, 9977.66it/s] \u001b[A\n"," 62%|██████▏   | 737051/1193514 [01:09<00:46, 9912.29it/s]\u001b[A\n"," 62%|██████▏   | 738048/1193514 [01:09<00:46, 9836.36it/s]\u001b[A\n"," 62%|██████▏   | 739096/1193514 [01:09<00:45, 10020.73it/s]\u001b[A\n"," 62%|██████▏   | 740176/1193514 [01:09<00:44, 10240.91it/s]\u001b[A\n"," 62%|██████▏   | 741237/1193514 [01:09<00:43, 10347.53it/s]\u001b[A\n"," 62%|██████▏   | 742361/1193514 [01:09<00:42, 10597.31it/s]\u001b[A\n"," 62%|██████▏   | 743488/1193514 [01:09<00:41, 10789.85it/s]\u001b[A\n"," 62%|██████▏   | 744647/1193514 [01:09<00:40, 11017.40it/s]\u001b[A\n"," 62%|██████▏   | 745753/1193514 [01:09<00:40, 10992.79it/s]\u001b[A\n"," 63%|██████▎   | 746866/1193514 [01:09<00:40, 11031.52it/s]\u001b[A\n"," 63%|██████▎   | 747971/1193514 [01:10<00:41, 10767.10it/s]\u001b[A\n"," 63%|██████▎   | 749051/1193514 [01:10<00:41, 10602.64it/s]\u001b[A\n"," 63%|██████▎   | 750196/1193514 [01:10<00:40, 10842.49it/s]\u001b[A\n"," 63%|██████▎   | 751284/1193514 [01:10<00:40, 10837.90it/s]\u001b[A\n"," 63%|██████▎   | 752370/1193514 [01:10<00:40, 10774.11it/s]\u001b[A\n"," 63%|██████▎   | 753475/1193514 [01:10<00:40, 10853.58it/s]\u001b[A\n"," 63%|██████▎   | 754633/1193514 [01:10<00:39, 11058.68it/s]\u001b[A\n"," 63%|██████▎   | 755767/1193514 [01:10<00:39, 11138.63it/s]\u001b[A\n"," 63%|██████▎   | 756883/1193514 [01:10<00:40, 10714.27it/s]\u001b[A\n"," 64%|██████▎   | 758015/1193514 [01:11<00:39, 10888.74it/s]\u001b[A\n"," 64%|██████▎   | 759108/1193514 [01:11<00:41, 10530.73it/s]\u001b[A\n"," 64%|██████▎   | 760190/1193514 [01:11<00:40, 10613.42it/s]\u001b[A\n"," 64%|██████▍   | 761256/1193514 [01:11<00:40, 10580.49it/s]\u001b[A\n"," 64%|██████▍   | 762330/1193514 [01:11<00:40, 10625.36it/s]\u001b[A\n"," 64%|██████▍   | 763395/1193514 [01:11<00:40, 10550.93it/s]\u001b[A\n"," 64%|██████▍   | 764499/1193514 [01:11<00:40, 10691.26it/s]\u001b[A\n"," 64%|██████▍   | 765623/1193514 [01:11<00:39, 10850.03it/s]\u001b[A\n"," 64%|██████▍   | 766753/1193514 [01:11<00:38, 10980.91it/s]\u001b[A\n"," 64%|██████▍   | 767853/1193514 [01:11<00:38, 10937.59it/s]\u001b[A\n"," 64%|██████▍   | 768997/1193514 [01:12<00:38, 11080.52it/s]\u001b[A\n"," 65%|██████▍   | 770107/1193514 [01:12<00:38, 10996.18it/s]\u001b[A\n"," 65%|██████▍   | 771231/1193514 [01:12<00:38, 11066.69it/s]\u001b[A\n"," 65%|██████▍   | 772383/1193514 [01:12<00:37, 11197.00it/s]\u001b[A\n"," 65%|██████▍   | 773514/1193514 [01:12<00:37, 11230.40it/s]\u001b[A\n"," 65%|██████▍   | 774638/1193514 [01:12<00:39, 10560.74it/s]\u001b[A\n"," 65%|██████▍   | 775703/1193514 [01:12<00:40, 10319.87it/s]\u001b[A\n"," 65%|██████▌   | 776743/1193514 [01:12<00:41, 10107.09it/s]\u001b[A\n"," 65%|██████▌   | 777895/1193514 [01:12<00:39, 10493.04it/s]\u001b[A\n"," 65%|██████▌   | 778953/1193514 [01:12<00:40, 10248.21it/s]\u001b[A\n"," 65%|██████▌   | 780064/1193514 [01:13<00:39, 10492.17it/s]\u001b[A\n"," 65%|██████▌   | 781197/1193514 [01:13<00:38, 10728.49it/s]\u001b[A\n"," 66%|██████▌   | 782360/1193514 [01:13<00:37, 10983.00it/s]\u001b[A\n"," 66%|██████▌   | 783464/1193514 [01:13<00:38, 10573.54it/s]\u001b[A\n"," 66%|██████▌   | 784529/1193514 [01:13<00:38, 10538.49it/s]\u001b[A\n"," 66%|██████▌   | 785588/1193514 [01:13<00:39, 10364.27it/s]\u001b[A\n"," 66%|██████▌   | 786682/1193514 [01:13<00:38, 10529.48it/s]\u001b[A\n"," 66%|██████▌   | 787811/1193514 [01:13<00:37, 10745.90it/s]\u001b[A\n"," 66%|██████▌   | 788927/1193514 [01:13<00:37, 10865.62it/s]\u001b[A\n"," 66%|██████▌   | 790032/1193514 [01:13<00:36, 10918.55it/s]\u001b[A\n"," 66%|██████▋   | 791126/1193514 [01:14<00:37, 10833.81it/s]\u001b[A\n"," 66%|██████▋   | 792211/1193514 [01:14<00:37, 10811.86it/s]\u001b[A\n"," 66%|██████▋   | 793294/1193514 [01:14<00:38, 10353.62it/s]\u001b[A\n"," 67%|██████▋   | 794366/1193514 [01:14<00:38, 10456.78it/s]\u001b[A\n"," 67%|██████▋   | 795475/1193514 [01:14<00:37, 10636.75it/s]\u001b[A\n"," 67%|██████▋   | 796622/1193514 [01:14<00:36, 10871.61it/s]\u001b[A\n"," 67%|██████▋   | 797769/1193514 [01:14<00:35, 11042.25it/s]\u001b[A\n"," 67%|██████▋   | 798905/1193514 [01:14<00:35, 11134.75it/s]\u001b[A\n"," 67%|██████▋   | 800068/1193514 [01:14<00:34, 11277.30it/s]\u001b[A\n"," 67%|██████▋   | 801209/1193514 [01:15<00:34, 11315.80it/s]\u001b[A\n"," 67%|██████▋   | 802343/1193514 [01:15<00:34, 11212.79it/s]\u001b[A\n"," 67%|██████▋   | 803466/1193514 [01:15<00:34, 11203.94it/s]\u001b[A\n"," 67%|██████▋   | 804590/1193514 [01:15<00:34, 11214.07it/s]\u001b[A\n"," 68%|██████▊   | 805717/1193514 [01:15<00:34, 11229.46it/s]\u001b[A\n"," 68%|██████▊   | 806842/1193514 [01:15<00:34, 11233.67it/s]\u001b[A\n"," 68%|██████▊   | 807966/1193514 [01:15<00:34, 11184.65it/s]\u001b[A\n"," 68%|██████▊   | 809122/1193514 [01:15<00:34, 11293.30it/s]\u001b[A\n"," 68%|██████▊   | 810252/1193514 [01:15<00:35, 10787.73it/s]\u001b[A\n"," 68%|██████▊   | 811336/1193514 [01:15<00:36, 10498.74it/s]\u001b[A\n"," 68%|██████▊   | 812457/1193514 [01:16<00:35, 10700.61it/s]\u001b[A\n"," 68%|██████▊   | 813593/1193514 [01:16<00:34, 10889.15it/s]\u001b[A\n"," 68%|██████▊   | 814699/1193514 [01:16<00:34, 10938.46it/s]\u001b[A\n"," 68%|██████▊   | 815823/1193514 [01:16<00:34, 11025.74it/s]\u001b[A\n"," 68%|██████▊   | 816928/1193514 [01:16<00:35, 10751.71it/s]\u001b[A\n"," 69%|██████▊   | 818007/1193514 [01:16<00:36, 10298.22it/s]\u001b[A\n"," 69%|██████▊   | 819144/1193514 [01:16<00:35, 10596.88it/s]\u001b[A\n"," 69%|██████▊   | 820285/1193514 [01:16<00:34, 10826.58it/s]\u001b[A\n"," 69%|██████▉   | 821408/1193514 [01:16<00:34, 10944.01it/s]\u001b[A\n"," 69%|██████▉   | 822507/1193514 [01:16<00:36, 10265.67it/s]\u001b[A\n"," 69%|██████▉   | 823546/1193514 [01:17<00:36, 10042.89it/s]\u001b[A\n"," 69%|██████▉   | 824648/1193514 [01:17<00:35, 10315.27it/s]\u001b[A\n"," 69%|██████▉   | 825814/1193514 [01:17<00:34, 10684.85it/s]\u001b[A\n"," 69%|██████▉   | 826946/1193514 [01:17<00:33, 10867.54it/s]\u001b[A\n"," 69%|██████▉   | 828083/1193514 [01:17<00:33, 11011.54it/s]\u001b[A\n"," 69%|██████▉   | 829190/1193514 [01:17<00:33, 10960.94it/s]\u001b[A\n"," 70%|██████▉   | 830312/1193514 [01:17<00:32, 11036.90it/s]\u001b[A\n"," 70%|██████▉   | 831419/1193514 [01:17<00:32, 11042.08it/s]\u001b[A\n"," 70%|██████▉   | 832542/1193514 [01:17<00:32, 11095.24it/s]\u001b[A\n"," 70%|██████▉   | 833693/1193514 [01:17<00:32, 11215.45it/s]\u001b[A\n"," 70%|██████▉   | 834835/1193514 [01:18<00:31, 11274.73it/s]\u001b[A\n"," 70%|███████   | 835964/1193514 [01:18<00:32, 11061.61it/s]\u001b[A\n"," 70%|███████   | 837072/1193514 [01:18<00:32, 10936.72it/s]\u001b[A\n"," 70%|███████   | 838168/1193514 [01:18<00:33, 10708.66it/s]\u001b[A\n"," 70%|███████   | 839241/1193514 [01:18<00:34, 10356.86it/s]\u001b[A\n"," 70%|███████   | 840281/1193514 [01:18<00:34, 10097.10it/s]\u001b[A\n"," 70%|███████   | 841295/1193514 [01:18<00:35, 9998.00it/s] \u001b[A\n"," 71%|███████   | 842298/1193514 [01:18<00:35, 9906.78it/s]\u001b[A\n"," 71%|███████   | 843292/1193514 [01:18<00:35, 9804.50it/s]\u001b[A\n"," 71%|███████   | 844398/1193514 [01:19<00:34, 10148.45it/s]\u001b[A\n"," 71%|███████   | 845539/1193514 [01:19<00:33, 10494.93it/s]\u001b[A\n"," 71%|███████   | 846685/1193514 [01:19<00:32, 10764.54it/s]\u001b[A\n"," 71%|███████   | 847834/1193514 [01:19<00:31, 10970.02it/s]\u001b[A\n"," 71%|███████   | 848941/1193514 [01:19<00:31, 10995.98it/s]\u001b[A\n"," 71%|███████   | 850045/1193514 [01:19<00:32, 10525.82it/s]\u001b[A\n"," 71%|███████▏  | 851139/1193514 [01:19<00:32, 10644.77it/s]\u001b[A\n"," 71%|███████▏  | 852270/1193514 [01:19<00:31, 10834.85it/s]\u001b[A\n"," 72%|███████▏  | 853392/1193514 [01:19<00:31, 10945.38it/s]\u001b[A\n"," 72%|███████▏  | 854498/1193514 [01:19<00:30, 10978.45it/s]\u001b[A\n"," 72%|███████▏  | 855599/1193514 [01:20<00:30, 10950.18it/s]\u001b[A\n"," 72%|███████▏  | 856736/1193514 [01:20<00:30, 11069.28it/s]\u001b[A\n"," 72%|███████▏  | 857845/1193514 [01:20<00:31, 10777.67it/s]\u001b[A\n"," 72%|███████▏  | 858926/1193514 [01:20<00:32, 10402.48it/s]\u001b[A\n"," 72%|███████▏  | 859971/1193514 [01:20<00:33, 10012.19it/s]\u001b[A\n"," 72%|███████▏  | 861060/1193514 [01:20<00:32, 10258.80it/s]\u001b[A\n"," 72%|███████▏  | 862092/1193514 [01:20<00:33, 9782.97it/s] \u001b[A\n"," 72%|███████▏  | 863218/1193514 [01:20<00:32, 10183.08it/s]\u001b[A\n"," 72%|███████▏  | 864354/1193514 [01:20<00:31, 10508.79it/s]\u001b[A\n"," 73%|███████▎  | 865490/1193514 [01:21<00:30, 10748.20it/s]\u001b[A\n"," 73%|███████▎  | 866645/1193514 [01:21<00:29, 10975.40it/s]\u001b[A\n"," 73%|███████▎  | 867769/1193514 [01:21<00:29, 11050.53it/s]\u001b[A\n"," 73%|███████▎  | 868912/1193514 [01:21<00:29, 11159.94it/s]\u001b[A\n"," 73%|███████▎  | 870032/1193514 [01:21<00:29, 11020.26it/s]\u001b[A\n"," 73%|███████▎  | 871179/1193514 [01:21<00:28, 11148.74it/s]\u001b[A\n"," 73%|███████▎  | 872305/1193514 [01:21<00:28, 11180.56it/s]\u001b[A\n"," 73%|███████▎  | 873431/1193514 [01:21<00:28, 11203.18it/s]\u001b[A\n"," 73%|███████▎  | 874605/1193514 [01:21<00:28, 11357.36it/s]\u001b[A\n"," 73%|███████▎  | 875743/1193514 [01:21<00:28, 11307.23it/s]\u001b[A\n"," 73%|███████▎  | 876875/1193514 [01:22<00:28, 11238.07it/s]\u001b[A\n"," 74%|███████▎  | 878000/1193514 [01:22<00:28, 11217.84it/s]\u001b[A\n"," 74%|███████▎  | 879123/1193514 [01:22<00:29, 10809.16it/s]\u001b[A\n"," 74%|███████▍  | 880228/1193514 [01:22<00:28, 10877.29it/s]\u001b[A\n"," 74%|███████▍  | 881319/1193514 [01:22<00:28, 10854.25it/s]\u001b[A\n"," 74%|███████▍  | 882442/1193514 [01:22<00:28, 10963.51it/s]\u001b[A\n"," 74%|███████▍  | 883552/1193514 [01:22<00:28, 11003.47it/s]\u001b[A\n"," 74%|███████▍  | 884654/1193514 [01:22<00:28, 10998.95it/s]\u001b[A\n"," 74%|███████▍  | 885796/1193514 [01:22<00:27, 11120.11it/s]\u001b[A\n"," 74%|███████▍  | 886928/1193514 [01:22<00:27, 11177.20it/s]\u001b[A\n"," 74%|███████▍  | 888047/1193514 [01:23<00:28, 10886.75it/s]\u001b[A\n"," 75%|███████▍  | 889192/1193514 [01:23<00:27, 11048.46it/s]\u001b[A\n"," 75%|███████▍  | 890299/1193514 [01:23<00:27, 11039.08it/s]\u001b[A\n"," 75%|███████▍  | 891418/1193514 [01:23<00:27, 11083.05it/s]\u001b[A\n"," 75%|███████▍  | 892530/1193514 [01:23<00:27, 11093.90it/s]\u001b[A\n"," 75%|███████▍  | 893681/1193514 [01:23<00:26, 11215.35it/s]\u001b[A\n"," 75%|███████▍  | 894820/1193514 [01:23<00:26, 11265.57it/s]\u001b[A\n"," 75%|███████▌  | 895949/1193514 [01:23<00:26, 11272.40it/s]\u001b[A\n"," 75%|███████▌  | 897121/1193514 [01:23<00:25, 11402.88it/s]\u001b[A\n"," 75%|███████▌  | 898269/1193514 [01:23<00:25, 11424.91it/s]\u001b[A\n"," 75%|███████▌  | 899412/1193514 [01:24<00:26, 11303.18it/s]\u001b[A\n"," 75%|███████▌  | 900569/1193514 [01:24<00:25, 11379.49it/s]\u001b[A\n"," 76%|███████▌  | 901708/1193514 [01:24<00:25, 11280.58it/s]\u001b[A\n"," 76%|███████▌  | 902840/1193514 [01:24<00:25, 11290.01it/s]\u001b[A\n"," 76%|███████▌  | 903970/1193514 [01:24<00:26, 10788.00it/s]\u001b[A\n"," 76%|███████▌  | 905054/1193514 [01:24<00:27, 10491.54it/s]\u001b[A\n"," 76%|███████▌  | 906109/1193514 [01:24<00:28, 10171.90it/s]\u001b[A\n"," 76%|███████▌  | 907133/1193514 [01:24<00:28, 10036.85it/s]\u001b[A\n"," 76%|███████▌  | 908142/1193514 [01:24<00:28, 9940.83it/s] \u001b[A\n"," 76%|███████▌  | 909186/1193514 [01:25<00:28, 10083.63it/s]\u001b[A\n"," 76%|███████▋  | 910283/1193514 [01:25<00:27, 10333.13it/s]\u001b[A\n"," 76%|███████▋  | 911419/1193514 [01:25<00:26, 10618.49it/s]\u001b[A\n"," 76%|███████▋  | 912523/1193514 [01:25<00:26, 10739.19it/s]\u001b[A\n"," 77%|███████▋  | 913644/1193514 [01:25<00:25, 10875.84it/s]\u001b[A\n"," 77%|███████▋  | 914785/1193514 [01:25<00:25, 11028.28it/s]\u001b[A\n"," 77%|███████▋  | 915901/1193514 [01:25<00:25, 11066.15it/s]\u001b[A\n"," 77%|███████▋  | 917015/1193514 [01:25<00:24, 11087.25it/s]\u001b[A\n"," 77%|███████▋  | 918125/1193514 [01:25<00:24, 11030.42it/s]\u001b[A\n"," 77%|███████▋  | 919255/1193514 [01:25<00:24, 11109.91it/s]\u001b[A\n"," 77%|███████▋  | 920409/1193514 [01:26<00:24, 11235.01it/s]\u001b[A\n"," 77%|███████▋  | 921534/1193514 [01:26<00:25, 10796.97it/s]\u001b[A\n"," 77%|███████▋  | 922618/1193514 [01:26<00:25, 10712.33it/s]\u001b[A\n"," 77%|███████▋  | 923693/1193514 [01:26<00:25, 10589.19it/s]\u001b[A\n"," 77%|███████▋  | 924811/1193514 [01:26<00:24, 10758.05it/s]\u001b[A\n"," 78%|███████▊  | 925946/1193514 [01:26<00:24, 10927.52it/s]\u001b[A\n"," 78%|███████▊  | 927042/1193514 [01:26<00:25, 10502.64it/s]\u001b[A\n"," 78%|███████▊  | 928098/1193514 [01:26<00:26, 10118.47it/s]\u001b[A\n"," 78%|███████▊  | 929117/1193514 [01:26<00:26, 9974.31it/s] \u001b[A\n"," 78%|███████▊  | 930120/1193514 [01:26<00:26, 9919.96it/s]\u001b[A\n"," 78%|███████▊  | 931256/1193514 [01:27<00:25, 10311.77it/s]\u001b[A\n"," 78%|███████▊  | 932394/1193514 [01:27<00:24, 10607.33it/s]\u001b[A\n"," 78%|███████▊  | 933525/1193514 [01:27<00:24, 10806.89it/s]\u001b[A\n"," 78%|███████▊  | 934612/1193514 [01:27<00:24, 10709.64it/s]\u001b[A\n"," 78%|███████▊  | 935707/1193514 [01:27<00:23, 10779.11it/s]\u001b[A\n"," 78%|███████▊  | 936853/1193514 [01:27<00:23, 10974.68it/s]\u001b[A\n"," 79%|███████▊  | 937963/1193514 [01:27<00:23, 11010.41it/s]\u001b[A\n"," 79%|███████▊  | 939067/1193514 [01:27<00:23, 10892.64it/s]\u001b[A\n"," 79%|███████▉  | 940159/1193514 [01:27<00:24, 10505.04it/s]\u001b[A\n"," 79%|███████▉  | 941214/1193514 [01:27<00:24, 10254.32it/s]\u001b[A\n"," 79%|███████▉  | 942244/1193514 [01:28<00:24, 10050.88it/s]\u001b[A\n"," 79%|███████▉  | 943253/1193514 [01:28<00:25, 9904.38it/s] \u001b[A\n"," 79%|███████▉  | 944249/1193514 [01:28<00:25, 9920.85it/s]\u001b[A\n"," 79%|███████▉  | 945349/1193514 [01:28<00:24, 10219.37it/s]\u001b[A\n"," 79%|███████▉  | 946469/1193514 [01:28<00:23, 10493.06it/s]\u001b[A\n"," 79%|███████▉  | 947583/1193514 [01:28<00:23, 10678.38it/s]\u001b[A\n"," 79%|███████▉  | 948655/1193514 [01:28<00:23, 10602.43it/s]\u001b[A\n"," 80%|███████▉  | 949742/1193514 [01:28<00:22, 10679.95it/s]\u001b[A\n"," 80%|███████▉  | 950877/1193514 [01:28<00:22, 10871.49it/s]\u001b[A\n"," 80%|███████▉  | 951991/1193514 [01:29<00:22, 10949.37it/s]\u001b[A\n"," 80%|███████▉  | 953105/1193514 [01:29<00:21, 11004.69it/s]\u001b[A\n"," 80%|███████▉  | 954210/1193514 [01:29<00:21, 11016.09it/s]\u001b[A\n"," 80%|████████  | 955337/1193514 [01:29<00:21, 11090.60it/s]\u001b[A\n"," 80%|████████  | 956477/1193514 [01:29<00:21, 11179.51it/s]\u001b[A\n"," 80%|████████  | 957596/1193514 [01:29<00:21, 11161.64it/s]\u001b[A\n"," 80%|████████  | 958718/1193514 [01:29<00:21, 11177.20it/s]\u001b[A\n"," 80%|████████  | 959837/1193514 [01:29<00:21, 11122.91it/s]\u001b[A\n"," 81%|████████  | 960950/1193514 [01:29<00:21, 10987.84it/s]\u001b[A\n"," 81%|████████  | 962097/1193514 [01:29<00:20, 11126.72it/s]\u001b[A\n"," 81%|████████  | 963233/1193514 [01:30<00:20, 11194.99it/s]\u001b[A\n"," 81%|████████  | 964371/1193514 [01:30<00:20, 11249.34it/s]\u001b[A\n"," 81%|████████  | 965505/1193514 [01:30<00:20, 11275.02it/s]\u001b[A\n"," 81%|████████  | 966633/1193514 [01:30<00:21, 10766.27it/s]\u001b[A\n"," 81%|████████  | 967730/1193514 [01:30<00:20, 10825.38it/s]\u001b[A\n"," 81%|████████  | 968882/1193514 [01:30<00:20, 11023.28it/s]\u001b[A\n"," 81%|████████▏ | 970014/1193514 [01:30<00:20, 11109.21it/s]\u001b[A\n"," 81%|████████▏ | 971129/1193514 [01:30<00:19, 11119.77it/s]\u001b[A\n"," 81%|████████▏ | 972243/1193514 [01:30<00:20, 10928.05it/s]\u001b[A\n"," 82%|████████▏ | 973354/1193514 [01:30<00:20, 10980.03it/s]\u001b[A\n"," 82%|████████▏ | 974486/1193514 [01:31<00:19, 11079.80it/s]\u001b[A\n"," 82%|████████▏ | 975627/1193514 [01:31<00:19, 11174.97it/s]\u001b[A\n"," 82%|████████▏ | 976752/1193514 [01:31<00:19, 11194.90it/s]\u001b[A\n"," 82%|████████▏ | 977909/1193514 [01:31<00:19, 11302.21it/s]\u001b[A\n"," 82%|████████▏ | 979040/1193514 [01:31<00:19, 11208.50it/s]\u001b[A\n"," 82%|████████▏ | 980162/1193514 [01:31<00:19, 11181.80it/s]\u001b[A\n"," 82%|████████▏ | 981291/1193514 [01:31<00:18, 11209.88it/s]\u001b[A\n"," 82%|████████▏ | 982413/1193514 [01:31<00:19, 11024.93it/s]\u001b[A\n"," 82%|████████▏ | 983517/1193514 [01:31<00:19, 11012.97it/s]\u001b[A\n"," 82%|████████▏ | 984619/1193514 [01:31<00:19, 10848.46it/s]\u001b[A\n"," 83%|████████▎ | 985747/1193514 [01:32<00:18, 10973.07it/s]\u001b[A\n"," 83%|████████▎ | 986900/1193514 [01:32<00:18, 11132.44it/s]\u001b[A\n"," 83%|████████▎ | 988015/1193514 [01:32<00:18, 11062.35it/s]\u001b[A\n"," 83%|████████▎ | 989176/1193514 [01:32<00:18, 11218.33it/s]\u001b[A\n"," 83%|████████▎ | 990358/1193514 [01:32<00:17, 11389.56it/s]\u001b[A\n"," 83%|████████▎ | 991504/1193514 [01:32<00:17, 11409.14it/s]\u001b[A\n"," 83%|████████▎ | 992656/1193514 [01:32<00:17, 11440.20it/s]\u001b[A\n"," 83%|████████▎ | 993801/1193514 [01:32<00:17, 11364.61it/s]\u001b[A\n"," 83%|████████▎ | 994939/1193514 [01:32<00:17, 11127.45it/s]\u001b[A\n"," 83%|████████▎ | 996054/1193514 [01:32<00:17, 11132.69it/s]\u001b[A\n"," 84%|████████▎ | 997169/1193514 [01:33<00:18, 10656.30it/s]\u001b[A\n"," 84%|████████▎ | 998240/1193514 [01:33<00:18, 10430.21it/s]\u001b[A\n"," 84%|████████▎ | 999371/1193514 [01:33<00:18, 10677.58it/s]\u001b[A\n"," 84%|████████▍ | 1000444/1193514 [01:33<00:18, 10537.34it/s]\u001b[A\n"," 84%|████████▍ | 1001502/1193514 [01:33<00:18, 10410.81it/s]\u001b[A\n"," 84%|████████▍ | 1002644/1193514 [01:33<00:17, 10692.70it/s]\u001b[A\n"," 84%|████████▍ | 1003718/1193514 [01:33<00:17, 10653.51it/s]\u001b[A\n"," 84%|████████▍ | 1004868/1193514 [01:33<00:17, 10892.00it/s]\u001b[A\n"," 84%|████████▍ | 1005992/1193514 [01:33<00:17, 10993.71it/s]\u001b[A\n"," 84%|████████▍ | 1007143/1193514 [01:33<00:16, 11142.27it/s]\u001b[A\n"," 84%|████████▍ | 1008267/1193514 [01:34<00:16, 11170.50it/s]\u001b[A\n"," 85%|████████▍ | 1009388/1193514 [01:34<00:16, 11180.51it/s]\u001b[A\n"," 85%|████████▍ | 1010544/1193514 [01:34<00:16, 11291.07it/s]\u001b[A\n"," 85%|████████▍ | 1011678/1193514 [01:34<00:16, 11302.83it/s]\u001b[A\n"," 85%|████████▍ | 1012828/1193514 [01:34<00:15, 11360.83it/s]\u001b[A\n"," 85%|████████▍ | 1013965/1193514 [01:34<00:16, 10937.76it/s]\u001b[A\n"," 85%|████████▌ | 1015091/1193514 [01:34<00:16, 11030.77it/s]\u001b[A\n"," 85%|████████▌ | 1016197/1193514 [01:34<00:16, 10664.67it/s]\u001b[A\n"," 85%|████████▌ | 1017269/1193514 [01:34<00:17, 10320.32it/s]\u001b[A\n"," 85%|████████▌ | 1018307/1193514 [01:35<00:17, 10289.22it/s]\u001b[A\n"," 85%|████████▌ | 1019340/1193514 [01:35<00:17, 10096.84it/s]\u001b[A\n"," 85%|████████▌ | 1020354/1193514 [01:35<00:17, 9998.12it/s] \u001b[A\n"," 86%|████████▌ | 1021497/1193514 [01:35<00:16, 10387.40it/s]\u001b[A\n"," 86%|████████▌ | 1022608/1193514 [01:35<00:16, 10593.06it/s]\u001b[A\n"," 86%|████████▌ | 1023727/1193514 [01:35<00:15, 10764.82it/s]\u001b[A\n"," 86%|████████▌ | 1024821/1193514 [01:35<00:15, 10816.67it/s]\u001b[A\n"," 86%|████████▌ | 1025906/1193514 [01:35<00:15, 10812.66it/s]\u001b[A\n"," 86%|████████▌ | 1027055/1193514 [01:35<00:15, 11006.54it/s]\u001b[A\n"," 86%|████████▌ | 1028158/1193514 [01:35<00:15, 10461.91it/s]\u001b[A\n"," 86%|████████▌ | 1029212/1193514 [01:36<00:16, 10259.03it/s]\u001b[A\n"," 86%|████████▋ | 1030244/1193514 [01:36<00:16, 10103.76it/s]\u001b[A\n"," 86%|████████▋ | 1031260/1193514 [01:36<00:16, 9993.90it/s] \u001b[A\n"," 86%|████████▋ | 1032263/1193514 [01:36<00:16, 9918.37it/s]\u001b[A\n"," 87%|████████▋ | 1033258/1193514 [01:36<00:16, 9897.74it/s]\u001b[A\n"," 87%|████████▋ | 1034257/1193514 [01:36<00:16, 9924.45it/s]\u001b[A\n"," 87%|████████▋ | 1035256/1193514 [01:36<00:15, 9942.92it/s]\u001b[A\n"," 87%|████████▋ | 1036252/1193514 [01:36<00:16, 9794.20it/s]\u001b[A\n"," 87%|████████▋ | 1037356/1193514 [01:36<00:15, 10135.35it/s]\u001b[A\n"," 87%|████████▋ | 1038463/1193514 [01:36<00:14, 10397.35it/s]\u001b[A\n"," 87%|████████▋ | 1039641/1193514 [01:37<00:14, 10775.92it/s]\u001b[A\n"," 87%|████████▋ | 1040726/1193514 [01:37<00:14, 10434.19it/s]\u001b[A\n"," 87%|████████▋ | 1041836/1193514 [01:37<00:14, 10624.43it/s]\u001b[A\n"," 87%|████████▋ | 1042988/1193514 [01:37<00:13, 10877.12it/s]\u001b[A\n"," 87%|████████▋ | 1044163/1193514 [01:37<00:13, 11123.89it/s]\u001b[A\n"," 88%|████████▊ | 1045329/1193514 [01:37<00:13, 11278.91it/s]\u001b[A\n"," 88%|████████▊ | 1046509/1193514 [01:37<00:12, 11428.57it/s]\u001b[A\n"," 88%|████████▊ | 1047662/1193514 [01:37<00:12, 11456.51it/s]\u001b[A\n"," 88%|████████▊ | 1048822/1193514 [01:37<00:12, 11497.24it/s]\u001b[A\n"," 88%|████████▊ | 1049974/1193514 [01:38<00:12, 11438.50it/s]\u001b[A\n"," 88%|████████▊ | 1051120/1193514 [01:38<00:12, 11329.21it/s]\u001b[A\n"," 88%|████████▊ | 1052255/1193514 [01:38<00:12, 10976.08it/s]\u001b[A\n"," 88%|████████▊ | 1053356/1193514 [01:38<00:13, 10581.68it/s]\u001b[A\n"," 88%|████████▊ | 1054457/1193514 [01:38<00:12, 10705.05it/s]\u001b[A\n"," 88%|████████▊ | 1055622/1193514 [01:38<00:12, 10971.14it/s]\u001b[A\n"," 89%|████████▊ | 1056783/1193514 [01:38<00:12, 11153.96it/s]\u001b[A\n"," 89%|████████▊ | 1057951/1193514 [01:38<00:11, 11306.55it/s]\u001b[A\n"," 89%|████████▊ | 1059085/1193514 [01:38<00:11, 11257.72it/s]\u001b[A\n"," 89%|████████▉ | 1060214/1193514 [01:38<00:11, 11250.20it/s]\u001b[A\n"," 89%|████████▉ | 1061341/1193514 [01:39<00:11, 11167.16it/s]\u001b[A\n"," 89%|████████▉ | 1062485/1193514 [01:39<00:11, 11245.15it/s]\u001b[A\n"," 89%|████████▉ | 1063623/1193514 [01:39<00:11, 11284.98it/s]\u001b[A\n"," 89%|████████▉ | 1064753/1193514 [01:39<00:12, 10677.58it/s]\u001b[A\n"," 89%|████████▉ | 1065853/1193514 [01:39<00:11, 10770.09it/s]\u001b[A\n"," 89%|████████▉ | 1066989/1193514 [01:39<00:11, 10938.31it/s]\u001b[A\n"," 89%|████████▉ | 1068097/1193514 [01:39<00:11, 10978.82it/s]\u001b[A\n"," 90%|████████▉ | 1069213/1193514 [01:39<00:11, 11029.44it/s]\u001b[A\n"," 90%|████████▉ | 1070319/1193514 [01:39<00:11, 11011.66it/s]\u001b[A\n"," 90%|████████▉ | 1071434/1193514 [01:39<00:11, 11052.23it/s]\u001b[A\n"," 90%|████████▉ | 1072541/1193514 [01:40<00:10, 11056.93it/s]\u001b[A\n"," 90%|████████▉ | 1073660/1193514 [01:40<00:10, 11095.39it/s]\u001b[A\n"," 90%|█████████ | 1074781/1193514 [01:40<00:10, 11126.88it/s]\u001b[A\n"," 90%|█████████ | 1075895/1193514 [01:40<00:10, 10881.55it/s]\u001b[A\n"," 90%|█████████ | 1077008/1193514 [01:40<00:10, 10954.49it/s]\u001b[A\n"," 90%|█████████ | 1078125/1193514 [01:40<00:10, 11017.71it/s]\u001b[A\n"," 90%|█████████ | 1079245/1193514 [01:40<00:10, 11071.29it/s]\u001b[A\n"," 91%|█████████ | 1080353/1193514 [01:40<00:10, 10985.25it/s]\u001b[A\n"," 91%|█████████ | 1081489/1193514 [01:40<00:10, 11093.58it/s]\u001b[A\n"," 91%|█████████ | 1082604/1193514 [01:40<00:09, 11109.18it/s]\u001b[A\n"," 91%|█████████ | 1083716/1193514 [01:41<00:09, 11052.39it/s]\u001b[A\n"," 91%|█████████ | 1084835/1193514 [01:41<00:09, 11092.11it/s]\u001b[A\n"," 91%|█████████ | 1085945/1193514 [01:41<00:09, 11064.94it/s]\u001b[A\n"," 91%|█████████ | 1087052/1193514 [01:41<00:09, 10715.54it/s]\u001b[A\n"," 91%|█████████ | 1088127/1193514 [01:41<00:10, 10263.51it/s]\u001b[A\n"," 91%|█████████▏| 1089256/1193514 [01:41<00:09, 10549.71it/s]\u001b[A\n"," 91%|█████████▏| 1090410/1193514 [01:41<00:09, 10826.72it/s]\u001b[A\n"," 91%|█████████▏| 1091532/1193514 [01:41<00:09, 10940.32it/s]\u001b[A\n"," 92%|█████████▏| 1092677/1193514 [01:41<00:09, 11087.12it/s]\u001b[A\n"," 92%|█████████▏| 1093829/1193514 [01:41<00:08, 11213.10it/s]\u001b[A\n"," 92%|█████████▏| 1094954/1193514 [01:42<00:08, 11210.41it/s]\u001b[A\n"," 92%|█████████▏| 1096093/1193514 [01:42<00:08, 11261.35it/s]\u001b[A\n"," 92%|█████████▏| 1097226/1193514 [01:42<00:08, 11281.09it/s]\u001b[A\n"," 92%|█████████▏| 1098356/1193514 [01:42<00:08, 11247.61it/s]\u001b[A\n"," 92%|█████████▏| 1099526/1193514 [01:42<00:08, 11377.04it/s]\u001b[A\n"," 92%|█████████▏| 1100668/1193514 [01:42<00:08, 11387.11it/s]\u001b[A\n"," 92%|█████████▏| 1101808/1193514 [01:42<00:08, 11373.63it/s]\u001b[A\n"," 92%|█████████▏| 1102946/1193514 [01:42<00:08, 11138.21it/s]\u001b[A\n"," 93%|█████████▎| 1104090/1193514 [01:42<00:07, 11226.83it/s]\u001b[A\n"," 93%|█████████▎| 1105214/1193514 [01:43<00:08, 11000.44it/s]\u001b[A\n"," 93%|█████████▎| 1106316/1193514 [01:43<00:08, 10563.02it/s]\u001b[A\n"," 93%|█████████▎| 1107378/1193514 [01:43<00:08, 10498.01it/s]\u001b[A\n"," 93%|█████████▎| 1108432/1193514 [01:43<00:08, 10249.91it/s]\u001b[A\n"," 93%|█████████▎| 1109532/1193514 [01:43<00:08, 10463.60it/s]\u001b[A\n"," 93%|█████████▎| 1110677/1193514 [01:43<00:07, 10741.09it/s]\u001b[A\n"," 93%|█████████▎| 1111846/1193514 [01:43<00:07, 11007.36it/s]\u001b[A\n"," 93%|█████████▎| 1113007/1193514 [01:43<00:07, 11179.80it/s]\u001b[A\n"," 93%|█████████▎| 1114155/1193514 [01:43<00:07, 11266.76it/s]\u001b[A\n"," 93%|█████████▎| 1115312/1193514 [01:43<00:06, 11355.34it/s]\u001b[A\n"," 94%|█████████▎| 1116485/1193514 [01:44<00:06, 11463.96it/s]\u001b[A\n"," 94%|█████████▎| 1117634/1193514 [01:44<00:06, 11361.11it/s]\u001b[A\n"," 94%|█████████▎| 1118772/1193514 [01:44<00:06, 11345.12it/s]\u001b[A\n"," 94%|█████████▍| 1119908/1193514 [01:44<00:06, 11232.56it/s]\u001b[A\n"," 94%|█████████▍| 1121033/1193514 [01:44<00:06, 11082.25it/s]\u001b[A\n"," 94%|█████████▍| 1122170/1193514 [01:44<00:06, 11166.46it/s]\u001b[A\n"," 94%|█████████▍| 1123323/1193514 [01:44<00:06, 11272.59it/s]\u001b[A\n"," 94%|█████████▍| 1124475/1193514 [01:44<00:06, 11342.62it/s]\u001b[A\n"," 94%|█████████▍| 1125628/1193514 [01:44<00:05, 11396.39it/s]\u001b[A\n"," 94%|█████████▍| 1126804/1193514 [01:44<00:05, 11503.07it/s]\u001b[A\n"," 95%|█████████▍| 1127992/1193514 [01:45<00:05, 11612.24it/s]\u001b[A\n"," 95%|█████████▍| 1129154/1193514 [01:45<00:05, 11608.87it/s]\u001b[A\n"," 95%|█████████▍| 1130319/1193514 [01:45<00:05, 11619.30it/s]\u001b[A\n"," 95%|█████████▍| 1131482/1193514 [01:45<00:05, 11447.06it/s]\u001b[A\n"," 95%|█████████▍| 1132633/1193514 [01:45<00:05, 11465.80it/s]\u001b[A\n"," 95%|█████████▍| 1133781/1193514 [01:45<00:05, 11337.81it/s]\u001b[A\n"," 95%|█████████▌| 1134921/1193514 [01:45<00:05, 11353.62it/s]\u001b[A\n"," 95%|█████████▌| 1136057/1193514 [01:45<00:05, 11245.20it/s]\u001b[A\n"," 95%|█████████▌| 1137183/1193514 [01:45<00:05, 11102.10it/s]\u001b[A\n"," 95%|█████████▌| 1138299/1193514 [01:45<00:04, 11116.50it/s]\u001b[A\n"," 95%|█████████▌| 1139442/1193514 [01:46<00:04, 11208.04it/s]\u001b[A\n"," 96%|█████████▌| 1140564/1193514 [01:46<00:04, 11142.70it/s]\u001b[A\n"," 96%|█████████▌| 1141710/1193514 [01:46<00:04, 11234.85it/s]\u001b[A\n"," 96%|█████████▌| 1142835/1193514 [01:46<00:04, 11107.49it/s]\u001b[A\n"," 96%|█████████▌| 1143994/1193514 [01:46<00:04, 11247.72it/s]\u001b[A\n"," 96%|█████████▌| 1145150/1193514 [01:46<00:04, 11337.94it/s]\u001b[A\n"," 96%|█████████▌| 1146302/1193514 [01:46<00:04, 11391.95it/s]\u001b[A\n"," 96%|█████████▌| 1147471/1193514 [01:46<00:04, 11478.65it/s]\u001b[A\n"," 96%|█████████▌| 1148620/1193514 [01:46<00:03, 11460.15it/s]\u001b[A\n"," 96%|█████████▋| 1149778/1193514 [01:46<00:03, 11494.85it/s]\u001b[A\n"," 96%|█████████▋| 1150928/1193514 [01:47<00:03, 11402.69it/s]\u001b[A\n"," 97%|█████████▋| 1152069/1193514 [01:47<00:03, 11366.94it/s]\u001b[A\n"," 97%|█████████▋| 1153240/1193514 [01:47<00:03, 11466.28it/s]\u001b[A\n"," 97%|█████████▋| 1154388/1193514 [01:47<00:03, 11344.51it/s]\u001b[A\n"," 97%|█████████▋| 1155532/1193514 [01:47<00:03, 11372.76it/s]\u001b[A\n"," 97%|█████████▋| 1156673/1193514 [01:47<00:03, 11381.32it/s]\u001b[A\n"," 97%|█████████▋| 1157812/1193514 [01:47<00:03, 11263.17it/s]\u001b[A\n"," 97%|█████████▋| 1158967/1193514 [01:47<00:03, 11344.83it/s]\u001b[A\n"," 97%|█████████▋| 1160102/1193514 [01:47<00:02, 11215.74it/s]\u001b[A\n"," 97%|█████████▋| 1161259/1193514 [01:47<00:02, 11317.75it/s]\u001b[A\n"," 97%|█████████▋| 1162398/1193514 [01:48<00:02, 11336.68it/s]\u001b[A\n"," 97%|█████████▋| 1163533/1193514 [01:48<00:02, 11258.58it/s]\u001b[A\n"," 98%|█████████▊| 1164705/1193514 [01:48<00:02, 11389.23it/s]\u001b[A\n"," 98%|█████████▊| 1165845/1193514 [01:48<00:02, 11390.63it/s]\u001b[A\n"," 98%|█████████▊| 1166985/1193514 [01:48<00:02, 11371.90it/s]\u001b[A\n"," 98%|█████████▊| 1168155/1193514 [01:48<00:02, 11466.83it/s]\u001b[A\n"," 98%|█████████▊| 1169303/1193514 [01:48<00:02, 11408.39it/s]\u001b[A\n"," 98%|█████████▊| 1170445/1193514 [01:48<00:02, 11404.33it/s]\u001b[A\n"," 98%|█████████▊| 1171586/1193514 [01:48<00:01, 11232.25it/s]\u001b[A\n"," 98%|█████████▊| 1172735/1193514 [01:49<00:01, 11308.08it/s]\u001b[A\n"," 98%|█████████▊| 1173867/1193514 [01:49<00:01, 11306.57it/s]\u001b[A\n"," 98%|█████████▊| 1174999/1193514 [01:49<00:01, 11259.48it/s]\u001b[A\n"," 99%|█████████▊| 1176126/1193514 [01:49<00:01, 10807.07it/s]\u001b[A\n"," 99%|█████████▊| 1177285/1193514 [01:49<00:01, 11029.14it/s]\u001b[A\n"," 99%|█████████▊| 1178456/1193514 [01:49<00:01, 11224.11it/s]\u001b[A\n"," 99%|█████████▉| 1179614/1193514 [01:49<00:01, 11328.26it/s]\u001b[A\n"," 99%|█████████▉| 1180796/1193514 [01:49<00:01, 11468.67it/s]\u001b[A\n"," 99%|█████████▉| 1181962/1193514 [01:49<00:01, 11523.04it/s]\u001b[A\n"," 99%|█████████▉| 1183117/1193514 [01:49<00:00, 11471.71it/s]\u001b[A\n"," 99%|█████████▉| 1184272/1193514 [01:50<00:00, 11492.35it/s]\u001b[A\n"," 99%|█████████▉| 1185424/1193514 [01:50<00:00, 11499.24it/s]\u001b[A\n"," 99%|█████████▉| 1186575/1193514 [01:50<00:00, 11272.60it/s]\u001b[A\n","100%|█████████▉| 1187714/1193514 [01:50<00:00, 11307.02it/s]\u001b[A\n","100%|█████████▉| 1188878/1193514 [01:50<00:00, 11403.83it/s]\u001b[A\n","100%|█████████▉| 1190020/1193514 [01:50<00:00, 11337.98it/s]\u001b[A\n","100%|█████████▉| 1191155/1193514 [01:50<00:00, 11272.40it/s]\u001b[A\n","100%|█████████▉| 1192307/1193514 [01:50<00:00, 11345.15it/s]\u001b[A\n","100%|█████████▉| 1193468/1193514 [01:50<00:00, 11421.47it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.346 | Val. Loss: 0.276\n","\tTrain Acc : 85.40% | Val. Acc : 88.25%\n","\tTrain Rec : 72.44% | Val. Rec : 80.20%\n","\tTrain Prec: 76.98% | Val. Prec: 82.41%\n","\tTrain F1  : 72.67% | Val. F1  : 80.34%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|█████████▉| 1193468/1193514 [02:06<00:00, 11421.47it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.265 | Val. Loss: 0.275\n","\tTrain Acc : 89.08% | Val. Acc : 89.35%\n","\tTrain Rec : 81.36% | Val. Rec : 79.93%\n","\tTrain Prec: 84.29% | Val. Prec: 85.63%\n","\tTrain F1  : 81.59% | Val. F1  : 81.14%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.221 | Val. Loss: 0.294\n","\tTrain Acc : 90.95% | Val. Acc : 89.35%\n","\tTrain Rec : 84.97% | Val. Rec : 79.02%\n","\tTrain Prec: 87.05% | Val. Prec: 86.59%\n","\tTrain F1  : 85.06% | Val. F1  : 80.55%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.418 | Val. Loss: 0.344\n","\tTrain Acc : 82.44% | Val. Acc : 86.30%\n","\tTrain Rec : 64.79% | Val. Rec : 79.70%\n","\tTrain Prec: 67.90% | Val. Prec: 78.61%\n","\tTrain F1  : 63.95% | Val. F1  : 78.09%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.336 | Val. Loss: 0.355\n","\tTrain Acc : 86.10% | Val. Acc : 88.09%\n","\tTrain Rec : 74.93% | Val. Rec : 75.44%\n","\tTrain Prec: 80.49% | Val. Prec: 84.25%\n","\tTrain F1  : 75.81% | Val. F1  : 77.40%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.315 | Val. Loss: 0.315\n","\tTrain Acc : 86.98% | Val. Acc : 88.80%\n","\tTrain Rec : 77.10% | Val. Rec : 80.33%\n","\tTrain Prec: 81.60% | Val. Prec: 84.29%\n","\tTrain F1  : 77.60% | Val. F1  : 80.75%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.347 | Val. Loss: 0.307\n","\tTrain Acc : 85.54% | Val. Acc : 88.07%\n","\tTrain Rec : 72.32% | Val. Rec : 74.29%\n","\tTrain Prec: 76.55% | Val. Prec: 85.32%\n","\tTrain F1  : 72.46% | Val. F1  : 76.73%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.265 | Val. Loss: 0.329\n","\tTrain Acc : 88.98% | Val. Acc : 86.18%\n","\tTrain Rec : 80.96% | Val. Rec : 67.70%\n","\tTrain Prec: 84.68% | Val. Prec: 85.00%\n","\tTrain F1  : 81.45% | Val. F1  : 70.02%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.225 | Val. Loss: 0.296\n","\tTrain Acc : 90.96% | Val. Acc : 89.17%\n","\tTrain Rec : 84.99% | Val. Rec : 78.65%\n","\tTrain Prec: 87.33% | Val. Prec: 86.39%\n","\tTrain F1  : 85.17% | Val. F1  : 80.32%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.411 | Val. Loss: 0.390\n","\tTrain Acc : 82.74% | Val. Acc : 87.28%\n","\tTrain Rec : 66.05% | Val. Rec : 75.49%\n","\tTrain Prec: 69.72% | Val. Prec: 82.25%\n","\tTrain F1  : 65.60% | Val. F1  : 76.83%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.343 | Val. Loss: 0.298\n","\tTrain Acc : 85.65% | Val. Acc : 88.30%\n","\tTrain Rec : 74.79% | Val. Rec : 79.96%\n","\tTrain Prec: 79.72% | Val. Prec: 82.10%\n","\tTrain F1  : 75.49% | Val. F1  : 79.82%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.311 | Val. Loss: 0.344\n","\tTrain Acc : 86.90% | Val. Acc : 88.78%\n","\tTrain Rec : 77.10% | Val. Rec : 77.15%\n","\tTrain Prec: 81.65% | Val. Prec: 85.50%\n","\tTrain F1  : 77.67% | Val. F1  : 79.12%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.349 | Val. Loss: 0.302\n","\tTrain Acc : 85.16% | Val. Acc : 88.22%\n","\tTrain Rec : 72.18% | Val. Rec : 76.42%\n","\tTrain Prec: 77.00% | Val. Prec: 84.79%\n","\tTrain F1  : 72.57% | Val. F1  : 78.23%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.266 | Val. Loss: 0.278\n","\tTrain Acc : 89.04% | Val. Acc : 88.96%\n","\tTrain Rec : 81.45% | Val. Rec : 79.14%\n","\tTrain Prec: 84.80% | Val. Prec: 85.73%\n","\tTrain F1  : 81.70% | Val. F1  : 80.41%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.223 | Val. Loss: 0.263\n","\tTrain Acc : 90.66% | Val. Acc : 89.09%\n","\tTrain Rec : 84.36% | Val. Rec : 81.78%\n","\tTrain Prec: 86.79% | Val. Prec: 83.72%\n","\tTrain F1  : 84.52% | Val. F1  : 81.67%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.403 | Val. Loss: 0.365\n","\tTrain Acc : 82.92% | Val. Acc : 86.83%\n","\tTrain Rec : 67.41% | Val. Rec : 75.71%\n","\tTrain Prec: 72.14% | Val. Prec: 80.79%\n","\tTrain F1  : 67.18% | Val. F1  : 76.68%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.339 | Val. Loss: 0.330\n","\tTrain Acc : 85.68% | Val. Acc : 87.77%\n","\tTrain Rec : 74.21% | Val. Rec : 74.30%\n","\tTrain Prec: 79.33% | Val. Prec: 84.71%\n","\tTrain F1  : 75.07% | Val. F1  : 76.53%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.314 | Val. Loss: 0.284\n","\tTrain Acc : 86.68% | Val. Acc : 89.09%\n","\tTrain Rec : 75.93% | Val. Rec : 78.49%\n","\tTrain Prec: 81.05% | Val. Prec: 85.09%\n","\tTrain F1  : 76.70% | Val. F1  : 80.11%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.363 | Val. Loss: 0.276\n","\tTrain Acc : 84.63% | Val. Acc : 88.46%\n","\tTrain Rec : 71.35% | Val. Rec : 79.15%\n","\tTrain Prec: 76.32% | Val. Prec: 83.00%\n","\tTrain F1  : 71.45% | Val. F1  : 79.82%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.268 | Val. Loss: 0.273\n","\tTrain Acc : 88.75% | Val. Acc : 89.22%\n","\tTrain Rec : 80.86% | Val. Rec : 78.01%\n","\tTrain Prec: 84.37% | Val. Prec: 86.50%\n","\tTrain F1  : 81.16% | Val. F1  : 80.16%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.229 | Val. Loss: 0.263\n","\tTrain Acc : 90.61% | Val. Acc : 88.79%\n","\tTrain Rec : 84.33% | Val. Rec : 83.48%\n","\tTrain Prec: 86.60% | Val. Prec: 82.62%\n","\tTrain F1  : 84.42% | Val. F1  : 81.92%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.406 | Val. Loss: 0.322\n","\tTrain Acc : 82.71% | Val. Acc : 87.44%\n","\tTrain Rec : 66.20% | Val. Rec : 75.66%\n","\tTrain Prec: 70.50% | Val. Prec: 82.82%\n","\tTrain F1  : 65.67% | Val. F1  : 77.18%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.356 | Val. Loss: 0.307\n","\tTrain Acc : 85.18% | Val. Acc : 87.88%\n","\tTrain Rec : 73.53% | Val. Rec : 77.28%\n","\tTrain Prec: 79.13% | Val. Prec: 83.01%\n","\tTrain F1  : 74.13% | Val. F1  : 78.45%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.334 | Val. Loss: 0.331\n","\tTrain Acc : 85.90% | Val. Acc : 87.33%\n","\tTrain Rec : 74.70% | Val. Rec : 80.31%\n","\tTrain Prec: 79.93% | Val. Prec: 81.50%\n","\tTrain F1  : 75.37% | Val. F1  : 79.54%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.342 | Val. Loss: 0.270\n","\tTrain Acc : 85.57% | Val. Acc : 88.72%\n","\tTrain Rec : 72.87% | Val. Rec : 81.03%\n","\tTrain Prec: 76.85% | Val. Prec: 82.34%\n","\tTrain F1  : 72.80% | Val. F1  : 80.68%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.244 | Val. Loss: 0.287\n","\tTrain Acc : 89.99% | Val. Acc : 89.40%\n","\tTrain Rec : 83.34% | Val. Rec : 80.73%\n","\tTrain Prec: 86.12% | Val. Prec: 85.24%\n","\tTrain F1  : 83.47% | Val. F1  : 81.50%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.188 | Val. Loss: 0.310\n","\tTrain Acc : 92.57% | Val. Acc : 88.96%\n","\tTrain Rec : 88.05% | Val. Rec : 78.81%\n","\tTrain Prec: 89.52% | Val. Prec: 84.65%\n","\tTrain F1  : 88.02% | Val. F1  : 79.86%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.405 | Val. Loss: 0.312\n","\tTrain Acc : 82.84% | Val. Acc : 87.59%\n","\tTrain Rec : 65.58% | Val. Rec : 79.55%\n","\tTrain Prec: 68.67% | Val. Prec: 81.29%\n","\tTrain F1  : 64.76% | Val. F1  : 79.14%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.309 | Val. Loss: 0.287\n","\tTrain Acc : 87.14% | Val. Acc : 88.62%\n","\tTrain Rec : 77.03% | Val. Rec : 76.31%\n","\tTrain Prec: 82.16% | Val. Prec: 86.33%\n","\tTrain F1  : 77.93% | Val. F1  : 78.64%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.276 | Val. Loss: 0.335\n","\tTrain Acc : 88.59% | Val. Acc : 88.83%\n","\tTrain Rec : 80.46% | Val. Rec : 77.22%\n","\tTrain Prec: 83.94% | Val. Prec: 85.87%\n","\tTrain F1  : 80.88% | Val. F1  : 79.25%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.350 | Val. Loss: 0.275\n","\tTrain Acc : 85.30% | Val. Acc : 88.83%\n","\tTrain Rec : 72.59% | Val. Rec : 79.50%\n","\tTrain Prec: 76.37% | Val. Prec: 83.73%\n","\tTrain F1  : 72.48% | Val. F1  : 80.34%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.248 | Val. Loss: 0.299\n","\tTrain Acc : 89.93% | Val. Acc : 88.54%\n","\tTrain Rec : 82.97% | Val. Rec : 75.31%\n","\tTrain Prec: 85.85% | Val. Prec: 87.14%\n","\tTrain F1  : 83.15% | Val. F1  : 77.89%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.197 | Val. Loss: 0.286\n","\tTrain Acc : 92.32% | Val. Acc : 88.62%\n","\tTrain Rec : 86.92% | Val. Rec : 77.74%\n","\tTrain Prec: 89.31% | Val. Prec: 84.98%\n","\tTrain F1  : 87.16% | Val. F1  : 79.29%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.407 | Val. Loss: 0.328\n","\tTrain Acc : 82.94% | Val. Acc : 87.23%\n","\tTrain Rec : 66.29% | Val. Rec : 72.03%\n","\tTrain Prec: 69.50% | Val. Prec: 84.25%\n","\tTrain F1  : 65.53% | Val. F1  : 74.52%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.315 | Val. Loss: 0.291\n","\tTrain Acc : 86.88% | Val. Acc : 88.33%\n","\tTrain Rec : 76.65% | Val. Rec : 80.05%\n","\tTrain Prec: 81.39% | Val. Prec: 83.00%\n","\tTrain F1  : 77.22% | Val. F1  : 80.13%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.280 | Val. Loss: 0.301\n","\tTrain Acc : 88.57% | Val. Acc : 88.33%\n","\tTrain Rec : 80.01% | Val. Rec : 74.92%\n","\tTrain Prec: 84.19% | Val. Prec: 86.95%\n","\tTrain F1  : 80.52% | Val. F1  : 77.56%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.342 | Val. Loss: 0.273\n","\tTrain Acc : 85.82% | Val. Acc : 88.64%\n","\tTrain Rec : 73.08% | Val. Rec : 81.16%\n","\tTrain Prec: 78.17% | Val. Prec: 82.94%\n","\tTrain F1  : 73.46% | Val. F1  : 80.96%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.245 | Val. Loss: 0.308\n","\tTrain Acc : 90.16% | Val. Acc : 88.27%\n","\tTrain Rec : 83.08% | Val. Rec : 80.17%\n","\tTrain Prec: 86.30% | Val. Prec: 83.36%\n","\tTrain F1  : 83.51% | Val. F1  : 80.54%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.194 | Val. Loss: 0.277\n","\tTrain Acc : 92.13% | Val. Acc : 88.85%\n","\tTrain Rec : 86.86% | Val. Rec : 79.09%\n","\tTrain Prec: 89.01% | Val. Prec: 84.13%\n","\tTrain F1  : 87.08% | Val. F1  : 80.18%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.393 | Val. Loss: 0.295\n","\tTrain Acc : 83.32% | Val. Acc : 87.75%\n","\tTrain Rec : 67.68% | Val. Rec : 76.27%\n","\tTrain Prec: 72.77% | Val. Prec: 83.16%\n","\tTrain F1  : 67.55% | Val. F1  : 77.84%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.314 | Val. Loss: 0.292\n","\tTrain Acc : 86.48% | Val. Acc : 88.88%\n","\tTrain Rec : 76.51% | Val. Rec : 77.93%\n","\tTrain Prec: 80.93% | Val. Prec: 85.58%\n","\tTrain F1  : 76.96% | Val. F1  : 79.63%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.276 | Val. Loss: 0.330\n","\tTrain Acc : 88.41% | Val. Acc : 88.99%\n","\tTrain Rec : 80.04% | Val. Rec : 80.05%\n","\tTrain Prec: 83.69% | Val. Prec: 84.92%\n","\tTrain F1  : 80.54% | Val. F1  : 80.96%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.351 | Val. Loss: 0.290\n","\tTrain Acc : 85.17% | Val. Acc : 88.49%\n","\tTrain Rec : 72.65% | Val. Rec : 77.40%\n","\tTrain Prec: 77.46% | Val. Prec: 84.69%\n","\tTrain F1  : 72.66% | Val. F1  : 79.14%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.252 | Val. Loss: 0.306\n","\tTrain Acc : 89.97% | Val. Acc : 89.56%\n","\tTrain Rec : 83.03% | Val. Rec : 79.31%\n","\tTrain Prec: 86.01% | Val. Prec: 87.37%\n","\tTrain F1  : 83.12% | Val. F1  : 81.24%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.195 | Val. Loss: 0.284\n","\tTrain Acc : 92.00% | Val. Acc : 88.20%\n","\tTrain Rec : 86.90% | Val. Rec : 81.33%\n","\tTrain Prec: 88.72% | Val. Prec: 82.19%\n","\tTrain F1  : 86.81% | Val. F1  : 80.74%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.404 | Val. Loss: 0.314\n","\tTrain Acc : 82.97% | Val. Acc : 87.56%\n","\tTrain Rec : 67.29% | Val. Rec : 73.57%\n","\tTrain Prec: 71.60% | Val. Prec: 84.52%\n","\tTrain F1  : 66.62% | Val. F1  : 76.04%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.316 | Val. Loss: 0.293\n","\tTrain Acc : 87.08% | Val. Acc : 88.09%\n","\tTrain Rec : 77.14% | Val. Rec : 74.79%\n","\tTrain Prec: 82.02% | Val. Prec: 85.18%\n","\tTrain F1  : 77.69% | Val. F1  : 77.15%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.278 | Val. Loss: 0.299\n","\tTrain Acc : 88.67% | Val. Acc : 89.01%\n","\tTrain Rec : 80.63% | Val. Rec : 79.17%\n","\tTrain Prec: 84.27% | Val. Prec: 84.80%\n","\tTrain F1  : 81.01% | Val. F1  : 80.42%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.349 | Val. Loss: 0.293\n","\tTrain Acc : 85.22% | Val. Acc : 88.33%\n","\tTrain Rec : 72.47% | Val. Rec : 80.52%\n","\tTrain Prec: 76.58% | Val. Prec: 82.33%\n","\tTrain F1  : 72.52% | Val. F1  : 80.30%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.263 | Val. Loss: 0.268\n","\tTrain Acc : 89.12% | Val. Acc : 89.14%\n","\tTrain Rec : 81.68% | Val. Rec : 77.55%\n","\tTrain Prec: 84.65% | Val. Prec: 86.30%\n","\tTrain F1  : 81.80% | Val. F1  : 79.68%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.222 | Val. Loss: 0.310\n","\tTrain Acc : 91.03% | Val. Acc : 88.41%\n","\tTrain Rec : 85.48% | Val. Rec : 79.83%\n","\tTrain Prec: 87.26% | Val. Prec: 83.39%\n","\tTrain F1  : 85.32% | Val. F1  : 80.22%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.418 | Val. Loss: 0.438\n","\tTrain Acc : 82.49% | Val. Acc : 79.85%\n","\tTrain Rec : 64.90% | Val. Rec : 80.40%\n","\tTrain Prec: 67.76% | Val. Prec: 72.65%\n","\tTrain F1  : 64.00% | Val. F1  : 73.38%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.343 | Val. Loss: 0.323\n","\tTrain Acc : 85.63% | Val. Acc : 87.86%\n","\tTrain Rec : 74.12% | Val. Rec : 74.74%\n","\tTrain Prec: 79.45% | Val. Prec: 84.08%\n","\tTrain F1  : 75.03% | Val. F1  : 76.75%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.313 | Val. Loss: 0.329\n","\tTrain Acc : 86.72% | Val. Acc : 89.09%\n","\tTrain Rec : 76.11% | Val. Rec : 79.17%\n","\tTrain Prec: 81.23% | Val. Prec: 84.92%\n","\tTrain F1  : 76.90% | Val. F1  : 80.45%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.352 | Val. Loss: 0.283\n","\tTrain Acc : 84.90% | Val. Acc : 88.09%\n","\tTrain Rec : 71.61% | Val. Rec : 79.78%\n","\tTrain Prec: 75.92% | Val. Prec: 82.59%\n","\tTrain F1  : 71.70% | Val. F1  : 79.59%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.264 | Val. Loss: 0.301\n","\tTrain Acc : 88.94% | Val. Acc : 88.81%\n","\tTrain Rec : 80.93% | Val. Rec : 77.62%\n","\tTrain Prec: 84.32% | Val. Prec: 86.27%\n","\tTrain F1  : 81.27% | Val. F1  : 79.50%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.222 | Val. Loss: 0.274\n","\tTrain Acc : 91.03% | Val. Acc : 88.82%\n","\tTrain Rec : 85.22% | Val. Rec : 82.15%\n","\tTrain Prec: 87.33% | Val. Prec: 83.81%\n","\tTrain F1  : 85.34% | Val. F1  : 81.67%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.413 | Val. Loss: 0.330\n","\tTrain Acc : 82.73% | Val. Acc : 87.07%\n","\tTrain Rec : 66.12% | Val. Rec : 76.01%\n","\tTrain Prec: 69.56% | Val. Prec: 81.38%\n","\tTrain F1  : 65.45% | Val. F1  : 77.03%\n","Epoch: 02 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.342 | Val. Loss: 0.341\n","\tTrain Acc : 85.77% | Val. Acc : 85.99%\n","\tTrain Rec : 74.29% | Val. Rec : 81.74%\n","\tTrain Prec: 80.09% | Val. Prec: 78.58%\n","\tTrain F1  : 75.17% | Val. F1  : 78.78%\n","Epoch: 03 | Epoch Time: 0m 10s\n","\tTrain Loss: 0.313 | Val. Loss: 0.343\n","\tTrain Acc : 86.57% | Val. Acc : 88.62%\n","\tTrain Rec : 76.62% | Val. Rec : 77.34%\n","\tTrain Prec: 81.32% | Val. Prec: 85.11%\n","\tTrain F1  : 77.08% | Val. F1  : 79.13%\n","Early stopping!\n","best valid loss:  0.2607204321490115\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.351 | Val. Loss: 0.273\n","\tTrain Acc : 85.09% | Val. Acc : 88.67%\n","\tTrain Rec : 72.25% | Val. Rec : 78.61%\n","\tTrain Prec: 77.30% | Val. Prec: 83.54%\n","\tTrain F1  : 72.52% | Val. F1  : 79.57%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.271 | Val. Loss: 0.269\n","\tTrain Acc : 88.71% | Val. Acc : 89.09%\n","\tTrain Rec : 80.86% | Val. Rec : 81.80%\n","\tTrain Prec: 84.22% | Val. Prec: 84.18%\n","\tTrain F1  : 81.06% | Val. F1  : 81.74%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.221 | Val. Loss: 0.261\n","\tTrain Acc : 91.07% | Val. Acc : 89.30%\n","\tTrain Rec : 85.10% | Val. Rec : 80.46%\n","\tTrain Prec: 87.47% | Val. Prec: 84.82%\n","\tTrain F1  : 85.26% | Val. F1  : 81.24%\n","Epoch: 04 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.190 | Val. Loss: 0.290\n","\tTrain Acc : 92.48% | Val. Acc : 89.01%\n","\tTrain Rec : 87.67% | Val. Rec : 79.12%\n","\tTrain Prec: 89.40% | Val. Prec: 85.46%\n","\tTrain F1  : 87.68% | Val. F1  : 80.49%\n","Early stopping!\n","best valid loss:  0.26069329287700294\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.410 | Val. Loss: 0.313\n","\tTrain Acc : 82.68% | Val. Acc : 86.62%\n","\tTrain Rec : 66.26% | Val. Rec : 71.20%\n","\tTrain Prec: 70.59% | Val. Prec: 83.53%\n","\tTrain F1  : 65.79% | Val. F1  : 73.52%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.340 | Val. Loss: 0.301\n","\tTrain Acc : 85.91% | Val. Acc : 88.38%\n","\tTrain Rec : 74.29% | Val. Rec : 77.20%\n","\tTrain Prec: 80.23% | Val. Prec: 83.70%\n","\tTrain F1  : 75.21% | Val. F1  : 78.66%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.319 | Val. Loss: 0.280\n","\tTrain Acc : 86.75% | Val. Acc : 88.85%\n","\tTrain Rec : 76.70% | Val. Rec : 79.95%\n","\tTrain Prec: 81.91% | Val. Prec: 84.24%\n","\tTrain F1  : 77.41% | Val. F1  : 80.80%\n","Early stopping!\n","best valid loss:  0.26069329287700294\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.353 | Val. Loss: 0.282\n","\tTrain Acc : 85.15% | Val. Acc : 88.25%\n","\tTrain Rec : 72.50% | Val. Rec : 77.76%\n","\tTrain Prec: 77.40% | Val. Prec: 84.79%\n","\tTrain F1  : 72.60% | Val. F1  : 79.38%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.273 | Val. Loss: 0.262\n","\tTrain Acc : 88.18% | Val. Acc : 89.43%\n","\tTrain Rec : 80.05% | Val. Rec : 81.44%\n","\tTrain Prec: 83.25% | Val. Prec: 84.92%\n","\tTrain F1  : 80.11% | Val. F1  : 81.95%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.227 | Val. Loss: 0.260\n","\tTrain Acc : 90.61% | Val. Acc : 89.85%\n","\tTrain Rec : 84.55% | Val. Rec : 81.77%\n","\tTrain Prec: 86.67% | Val. Prec: 86.47%\n","\tTrain F1  : 84.40% | Val. F1  : 82.67%\n","Epoch: 04 | Epoch Time: 0m 13s\n","\tTrain Loss: 0.191 | Val. Loss: 0.263\n","\tTrain Acc : 92.51% | Val. Acc : 89.16%\n","\tTrain Rec : 87.46% | Val. Rec : 81.03%\n","\tTrain Prec: 89.46% | Val. Prec: 84.89%\n","\tTrain F1  : 87.54% | Val. F1  : 81.55%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.406 | Val. Loss: 0.328\n","\tTrain Acc : 82.43% | Val. Acc : 85.41%\n","\tTrain Rec : 66.58% | Val. Rec : 80.28%\n","\tTrain Prec: 70.64% | Val. Prec: 77.45%\n","\tTrain F1  : 66.08% | Val. F1  : 77.54%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.359 | Val. Loss: 0.336\n","\tTrain Acc : 84.82% | Val. Acc : 87.65%\n","\tTrain Rec : 72.34% | Val. Rec : 74.67%\n","\tTrain Prec: 78.00% | Val. Prec: 84.43%\n","\tTrain F1  : 72.91% | Val. F1  : 76.78%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.317 | Val. Loss: 0.290\n","\tTrain Acc : 86.35% | Val. Acc : 87.46%\n","\tTrain Rec : 75.95% | Val. Rec : 81.97%\n","\tTrain Prec: 81.15% | Val. Prec: 80.72%\n","\tTrain F1  : 76.52% | Val. F1  : 80.05%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.355 | Val. Loss: 0.293\n","\tTrain Acc : 85.17% | Val. Acc : 88.36%\n","\tTrain Rec : 70.99% | Val. Rec : 76.03%\n","\tTrain Prec: 74.08% | Val. Prec: 85.88%\n","\tTrain F1  : 71.06% | Val. F1  : 78.72%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.248 | Val. Loss: 0.270\n","\tTrain Acc : 89.93% | Val. Acc : 88.41%\n","\tTrain Rec : 82.55% | Val. Rec : 82.70%\n","\tTrain Prec: 86.13% | Val. Prec: 82.46%\n","\tTrain F1  : 83.58% | Val. F1  : 82.01%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.205 | Val. Loss: 0.314\n","\tTrain Acc : 91.74% | Val. Acc : 88.31%\n","\tTrain Rec : 86.34% | Val. Rec : 79.40%\n","\tTrain Prec: 88.45% | Val. Prec: 83.93%\n","\tTrain F1  : 86.76% | Val. F1  : 80.44%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.421 | Val. Loss: 0.332\n","\tTrain Acc : 81.70% | Val. Acc : 86.33%\n","\tTrain Rec : 62.85% | Val. Rec : 70.87%\n","\tTrain Prec: 65.30% | Val. Prec: 84.32%\n","\tTrain F1  : 61.85% | Val. F1  : 73.77%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.323 | Val. Loss: 0.355\n","\tTrain Acc : 86.49% | Val. Acc : 87.77%\n","\tTrain Rec : 75.81% | Val. Rec : 74.22%\n","\tTrain Prec: 80.89% | Val. Prec: 85.41%\n","\tTrain F1  : 77.25% | Val. F1  : 77.24%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.290 | Val. Loss: 0.304\n","\tTrain Acc : 88.16% | Val. Acc : 87.82%\n","\tTrain Rec : 79.07% | Val. Rec : 81.02%\n","\tTrain Prec: 83.64% | Val. Prec: 82.10%\n","\tTrain F1  : 80.29% | Val. F1  : 80.97%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.365 | Val. Loss: 0.284\n","\tTrain Acc : 84.90% | Val. Acc : 88.53%\n","\tTrain Rec : 70.31% | Val. Rec : 78.76%\n","\tTrain Prec: 73.19% | Val. Prec: 84.39%\n","\tTrain F1  : 70.24% | Val. F1  : 80.49%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.249 | Val. Loss: 0.297\n","\tTrain Acc : 89.90% | Val. Acc : 88.86%\n","\tTrain Rec : 82.80% | Val. Rec : 76.85%\n","\tTrain Prec: 86.06% | Val. Prec: 86.84%\n","\tTrain F1  : 83.71% | Val. F1  : 79.76%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.201 | Val. Loss: 0.283\n","\tTrain Acc : 91.92% | Val. Acc : 88.72%\n","\tTrain Rec : 86.48% | Val. Rec : 79.67%\n","\tTrain Prec: 88.83% | Val. Prec: 84.64%\n","\tTrain F1  : 87.08% | Val. F1  : 81.10%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.415 | Val. Loss: 0.336\n","\tTrain Acc : 82.56% | Val. Acc : 88.05%\n","\tTrain Rec : 65.13% | Val. Rec : 77.69%\n","\tTrain Prec: 67.15% | Val. Prec: 84.18%\n","\tTrain F1  : 64.38% | Val. F1  : 79.65%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.319 | Val. Loss: 0.346\n","\tTrain Acc : 86.94% | Val. Acc : 88.03%\n","\tTrain Rec : 76.57% | Val. Rec : 74.63%\n","\tTrain Prec: 81.88% | Val. Prec: 85.81%\n","\tTrain F1  : 78.04% | Val. F1  : 77.63%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.284 | Val. Loss: 0.340\n","\tTrain Acc : 88.33% | Val. Acc : 88.65%\n","\tTrain Rec : 79.51% | Val. Rec : 77.15%\n","\tTrain Prec: 83.91% | Val. Prec: 85.52%\n","\tTrain F1  : 80.74% | Val. F1  : 79.52%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.357 | Val. Loss: 0.320\n","\tTrain Acc : 84.91% | Val. Acc : 85.84%\n","\tTrain Rec : 71.64% | Val. Rec : 66.87%\n","\tTrain Prec: 75.76% | Val. Prec: 87.87%\n","\tTrain F1  : 71.85% | Val. F1  : 70.24%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.254 | Val. Loss: 0.269\n","\tTrain Acc : 89.54% | Val. Acc : 89.14%\n","\tTrain Rec : 81.89% | Val. Rec : 79.84%\n","\tTrain Prec: 85.69% | Val. Prec: 86.13%\n","\tTrain F1  : 82.78% | Val. F1  : 81.59%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.197 | Val. Loss: 0.323\n","\tTrain Acc : 92.05% | Val. Acc : 88.54%\n","\tTrain Rec : 86.97% | Val. Rec : 77.58%\n","\tTrain Prec: 88.52% | Val. Prec: 86.20%\n","\tTrain F1  : 87.28% | Val. F1  : 80.00%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.405 | Val. Loss: 0.314\n","\tTrain Acc : 82.99% | Val. Acc : 87.30%\n","\tTrain Rec : 65.96% | Val. Rec : 73.30%\n","\tTrain Prec: 70.63% | Val. Prec: 84.49%\n","\tTrain F1  : 65.90% | Val. F1  : 76.15%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.318 | Val. Loss: 0.287\n","\tTrain Acc : 86.57% | Val. Acc : 88.26%\n","\tTrain Rec : 76.34% | Val. Rec : 75.41%\n","\tTrain Prec: 81.35% | Val. Prec: 86.11%\n","\tTrain F1  : 77.63% | Val. F1  : 78.39%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.290 | Val. Loss: 0.289\n","\tTrain Acc : 87.93% | Val. Acc : 89.46%\n","\tTrain Rec : 78.88% | Val. Rec : 78.87%\n","\tTrain Prec: 83.08% | Val. Prec: 87.12%\n","\tTrain F1  : 79.96% | Val. F1  : 81.45%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.347 | Val. Loss: 0.295\n","\tTrain Acc : 85.49% | Val. Acc : 88.26%\n","\tTrain Rec : 72.79% | Val. Rec : 78.48%\n","\tTrain Prec: 77.87% | Val. Prec: 83.76%\n","\tTrain F1  : 73.41% | Val. F1  : 79.90%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.252 | Val. Loss: 0.271\n","\tTrain Acc : 89.57% | Val. Acc : 89.03%\n","\tTrain Rec : 82.58% | Val. Rec : 78.17%\n","\tTrain Prec: 85.68% | Val. Prec: 87.14%\n","\tTrain F1  : 83.38% | Val. F1  : 80.69%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.205 | Val. Loss: 0.281\n","\tTrain Acc : 91.64% | Val. Acc : 87.63%\n","\tTrain Rec : 86.48% | Val. Rec : 82.50%\n","\tTrain Prec: 88.46% | Val. Prec: 81.06%\n","\tTrain F1  : 86.96% | Val. F1  : 81.20%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.408 | Val. Loss: 0.304\n","\tTrain Acc : 82.92% | Val. Acc : 87.14%\n","\tTrain Rec : 66.34% | Val. Rec : 73.61%\n","\tTrain Prec: 71.11% | Val. Prec: 83.20%\n","\tTrain F1  : 66.21% | Val. F1  : 76.16%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.318 | Val. Loss: 0.283\n","\tTrain Acc : 86.58% | Val. Acc : 88.55%\n","\tTrain Rec : 76.16% | Val. Rec : 78.18%\n","\tTrain Prec: 81.21% | Val. Prec: 84.28%\n","\tTrain F1  : 77.44% | Val. F1  : 79.92%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.284 | Val. Loss: 0.293\n","\tTrain Acc : 88.17% | Val. Acc : 88.83%\n","\tTrain Rec : 79.33% | Val. Rec : 77.37%\n","\tTrain Prec: 83.62% | Val. Prec: 86.64%\n","\tTrain F1  : 80.44% | Val. F1  : 80.00%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.363 | Val. Loss: 0.313\n","\tTrain Acc : 84.74% | Val. Acc : 87.19%\n","\tTrain Rec : 70.39% | Val. Rec : 72.05%\n","\tTrain Prec: 73.77% | Val. Prec: 85.87%\n","\tTrain F1  : 70.50% | Val. F1  : 75.22%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.277 | Val. Loss: 0.266\n","\tTrain Acc : 88.55% | Val. Acc : 88.41%\n","\tTrain Rec : 80.09% | Val. Rec : 82.45%\n","\tTrain Prec: 84.05% | Val. Prec: 82.58%\n","\tTrain F1  : 81.17% | Val. F1  : 81.99%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.237 | Val. Loss: 0.304\n","\tTrain Acc : 90.10% | Val. Acc : 89.07%\n","\tTrain Rec : 83.39% | Val. Rec : 79.18%\n","\tTrain Prec: 86.36% | Val. Prec: 85.93%\n","\tTrain F1  : 84.05% | Val. F1  : 81.27%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.423 | Val. Loss: 0.360\n","\tTrain Acc : 81.69% | Val. Acc : 84.35%\n","\tTrain Rec : 62.92% | Val. Rec : 80.47%\n","\tTrain Prec: 65.81% | Val. Prec: 76.61%\n","\tTrain F1  : 62.20% | Val. F1  : 77.42%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.346 | Val. Loss: 0.394\n","\tTrain Acc : 85.19% | Val. Acc : 87.21%\n","\tTrain Rec : 73.36% | Val. Rec : 75.75%\n","\tTrain Prec: 79.01% | Val. Prec: 82.75%\n","\tTrain F1  : 74.83% | Val. F1  : 77.70%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.328 | Val. Loss: 0.382\n","\tTrain Acc : 86.23% | Val. Acc : 83.70%\n","\tTrain Rec : 75.47% | Val. Rec : 82.60%\n","\tTrain Prec: 80.97% | Val. Prec: 76.62%\n","\tTrain F1  : 76.84% | Val. F1  : 77.90%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.371 | Val. Loss: 0.308\n","\tTrain Acc : 84.32% | Val. Acc : 88.12%\n","\tTrain Rec : 70.34% | Val. Rec : 77.36%\n","\tTrain Prec: 73.87% | Val. Prec: 84.61%\n","\tTrain F1  : 70.31% | Val. F1  : 79.31%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.269 | Val. Loss: 0.290\n","\tTrain Acc : 88.70% | Val. Acc : 89.50%\n","\tTrain Rec : 80.55% | Val. Rec : 80.03%\n","\tTrain Prec: 84.53% | Val. Prec: 86.57%\n","\tTrain F1  : 81.69% | Val. F1  : 82.01%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.242 | Val. Loss: 0.316\n","\tTrain Acc : 90.05% | Val. Acc : 89.37%\n","\tTrain Rec : 83.28% | Val. Rec : 80.50%\n","\tTrain Prec: 86.32% | Val. Prec: 86.05%\n","\tTrain F1  : 84.01% | Val. F1  : 82.13%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.434 | Val. Loss: 0.334\n","\tTrain Acc : 81.49% | Val. Acc : 87.06%\n","\tTrain Rec : 63.03% | Val. Rec : 76.25%\n","\tTrain Prec: 65.86% | Val. Prec: 81.47%\n","\tTrain F1  : 62.26% | Val. F1  : 77.69%\n","Epoch: 02 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.352 | Val. Loss: 0.414\n","\tTrain Acc : 85.02% | Val. Acc : 87.28%\n","\tTrain Rec : 72.92% | Val. Rec : 72.58%\n","\tTrain Prec: 78.78% | Val. Prec: 85.49%\n","\tTrain F1  : 74.36% | Val. F1  : 75.68%\n","Epoch: 03 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.323 | Val. Loss: 0.439\n","\tTrain Acc : 86.64% | Val. Acc : 87.75%\n","\tTrain Rec : 76.14% | Val. Rec : 74.99%\n","\tTrain Prec: 81.57% | Val. Prec: 85.00%\n","\tTrain F1  : 77.60% | Val. F1  : 77.66%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.366 | Val. Loss: 0.317\n","\tTrain Acc : 84.43% | Val. Acc : 86.63%\n","\tTrain Rec : 70.25% | Val. Rec : 70.14%\n","\tTrain Prec: 75.19% | Val. Prec: 86.31%\n","\tTrain F1  : 70.82% | Val. F1  : 73.53%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.276 | Val. Loss: 0.267\n","\tTrain Acc : 88.29% | Val. Acc : 88.45%\n","\tTrain Rec : 79.97% | Val. Rec : 81.34%\n","\tTrain Prec: 83.28% | Val. Prec: 82.82%\n","\tTrain F1  : 80.83% | Val. F1  : 81.62%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.235 | Val. Loss: 0.268\n","\tTrain Acc : 90.33% | Val. Acc : 89.09%\n","\tTrain Rec : 83.78% | Val. Rec : 78.88%\n","\tTrain Prec: 86.50% | Val. Prec: 86.32%\n","\tTrain F1  : 84.51% | Val. F1  : 81.10%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.416 | Val. Loss: 0.314\n","\tTrain Acc : 82.27% | Val. Acc : 86.81%\n","\tTrain Rec : 65.25% | Val. Rec : 73.70%\n","\tTrain Prec: 69.25% | Val. Prec: 82.30%\n","\tTrain F1  : 65.29% | Val. F1  : 76.12%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.349 | Val. Loss: 0.290\n","\tTrain Acc : 84.89% | Val. Acc : 88.11%\n","\tTrain Rec : 72.97% | Val. Rec : 76.80%\n","\tTrain Prec: 78.14% | Val. Prec: 84.52%\n","\tTrain F1  : 74.20% | Val. F1  : 78.98%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.320 | Val. Loss: 0.334\n","\tTrain Acc : 86.89% | Val. Acc : 87.77%\n","\tTrain Rec : 76.66% | Val. Rec : 74.88%\n","\tTrain Prec: 81.89% | Val. Prec: 85.22%\n","\tTrain F1  : 77.95% | Val. F1  : 77.71%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.358 | Val. Loss: 0.305\n","\tTrain Acc : 84.48% | Val. Acc : 87.37%\n","\tTrain Rec : 71.00% | Val. Rec : 72.62%\n","\tTrain Prec: 76.53% | Val. Prec: 86.11%\n","\tTrain F1  : 71.68% | Val. F1  : 75.79%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.280 | Val. Loss: 0.273\n","\tTrain Acc : 88.25% | Val. Acc : 88.62%\n","\tTrain Rec : 79.65% | Val. Rec : 79.48%\n","\tTrain Prec: 84.16% | Val. Prec: 84.71%\n","\tTrain F1  : 80.76% | Val. F1  : 80.93%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.245 | Val. Loss: 0.283\n","\tTrain Acc : 89.80% | Val. Acc : 88.32%\n","\tTrain Rec : 83.12% | Val. Rec : 82.15%\n","\tTrain Prec: 85.76% | Val. Prec: 82.29%\n","\tTrain F1  : 83.72% | Val. F1  : 81.68%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.413 | Val. Loss: 0.325\n","\tTrain Acc : 82.24% | Val. Acc : 86.60%\n","\tTrain Rec : 65.20% | Val. Rec : 76.82%\n","\tTrain Prec: 70.37% | Val. Prec: 79.92%\n","\tTrain F1  : 65.17% | Val. F1  : 77.56%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.353 | Val. Loss: 0.301\n","\tTrain Acc : 85.37% | Val. Acc : 87.53%\n","\tTrain Rec : 73.17% | Val. Rec : 77.52%\n","\tTrain Prec: 80.00% | Val. Prec: 82.19%\n","\tTrain F1  : 74.95% | Val. F1  : 78.78%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.319 | Val. Loss: 0.311\n","\tTrain Acc : 86.55% | Val. Acc : 88.06%\n","\tTrain Rec : 75.74% | Val. Rec : 75.28%\n","\tTrain Prec: 81.00% | Val. Prec: 85.53%\n","\tTrain F1  : 77.16% | Val. F1  : 78.10%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.358 | Val. Loss: 0.298\n","\tTrain Acc : 84.87% | Val. Acc : 87.66%\n","\tTrain Rec : 70.79% | Val. Rec : 72.53%\n","\tTrain Prec: 74.35% | Val. Prec: 87.72%\n","\tTrain F1  : 70.81% | Val. F1  : 76.12%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.250 | Val. Loss: 0.263\n","\tTrain Acc : 89.92% | Val. Acc : 88.98%\n","\tTrain Rec : 82.83% | Val. Rec : 82.60%\n","\tTrain Prec: 86.01% | Val. Prec: 83.75%\n","\tTrain F1  : 83.67% | Val. F1  : 82.57%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.201 | Val. Loss: 0.296\n","\tTrain Acc : 92.10% | Val. Acc : 88.59%\n","\tTrain Rec : 87.10% | Val. Rec : 78.75%\n","\tTrain Prec: 88.75% | Val. Prec: 85.01%\n","\tTrain F1  : 87.32% | Val. F1  : 80.48%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6382498\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.413 | Val. Loss: 0.307\n","\tTrain Acc : 82.33% | Val. Acc : 87.35%\n","\tTrain Rec : 64.65% | Val. Rec : 74.98%\n","\tTrain Prec: 67.82% | Val. Prec: 83.58%\n","\tTrain F1  : 64.02% | Val. F1  : 77.27%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.321 | Val. Loss: 0.356\n","\tTrain Acc : 86.72% | Val. Acc : 88.42%\n","\tTrain Rec : 76.38% | Val. Rec : 76.83%\n","\tTrain Prec: 81.31% | Val. Prec: 84.69%\n","\tTrain F1  : 77.75% | Val. F1  : 79.08%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.287 | Val. Loss: 0.309\n","\tTrain Acc : 88.21% | Val. Acc : 86.86%\n","\tTrain Rec : 79.89% | Val. Rec : 83.12%\n","\tTrain Prec: 83.50% | Val. Prec: 80.25%\n","\tTrain F1  : 80.76% | Val. F1  : 80.93%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.360 | Val. Loss: 0.287\n","\tTrain Acc : 85.00% | Val. Acc : 88.87%\n","\tTrain Rec : 70.94% | Val. Rec : 79.14%\n","\tTrain Prec: 74.00% | Val. Prec: 85.38%\n","\tTrain F1  : 70.92% | Val. F1  : 80.83%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.252 | Val. Loss: 0.285\n","\tTrain Acc : 89.78% | Val. Acc : 89.09%\n","\tTrain Rec : 82.69% | Val. Rec : 77.58%\n","\tTrain Prec: 85.90% | Val. Prec: 87.93%\n","\tTrain F1  : 83.59% | Val. F1  : 80.36%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.205 | Val. Loss: 0.301\n","\tTrain Acc : 91.73% | Val. Acc : 88.64%\n","\tTrain Rec : 86.28% | Val. Rec : 78.26%\n","\tTrain Prec: 88.66% | Val. Prec: 85.70%\n","\tTrain F1  : 86.82% | Val. F1  : 80.32%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6777762\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.422 | Val. Loss: 0.313\n","\tTrain Acc : 82.36% | Val. Acc : 87.79%\n","\tTrain Rec : 64.55% | Val. Rec : 76.25%\n","\tTrain Prec: 66.08% | Val. Prec: 84.33%\n","\tTrain F1  : 63.51% | Val. F1  : 78.58%\n","Epoch: 02 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.321 | Val. Loss: 0.352\n","\tTrain Acc : 86.78% | Val. Acc : 86.98%\n","\tTrain Rec : 75.95% | Val. Rec : 71.22%\n","\tTrain Prec: 82.04% | Val. Prec: 86.34%\n","\tTrain F1  : 77.62% | Val. F1  : 74.60%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.284 | Val. Loss: 0.366\n","\tTrain Acc : 88.28% | Val. Acc : 87.09%\n","\tTrain Rec : 79.73% | Val. Rec : 71.21%\n","\tTrain Prec: 83.71% | Val. Prec: 87.32%\n","\tTrain F1  : 80.83% | Val. F1  : 74.66%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.361 | Val. Loss: 0.313\n","\tTrain Acc : 84.87% | Val. Acc : 87.17%\n","\tTrain Rec : 70.61% | Val. Rec : 71.97%\n","\tTrain Prec: 75.04% | Val. Prec: 86.35%\n","\tTrain F1  : 70.92% | Val. F1  : 75.32%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.248 | Val. Loss: 0.269\n","\tTrain Acc : 89.71% | Val. Acc : 88.78%\n","\tTrain Rec : 82.39% | Val. Rec : 79.60%\n","\tTrain Prec: 85.49% | Val. Prec: 84.94%\n","\tTrain F1  : 83.24% | Val. F1  : 81.04%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.201 | Val. Loss: 0.320\n","\tTrain Acc : 91.83% | Val. Acc : 88.49%\n","\tTrain Rec : 86.45% | Val. Rec : 75.98%\n","\tTrain Prec: 88.40% | Val. Prec: 87.46%\n","\tTrain F1  : 86.92% | Val. F1  : 79.00%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8267170\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.404 | Val. Loss: 0.298\n","\tTrain Acc : 82.90% | Val. Acc : 87.58%\n","\tTrain Rec : 66.45% | Val. Rec : 74.58%\n","\tTrain Prec: 70.11% | Val. Prec: 84.68%\n","\tTrain F1  : 66.30% | Val. F1  : 77.21%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.313 | Val. Loss: 0.273\n","\tTrain Acc : 86.88% | Val. Acc : 88.94%\n","\tTrain Rec : 76.60% | Val. Rec : 79.78%\n","\tTrain Prec: 81.78% | Val. Prec: 85.00%\n","\tTrain F1  : 78.03% | Val. F1  : 81.26%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.285 | Val. Loss: 0.309\n","\tTrain Acc : 87.94% | Val. Acc : 88.21%\n","\tTrain Rec : 79.41% | Val. Rec : 74.52%\n","\tTrain Prec: 82.89% | Val. Prec: 87.97%\n","\tTrain F1  : 80.24% | Val. F1  : 77.97%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.348 | Val. Loss: 0.292\n","\tTrain Acc : 85.46% | Val. Acc : 88.42%\n","\tTrain Rec : 72.31% | Val. Rec : 77.38%\n","\tTrain Prec: 77.76% | Val. Prec: 84.94%\n","\tTrain F1  : 73.20% | Val. F1  : 79.48%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.257 | Val. Loss: 0.273\n","\tTrain Acc : 89.40% | Val. Acc : 88.54%\n","\tTrain Rec : 82.20% | Val. Rec : 76.32%\n","\tTrain Prec: 85.52% | Val. Prec: 87.30%\n","\tTrain F1  : 82.94% | Val. F1  : 79.21%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.208 | Val. Loss: 0.279\n","\tTrain Acc : 91.81% | Val. Acc : 88.73%\n","\tTrain Rec : 85.98% | Val. Rec : 81.04%\n","\tTrain Prec: 88.75% | Val. Prec: 83.23%\n","\tTrain F1  : 86.76% | Val. F1  : 81.49%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9844130\n","RNN(\n","  (embedding): Embedding(18488, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","5546400\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.411 | Val. Loss: 0.303\n","\tTrain Acc : 82.75% | Val. Acc : 87.63%\n","\tTrain Rec : 66.00% | Val. Rec : 74.93%\n","\tTrain Prec: 70.62% | Val. Prec: 83.91%\n","\tTrain F1  : 65.91% | Val. F1  : 77.47%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.317 | Val. Loss: 0.282\n","\tTrain Acc : 86.61% | Val. Acc : 89.03%\n","\tTrain Rec : 76.29% | Val. Rec : 80.45%\n","\tTrain Prec: 81.56% | Val. Prec: 84.64%\n","\tTrain F1  : 77.55% | Val. F1  : 81.55%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.284 | Val. Loss: 0.283\n","\tTrain Acc : 88.21% | Val. Acc : 89.08%\n","\tTrain Rec : 79.58% | Val. Rec : 78.50%\n","\tTrain Prec: 83.38% | Val. Prec: 86.88%\n","\tTrain F1  : 80.67% | Val. F1  : 80.89%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.363 | Val. Loss: 0.302\n","\tTrain Acc : 84.46% | Val. Acc : 86.85%\n","\tTrain Rec : 69.84% | Val. Rec : 71.04%\n","\tTrain Prec: 73.64% | Val. Prec: 85.91%\n","\tTrain F1  : 70.14% | Val. F1  : 74.27%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.278 | Val. Loss: 0.272\n","\tTrain Acc : 88.25% | Val. Acc : 88.11%\n","\tTrain Rec : 79.37% | Val. Rec : 82.58%\n","\tTrain Prec: 83.74% | Val. Prec: 81.96%\n","\tTrain F1  : 80.60% | Val. F1  : 81.68%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.240 | Val. Loss: 0.287\n","\tTrain Acc : 89.88% | Val. Acc : 89.24%\n","\tTrain Rec : 83.02% | Val. Rec : 78.61%\n","\tTrain Prec: 85.90% | Val. Prec: 87.09%\n","\tTrain F1  : 83.61% | Val. F1  : 81.04%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4431298\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.431 | Val. Loss: 0.349\n","\tTrain Acc : 81.24% | Val. Acc : 85.89%\n","\tTrain Rec : 62.22% | Val. Rec : 78.55%\n","\tTrain Prec: 65.03% | Val. Prec: 78.76%\n","\tTrain F1  : 61.29% | Val. F1  : 77.95%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.351 | Val. Loss: 0.388\n","\tTrain Acc : 85.31% | Val. Acc : 87.40%\n","\tTrain Rec : 73.77% | Val. Rec : 74.81%\n","\tTrain Prec: 79.09% | Val. Prec: 83.16%\n","\tTrain F1  : 75.16% | Val. F1  : 77.02%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.327 | Val. Loss: 0.369\n","\tTrain Acc : 86.39% | Val. Acc : 86.05%\n","\tTrain Rec : 75.81% | Val. Rec : 82.03%\n","\tTrain Prec: 81.08% | Val. Prec: 78.96%\n","\tTrain F1  : 77.10% | Val. F1  : 79.81%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.371 | Val. Loss: 0.312\n","\tTrain Acc : 84.13% | Val. Acc : 88.13%\n","\tTrain Rec : 70.13% | Val. Rec : 76.38%\n","\tTrain Prec: 73.82% | Val. Prec: 85.17%\n","\tTrain F1  : 69.98% | Val. F1  : 78.77%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.273 | Val. Loss: 0.290\n","\tTrain Acc : 88.40% | Val. Acc : 89.37%\n","\tTrain Rec : 80.29% | Val. Rec : 80.85%\n","\tTrain Prec: 83.94% | Val. Prec: 85.58%\n","\tTrain F1  : 81.28% | Val. F1  : 82.34%\n","Epoch: 03 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.240 | Val. Loss: 0.302\n","\tTrain Acc : 89.90% | Val. Acc : 89.37%\n","\tTrain Rec : 83.14% | Val. Rec : 78.96%\n","\tTrain Prec: 86.18% | Val. Prec: 87.56%\n","\tTrain F1  : 83.84% | Val. F1  : 81.50%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","4826562\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","512\n","2\n","Epoch: 01 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.429 | Val. Loss: 0.323\n","\tTrain Acc : 81.84% | Val. Acc : 86.83%\n","\tTrain Rec : 63.83% | Val. Rec : 74.38%\n","\tTrain Prec: 66.00% | Val. Prec: 82.32%\n","\tTrain F1  : 63.07% | Val. F1  : 76.54%\n","Epoch: 02 | Epoch Time: 0m 6s\n","\tTrain Loss: 0.346 | Val. Loss: 0.418\n","\tTrain Acc : 85.70% | Val. Acc : 86.80%\n","\tTrain Rec : 73.62% | Val. Rec : 79.53%\n","\tTrain Prec: 80.06% | Val. Prec: 80.23%\n","\tTrain F1  : 75.34% | Val. F1  : 79.29%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.323 | Val. Loss: 0.408\n","\tTrain Acc : 86.42% | Val. Acc : 87.56%\n","\tTrain Rec : 75.63% | Val. Rec : 72.31%\n","\tTrain Prec: 81.44% | Val. Prec: 87.49%\n","\tTrain F1  : 77.08% | Val. F1  : 75.71%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.368 | Val. Loss: 0.314\n","\tTrain Acc : 84.41% | Val. Acc : 87.17%\n","\tTrain Rec : 70.20% | Val. Rec : 73.75%\n","\tTrain Prec: 75.48% | Val. Prec: 83.89%\n","\tTrain F1  : 70.72% | Val. F1  : 76.49%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.276 | Val. Loss: 0.264\n","\tTrain Acc : 88.27% | Val. Acc : 89.07%\n","\tTrain Rec : 79.78% | Val. Rec : 80.13%\n","\tTrain Prec: 83.78% | Val. Prec: 84.70%\n","\tTrain F1  : 80.72% | Val. F1  : 81.47%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.235 | Val. Loss: 0.266\n","\tTrain Acc : 90.34% | Val. Acc : 89.27%\n","\tTrain Rec : 83.76% | Val. Rec : 81.14%\n","\tTrain Prec: 86.46% | Val. Prec: 85.62%\n","\tTrain F1  : 84.51% | Val. F1  : 82.26%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","6213570\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.414 | Val. Loss: 0.334\n","\tTrain Acc : 82.41% | Val. Acc : 87.01%\n","\tTrain Rec : 65.12% | Val. Rec : 76.88%\n","\tTrain Prec: 69.95% | Val. Prec: 81.14%\n","\tTrain F1  : 65.08% | Val. F1  : 78.02%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.347 | Val. Loss: 0.344\n","\tTrain Acc : 85.37% | Val. Acc : 84.28%\n","\tTrain Rec : 73.59% | Val. Rec : 83.12%\n","\tTrain Prec: 79.12% | Val. Prec: 77.40%\n","\tTrain F1  : 75.05% | Val. F1  : 78.58%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.322 | Val. Loss: 0.302\n","\tTrain Acc : 86.29% | Val. Acc : 87.69%\n","\tTrain Rec : 75.31% | Val. Rec : 73.76%\n","\tTrain Prec: 81.18% | Val. Prec: 86.18%\n","\tTrain F1  : 76.57% | Val. F1  : 76.95%\n","Early stopping!\n","best valid loss:  0.2603081101316865\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.363 | Val. Loss: 0.289\n","\tTrain Acc : 84.55% | Val. Acc : 87.50%\n","\tTrain Rec : 70.84% | Val. Rec : 77.25%\n","\tTrain Prec: 76.65% | Val. Prec: 82.05%\n","\tTrain F1  : 71.62% | Val. F1  : 78.56%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.280 | Val. Loss: 0.271\n","\tTrain Acc : 88.00% | Val. Acc : 89.04%\n","\tTrain Rec : 79.29% | Val. Rec : 79.61%\n","\tTrain Prec: 83.33% | Val. Prec: 85.51%\n","\tTrain F1  : 80.26% | Val. F1  : 81.44%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.246 | Val. Loss: 0.259\n","\tTrain Acc : 89.76% | Val. Acc : 88.91%\n","\tTrain Rec : 83.25% | Val. Rec : 82.00%\n","\tTrain Prec: 85.41% | Val. Prec: 83.68%\n","\tTrain F1  : 83.68% | Val. F1  : 82.24%\n","Epoch: 04 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.206 | Val. Loss: 0.263\n","\tTrain Acc : 91.71% | Val. Acc : 89.16%\n","\tTrain Rec : 86.41% | Val. Rec : 82.12%\n","\tTrain Prec: 88.21% | Val. Prec: 84.46%\n","\tTrain F1  : 86.79% | Val. F1  : 82.54%\n","Early stopping!\n","best valid loss:  0.25923358673850694\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7790530\n","RNN(\n","  (embedding): Embedding(18488, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","3697600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1024\n","2\n","Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.414 | Val. Loss: 0.304\n","\tTrain Acc : 82.25% | Val. Acc : 87.30%\n","\tTrain Rec : 64.90% | Val. Rec : 77.49%\n","\tTrain Prec: 69.61% | Val. Prec: 81.70%\n","\tTrain F1  : 64.92% | Val. F1  : 78.55%\n","Epoch: 02 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.342 | Val. Loss: 0.289\n","\tTrain Acc : 85.45% | Val. Acc : 88.34%\n","\tTrain Rec : 73.53% | Val. Rec : 77.59%\n","\tTrain Prec: 79.78% | Val. Prec: 84.72%\n","\tTrain F1  : 75.10% | Val. F1  : 79.50%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.320 | Val. Loss: 0.289\n","\tTrain Acc : 86.50% | Val. Acc : 87.48%\n","\tTrain Rec : 76.29% | Val. Rec : 74.26%\n","\tTrain Prec: 81.38% | Val. Prec: 83.95%\n","\tTrain F1  : 77.59% | Val. F1  : 76.77%\n","Early stopping!\n","best valid loss:  0.25923358673850694\n"],"name":"stdout"}]}]}