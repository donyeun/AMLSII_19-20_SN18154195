{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[{"file_id":"https://github.com/donyeun/AMLSII_19-20_SN18154195/blob/master/src/original.ipynb","timestamp":1588589800978}],"collapsed_sections":["iFnxpFTZEe7E"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNQ/2VFI6lq4lVCw1VRgHJE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oTTXAniWmfMi","colab_type":"code","outputId":"0142fc57-0d21-49bd-e46c-36f5302f7979","executionInfo":{"status":"ok","timestamp":1588582680554,"user_tz":-60,"elapsed":1231,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"quCshhPQVHkv","colab_type":"code","colab":{}},"source":["# Please change this into either A or B to switch between the two tasks\n","TASK_NUMBER = 'A'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pe4VG6osyCiF","colab_type":"text"},"source":["# Libraries and Variables\n","\n","* Reading `config.yaml` which contains all ML parameters as well as filepaths\n","* Import all dependencies and libraries\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SkxFAcT8itpN","colab_type":"code","colab":{}},"source":["# load configuration file that store all the constant and parameters settings\n","import yaml\n","CONFIG_YAML_FILEPATH = '/content/drive/My Drive/public/AMLSII_19-20_SN18154195/config.yaml'\n","with open(CONFIG_YAML_FILEPATH, 'r') as file:\n","  cfg = yaml.safe_load(file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdV1CMmWUxiS","colab_type":"code","colab":{}},"source":["if TASK_NUMBER == 'A':\n","  train_csv_filepath = cfg['paths']['cleaned_train_a']\n","  test_csv_filepath = cfg['paths']['cleaned_test_a']\n","  saved_model_filepath = cfg['paths']['task_a_model']\n","elif TASK_NUMBER == 'B':\n","  train_csv_filepath = cfg['paths']['cleaned_train_b']\n","  test_csv_filepath = cfg['paths']['cleaned_test_b']\n","  saved_model_filepath = cfg['paths']['task_b_model']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5LCvzjssSJf","colab_type":"code","outputId":"37aeea2c-79cd-483f-88db-a19dc4fea878","executionInfo":{"status":"ok","timestamp":1588582694666,"user_tz":-60,"elapsed":15308,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# install additional dependencies\n","! pip install -r {cfg['paths']['requirements']}\n","\n","import pandas as pd\n","import os\n","import torch\n","from tqdm import tqdm\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import nltk\n","import csv\n","import numpy as np\n","# import tensorboard as tb\n","# import tensorflow as tf\n","\n","\n","# # load the TensorBoard notebook extension\n","# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n","# %load_ext tensorboard"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.2.0rc3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (1.0.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.2.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (3.2.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 5)) (4.38.0)\n","Collecting ekphrasis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n","\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 7)) (3.2.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 8)) (3.13)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 9)) (1.5.0+cu101)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 10)) (1.5.0)\n","Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.2.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.18.3)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.9.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.34.2)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.12.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.12.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (3.10.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (1.28.1)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 1)) (2.10.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 2)) (2.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.6.0.post3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.2.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.7.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (46.1.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 4)) (0.10.0)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n","Collecting ujson\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/e4/a79c57e22d6d09bbeb5e8febb8cfa0fe10ede69eed9c3458d3ec99014e20/ujson-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (174kB)\n","\u001b[K     |████████████████████████████████| 184kB 14.8MB/s \n","\u001b[?25hCollecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 9)) (0.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 10)) (7.0.0)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (4.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 6)) (0.1.9)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/public/AMLSII_19-20_SN18154195/requirements.txt (line 3)) (3.1.0)\n","Building wheels for collected packages: ekphrasis, ftfy\n","  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82843 sha256=bd68edaa8299a74def816af77ff5af95a75b1dd2df92907f37b1f1fcbd6252ab\n","  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=9fd8c31a542a920828ede39bb7f10215e9104f0a7398e504c25d6db20c67d51b\n","  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n","Successfully built ekphrasis ftfy\n","Installing collected packages: colorama, ujson, ftfy, ekphrasis\n","Successfully installed colorama-0.4.3 ekphrasis-0.5.1 ftfy-5.7 ujson-2.0.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RuEK2LfkrPYV","colab_type":"text"},"source":["# Load Dataset and Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"6EVaQRfoFLyT","colab_type":"text"},"source":["## Preprocess Additional Dataset\n","In task A, there are some additional datasets (sms and livejournal datasets) that were given from the competition apart from the standard twitter corpus. We can use both of these additional datasets, after we preprocess the formatting so that it matches the rest of the twitter datasets."]},{"cell_type":"code","metadata":{"id":"RUAJontBFp21","colab_type":"code","outputId":"8341f96f-62f1-4542-b30c-3bbb48e6ccd2","executionInfo":{"status":"ok","timestamp":1588582697085,"user_tz":-60,"elapsed":17719,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["task_a_train_additional_dfs = {}\n","\n","for filename in cfg['paths']['train_additional_dataset_filenames']:\n","  # read additional corpora\n","  task_a_train_additional_dfs[filename] = pd.read_csv(os.path.join(cfg['paths']['train_folder_task_a'], filename), sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n","  \n","  if filename == 'livejournal-2014test-A.tsv':\n","    # remove the 0-th column from livejournal corpora, as it is unnecessary\n","    task_a_train_additional_dfs[filename] = task_a_train_additional_dfs[filename].drop(columns=[0])\n","  elif filename == 'sms-2013test-A.tsv':\n","    # remove the 1st column from sms corpora, as it is unnecessary\n","    task_a_train_additional_dfs[filename] = task_a_train_additional_dfs[filename].drop(columns=[1])\n","  \n","  # reset the column index to make it incremental\n","  task_a_train_additional_dfs[filename].columns = range(task_a_train_additional_dfs[filename].shape[1])\n","  \n","  print(task_a_train_additional_dfs[filename])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["             0         1                                                  2\n","0     LJ111111  negative  I know I missed something here , but what does...\n","1     LJ111113   neutral  What do you think of Beside Ourselves as a tit...\n","2     LJ111114  positive                    :D I intend to be one someday .\n","3     LJ111117  negative  LLLINKKK LLLINKKK IIIMAGEEELLLINKKK The choice...\n","4     LJ111119   neutral                     LLLINKKK Some more mountains .\n","...        ...       ...                                                ...\n","1137  LJ113616  positive                     Maybe it was - his - fantasy ?\n","1138  LJ113618  negative  It was ok , but they always just seem so nervo...\n","1139  LJ113621  positive  It is streamable from YepRoc -- matter of fact...\n","1140  LJ113623  positive  comment telling me who you are , or how you fo...\n","1141  LJ113625   neutral  im on myspace ... ill try and find you and add...\n","\n","[1142 rows x 3 columns]\n","          0         1                                                  2\n","0     10936   neutral  Yes i am going from school have class till 5 c...\n","1     11051   neutral  can u tape the match for me?  i\\u2019ll rush o...\n","2     10966   neutral  Too many people at my house my relatives are h...\n","3     11211  negative  Yea I have spoken to him liao. Indeed he is up...\n","4     11350  positive  Haha... I want to see. E macdonalds here cheap...\n","...     ...       ...                                                ...\n","2089  10038  negative  Oki... Think i\\u2019m confused... I only know ...\n","2090  11799   neutral  Yup... Ok i go home look at the timings then i...\n","2091  11945   neutral            Here got lots of hair dresser fr china.\n","2092  10154   neutral  no alh  we are not discussing fromt he viewpoi...\n","2093  11428   neutral  Not dat i dun wan sign up but i wan only for a...\n","\n","[2094 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FH283206E6BF","colab_type":"text"},"source":["## Make Datasets As Pandas' DataFrames and Cleaning Dataset\n","\n"]},{"cell_type":"code","metadata":{"id":"LgC4YYhHrtdb","colab_type":"code","colab":{}},"source":["def append_txt_files_as_one_dataframe(folderpath, filename_keywords_list, additional_dataset_dfs=None):\n","  dataset_per_file_dfs = {}\n","  dataset_df = pd.DataFrame()\n","  filenames = os.listdir(folderpath)\n","  \n","  # open txt files (in tsv formatting)\n","  for filename in filenames:\n","    # if the filename contains a keyword in the filename_keywords_list, then open the txt file\n","    # this is to avoid opening unnecessary txt such as readme.txt file.\n","    if any(keyword in filename for keyword in filename_keywords_list):\n","      dataset_per_file_dfs[filename] = pd.read_csv(os.path.join(folderpath, filename), sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n","      print('shape : ', dataset_per_file_dfs[filename].shape, '\\tfilename: ', filename)\n","\n","  # combine the original dataset with additional dataset (if any)\n","  if additional_dataset_dfs is not None:\n","    for key, value in additional_dataset_dfs.items():\n","      dataset_per_file_dfs[key] = value\n","\n","  # append all the files as one dataframe\n","  for key, value in dataset_per_file_dfs.items():\n","    # print(key, '\\t', i, '\\t', dataset_per_file_df[key].shape[0])\n","    dataset_df = dataset_df.append(dataset_per_file_dfs[key], ignore_index=True)\n","  return dataset_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXBTf96GQeBI","colab_type":"code","colab":{}},"source":["def clean_dataframe_format(df, new_column_name_list, drop_column_list=[]):\n","  # drop unnecessary column\n","  df = df.drop(columns=drop_column_list)\n","  \n","  # rename column\n","  df.columns = new_column_name_list\n","\n","  # remove row in dataframe if the 'text' or 'sentiment' column value is missing\n","  df = df.dropna(subset=['sentiment', 'text'], how='any').reset_index(drop=True)\n","  \n","  # remove row if the sentiment is not 'positive', 'negative' or 'neutral'\n","  # this happens in the dataset, for example, there are some rows\n","  # where its sentiments are 'off topic'\n","  valid_sentiments = ['positive', 'negative', 'neutral']\n","  df = df[df['sentiment'].isin(valid_sentiments)].reset_index(drop=True)\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"znGSEFNntmVa","colab_type":"code","outputId":"515cd912-772f-416d-ffbb-62c29c795b12","executionInfo":{"status":"ok","timestamp":1588582700232,"user_tz":-60,"elapsed":20847,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# read dataset files and append it as one pandas dataframe\n","if cfg['task_a']['use_additional_dataset']:\n","  task_a_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_a'], ['twitter'], task_a_train_additional_dfs)\n","else:\n","  task_a_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_a'], ['twitter'])\n","\n","task_a_train_df = clean_dataframe_format(task_a_train_df, ['id', 'sentiment', 'text'], drop_column_list=[3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape :  (2000, 3) \tfilename:  twitter-2016devtest-A.txt\n","shape :  (1999, 3) \tfilename:  twitter-2016dev-A.txt\n","shape :  (6000, 3) \tfilename:  twitter-2016train-A.txt\n","shape :  (1654, 3) \tfilename:  twitter-2013dev-A.txt\n","shape :  (3547, 3) \tfilename:  twitter-2013test-A.txt\n","shape :  (9684, 3) \tfilename:  twitter-2013train-A.txt\n","shape :  (1853, 3) \tfilename:  twitter-2014test-A.txt\n","shape :  (2390, 3) \tfilename:  twitter-2015test-A.txt\n","shape :  (489, 3) \tfilename:  twitter-2015train-A.txt\n","shape :  (86, 3) \tfilename:  twitter-2014sarcasm-A.txt\n","shape :  (20633, 4) \tfilename:  twitter-2016test-A.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R_Aem1fwuZB8","colab_type":"code","outputId":"92092e5a-3534-4609-f899-dc0de625a3bd","executionInfo":{"status":"ok","timestamp":1588582702504,"user_tz":-60,"elapsed":23110,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["task_b_train_df = append_txt_files_as_one_dataframe(cfg['paths']['train_folder_task_b'], ['twitter'])\n","task_b_train_df = clean_dataframe_format(task_b_train_df, ['id', 'topic','sentiment', 'text'], drop_column_list=[4])\n","task_b_train_df.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape :  (4346, 4) \tfilename:  twitter-2016train-BD.txt\n","shape :  (1325, 4) \tfilename:  twitter-2016dev-BD.txt\n","shape :  (1417, 4) \tfilename:  twitter-2016devtest-BD.txt\n","shape :  (489, 4) \tfilename:  twitter-2015train-BD.txt\n","shape :  (10552, 5) \tfilename:  twitter-2016test-BD.txt\n","shape :  (2383, 5) \tfilename:  twitter-2015testBD.txt\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20508 entries, 0 to 20507\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   id         20508 non-null  object\n"," 1   topic      20508 non-null  object\n"," 2   sentiment  20508 non-null  object\n"," 3   text       20508 non-null  object\n","dtypes: object(4)\n","memory usage: 641.0+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1iApuaK7wj9O","colab_type":"code","outputId":"948b9c7e-72a9-404a-ec55-03f008bd1410","executionInfo":{"status":"ok","timestamp":1588582702842,"user_tz":-60,"elapsed":23441,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["task_a_test_df = pd.read_csv(cfg['paths']['test_file_task_a'], sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n","task_a_test_df = clean_dataframe_format(task_a_test_df, ['id', 'sentiment', 'text'])\n","task_a_test_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>801989080477154944</td>\n","      <td>neutral</td>\n","      <td>#ArianaGrande Ari By Ariana Grande 80% Full ht...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>801989272341453952</td>\n","      <td>positive</td>\n","      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>801990978424962944</td>\n","      <td>positive</td>\n","      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>801996232553963008</td>\n","      <td>positive</td>\n","      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>801998343442407040</td>\n","      <td>neutral</td>\n","      <td>SIDE TO SIDE 😘 @arianagrande #sidetoside #aria...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12279</th>\n","      <td>805699615781625856</td>\n","      <td>positive</td>\n","      <td>@dansen17 update: Zac Efron kissing a puppy ht...</td>\n","    </tr>\n","    <tr>\n","      <th>12280</th>\n","      <td>805701709356003328</td>\n","      <td>neutral</td>\n","      <td>#zac efron sex pic skins michelle sex https://...</td>\n","    </tr>\n","    <tr>\n","      <th>12281</th>\n","      <td>805701818357579776</td>\n","      <td>neutral</td>\n","      <td>First Look at Neighbors 2 with Zac Efron Shirt...</td>\n","    </tr>\n","    <tr>\n","      <th>12282</th>\n","      <td>805703557081075712</td>\n","      <td>neutral</td>\n","      <td>zac efron poses nude #lovely libra porn https:...</td>\n","    </tr>\n","    <tr>\n","      <th>12283</th>\n","      <td>805704324105940992</td>\n","      <td>neutral</td>\n","      <td>#Fashion #Style The Paperboy (NEW Blu-ray Disc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12284 rows × 3 columns</p>\n","</div>"],"text/plain":["                       id  ...                                               text\n","0      801989080477154944  ...  #ArianaGrande Ari By Ariana Grande 80% Full ht...\n","1      801989272341453952  ...  Ariana Grande KIIS FM Yours Truly CD listening...\n","2      801990978424962944  ...  Ariana Grande White House Easter Egg Roll in W...\n","3      801996232553963008  ...  #CD #Musics Ariana Grande Sweet Like Candy 3.4...\n","4      801998343442407040  ...  SIDE TO SIDE 😘 @arianagrande #sidetoside #aria...\n","...                   ...  ...                                                ...\n","12279  805699615781625856  ...  @dansen17 update: Zac Efron kissing a puppy ht...\n","12280  805701709356003328  ...  #zac efron sex pic skins michelle sex https://...\n","12281  805701818357579776  ...  First Look at Neighbors 2 with Zac Efron Shirt...\n","12282  805703557081075712  ...  zac efron poses nude #lovely libra porn https:...\n","12283  805704324105940992  ...  #Fashion #Style The Paperboy (NEW Blu-ray Disc...\n","\n","[12284 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"KqE-Y6XJBeA2","colab_type":"code","outputId":"305bec58-01d0-4149-d0a6-5ddc0fede0a6","executionInfo":{"status":"ok","timestamp":1588582703061,"user_tz":-60,"elapsed":23654,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["task_b_test_df = pd.read_csv(cfg['paths']['test_file_task_b'], sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n","task_b_test_df = clean_dataframe_format(task_b_test_df, ['id', 'topic', 'sentiment', 'text'])\n","task_b_test_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>topic</th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>801989272341453952</td>\n","      <td>#ArianaGrande</td>\n","      <td>positive</td>\n","      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>801990978424962944</td>\n","      <td>#ArianaGrande</td>\n","      <td>positive</td>\n","      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>801996232553963008</td>\n","      <td>#ArianaGrande</td>\n","      <td>positive</td>\n","      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>801998343442407040</td>\n","      <td>#ArianaGrande</td>\n","      <td>positive</td>\n","      <td>SIDE TO SIDE 😘 @arianagrande #sidetoside #aria...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>802001659970744064</td>\n","      <td>#ArianaGrande</td>\n","      <td>positive</td>\n","      <td>Hairspray Live! Previews at the Macy's Thanksg...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6180</th>\n","      <td>805696468959002624</td>\n","      <td>zac efron</td>\n","      <td>positive</td>\n","      <td>Abby *talking about the Hamilton soundtrack\": ...</td>\n","    </tr>\n","    <tr>\n","      <th>6181</th>\n","      <td>805699412257181697</td>\n","      <td>zac efron</td>\n","      <td>positive</td>\n","      <td>can we like get zac efron or justin bieber for...</td>\n","    </tr>\n","    <tr>\n","      <th>6182</th>\n","      <td>805699615781625856</td>\n","      <td>zac efron</td>\n","      <td>positive</td>\n","      <td>@dansen17 update: Zac Efron kissing a puppy ht...</td>\n","    </tr>\n","    <tr>\n","      <th>6183</th>\n","      <td>805701818357579776</td>\n","      <td>zac efron</td>\n","      <td>positive</td>\n","      <td>First Look at Neighbors 2 with Zac Efron Shirt...</td>\n","    </tr>\n","    <tr>\n","      <th>6184</th>\n","      <td>805703557081075712</td>\n","      <td>zac efron</td>\n","      <td>positive</td>\n","      <td>zac efron poses nude #lovely libra porn https:...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6185 rows × 4 columns</p>\n","</div>"],"text/plain":["                      id  ...                                               text\n","0     801989272341453952  ...  Ariana Grande KIIS FM Yours Truly CD listening...\n","1     801990978424962944  ...  Ariana Grande White House Easter Egg Roll in W...\n","2     801996232553963008  ...  #CD #Musics Ariana Grande Sweet Like Candy 3.4...\n","3     801998343442407040  ...  SIDE TO SIDE 😘 @arianagrande #sidetoside #aria...\n","4     802001659970744064  ...  Hairspray Live! Previews at the Macy's Thanksg...\n","...                  ...  ...                                                ...\n","6180  805696468959002624  ...  Abby *talking about the Hamilton soundtrack\": ...\n","6181  805699412257181697  ...  can we like get zac efron or justin bieber for...\n","6182  805699615781625856  ...  @dansen17 update: Zac Efron kissing a puppy ht...\n","6183  805701818357579776  ...  First Look at Neighbors 2 with Zac Efron Shirt...\n","6184  805703557081075712  ...  zac efron poses nude #lovely libra porn https:...\n","\n","[6185 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"0OirqNXwXfQx","colab_type":"code","colab":{}},"source":["# Checking and compare the frequency with what's written in the paper\n","# # save dataset as csv file\n","# task_a_train_df.to_csv('coba_train_a.csv', sep='\\t')\n","\n","# assert task_a_train_df[task_a_train_df['sentiment'] == 'positive'].shape[0] == 19902  #SALAH\n","assert task_a_train_df[task_a_train_df['sentiment'] == 'negative'].shape[0] == 7840\n","assert task_a_train_df[task_a_train_df['sentiment'] == 'neutral'].shape[0] == 22591\n","\n","assert task_a_test_df[task_a_test_df['sentiment'] == 'positive'].shape[0] == 2375\n","assert task_a_test_df[task_a_test_df['sentiment'] == 'negative'].shape[0] == 3972\n","assert task_a_test_df[task_a_test_df['sentiment'] == 'neutral'].shape[0] == 5937\n","\n","assert len(task_b_train_df['topic'].unique()) == 373\n","assert task_b_train_df[task_b_train_df['sentiment'] == 'positive'].shape[0] == 14951\n","assert task_b_train_df[task_b_train_df['sentiment'] == 'negative'].shape[0] == 4013\n","assert task_b_train_df[task_b_train_df['sentiment'] == 'neutral'].shape[0] == 1544\n","\n","\n","assert len(task_b_test_df['topic'].unique()) == 125\n","assert task_b_test_df[task_b_test_df['sentiment'] == 'positive'].shape[0] == 2463\n","assert task_b_test_df[task_b_test_df['sentiment'] == 'negative'].shape[0] == 3722"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tg90yjiKuw9Z","colab_type":"code","colab":{}},"source":["# save the cleaned datasets as csv\n","def df_to_csv(df, filtered_cols=[], csv_filename='output.csv', index_label='row_num'):\n","  '''\n","    df           : the panda dataframe\n","    filtered_cols: if not empty, it will only include the listed column name\n","    csv_filename : the csv filename\n","    index_label  : column name for the dataframe's index in the csv\n","  '''\n","  # filtering columns\n","  if len(filtered_cols) > 0:\n","    df = df[filtered_cols]\n","    \n","  # save to csv\n","  df.to_csv(\n","      csv_filename,\n","      sep='\\t',\n","      index_label=index_label\n","  )\n","\n","df_to_csv(task_a_train_df, ['text', 'sentiment'], cfg['paths']['cleaned_train_a'])\n","df_to_csv(task_a_test_df, ['text', 'sentiment'], cfg['paths']['cleaned_test_a'])\n","\n","# append the topic to the text for task B\n","task_b_train_df['ori_text'] = task_b_train_df['text']\n","task_b_train_df['text'] = task_b_train_df['text'].map(str) + \" \" + task_b_train_df['topic']\n","\n","task_b_test_df['ori_text'] = task_b_test_df['text']\n","task_b_test_df['text'] = task_b_test_df['text'].map(str) + \" \" + task_b_train_df['topic']\n","\n","# cut sentiments other than positive&negative (cut the neutral ones)\n","task_b_train_df = task_b_train_df[(task_b_train_df['sentiment'] == 'positive') | (task_b_train_df['sentiment'] == 'negative')]\n","\n","df_to_csv(task_b_train_df, ['text', 'sentiment', 'topic', 'ori_text'], cfg['paths']['cleaned_train_b'])\n","df_to_csv(task_b_test_df, ['text', 'sentiment', 'topic'], cfg['paths']['cleaned_test_b'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFnxpFTZEe7E","colab_type":"text"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"code","metadata":{"id":"d7YGmqNd2WrP","colab_type":"code","outputId":"343e01b3-3eab-47a0-b6e5-26c1461b9589","executionInfo":{"status":"ok","timestamp":1588582706635,"user_tz":-60,"elapsed":27181,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["def show_autopct(pct, data):\n","  absolute = int(pct/100.*np.sum(data))\n","  return  \"{:d}\\n({:.1f}%)\".format(absolute, pct)\n","  \n","plt.subplot(1, 2, 1)\n","task_a_train_df['sentiment'].value_counts().plot(\n","    kind='pie',\n","    autopct=lambda pct: show_autopct(pct, task_a_train_df['sentiment'].value_counts()),\n","    colors=['lightgray', 'darkseagreen', 'lightcoral']\n",");\n","plt.title('Training Set');\n","\n","\n","plt.subplot(1, 2, 2)\n","task_a_test_df['sentiment'].value_counts().plot(\n","    kind='pie',\n","    # figsize=(7,4),\n","    autopct=lambda pct: show_autopct(pct, task_a_test_df['sentiment'].value_counts()),\n","    colors=['lightgray', 'darkseagreen', 'lightcoral']\n",");\n","plt.title('Testing Set');\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Gauu8B_i9XHn","colab_type":"code","outputId":"246fec13-29aa-4f0f-f388-918d09c2efa7","executionInfo":{"status":"ok","timestamp":1588582706647,"user_tz":-60,"elapsed":27186,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":487}},"source":["task_b_train_df[task_b_train_df['text'].str.contains('not')]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>topic</th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","      <th>ori_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>629226490152914944</td>\n","      <td>@microsoft</td>\n","      <td>positive</td>\n","      <td>Microsoft, I may not prefer your gaming branch...</td>\n","      <td>Microsoft, I may not prefer your gaming branch...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>629797991826722816</td>\n","      <td>@microsoft</td>\n","      <td>negative</td>\n","      <td>After attempting a reinstall, it still bricks,...</td>\n","      <td>After attempting a reinstall, it still bricks,...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>631543121407442946</td>\n","      <td>@microsoft</td>\n","      <td>negative</td>\n","      <td>@Microsoft support for 365 has been terrible. ...</td>\n","      <td>@Microsoft support for 365 has been terrible. ...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>632536348419690496</td>\n","      <td>@microsoft</td>\n","      <td>negative</td>\n","      <td>@eyesonfoxorg @Microsoft I'm still using Vista...</td>\n","      <td>@eyesonfoxorg @Microsoft I'm still using Vista...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>632810315227467776</td>\n","      <td>@microsoft</td>\n","      <td>negative</td>\n","      <td>@MisterMetokur @Microsoft @ATT @NSAGov That's ...</td>\n","      <td>@MisterMetokur @Microsoft @ATT @NSAGov That's ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20377</th>\n","      <td>521536294670065664</td>\n","      <td>welbeck</td>\n","      <td>positive</td>\n","      <td>\"@asadaslamawan today was all about Ozil not b...</td>\n","      <td>\"@asadaslamawan today was all about Ozil not b...</td>\n","    </tr>\n","    <tr>\n","      <th>20433</th>\n","      <td>522106648916393985</td>\n","      <td>wwat</td>\n","      <td>positive</td>\n","      <td>\"GUYS there's a chance I might go back to the ...</td>\n","      <td>\"GUYS there's a chance I might go back to the ...</td>\n","    </tr>\n","    <tr>\n","      <th>20436</th>\n","      <td>521315004289912832</td>\n","      <td>wwat</td>\n","      <td>positive</td>\n","      <td>i may or may not be driving an hour and a half...</td>\n","      <td>i may or may not be driving an hour and a half...</td>\n","    </tr>\n","    <tr>\n","      <th>20474</th>\n","      <td>520384978820296705</td>\n","      <td>yougov</td>\n","      <td>negative</td>\n","      <td>\"@LouiseBaldock great work; you may or may not...</td>\n","      <td>\"@LouiseBaldock great work; you may or may not...</td>\n","    </tr>\n","    <tr>\n","      <th>20506</th>\n","      <td>522912399322394624</td>\n","      <td>younique</td>\n","      <td>positive</td>\n","      <td>Get the best mascara in the industry here! Im ...</td>\n","      <td>Get the best mascara in the industry here! Im ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1521 rows × 5 columns</p>\n","</div>"],"text/plain":["                       id  ...                                           ori_text\n","4      629226490152914944  ...  Microsoft, I may not prefer your gaming branch...\n","8      629797991826722816  ...  After attempting a reinstall, it still bricks,...\n","16     631543121407442946  ...  @Microsoft support for 365 has been terrible. ...\n","20     632536348419690496  ...  @eyesonfoxorg @Microsoft I'm still using Vista...\n","22     632810315227467776  ...  @MisterMetokur @Microsoft @ATT @NSAGov That's ...\n","...                   ...  ...                                                ...\n","20377  521536294670065664  ...  \"@asadaslamawan today was all about Ozil not b...\n","20433  522106648916393985  ...  \"GUYS there's a chance I might go back to the ...\n","20436  521315004289912832  ...  i may or may not be driving an hour and a half...\n","20474  520384978820296705  ...  \"@LouiseBaldock great work; you may or may not...\n","20506  522912399322394624  ...  Get the best mascara in the industry here! Im ...\n","\n","[1521 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"nwFx7A43Aju4","colab_type":"code","outputId":"1f575043-ef79-40c4-a17c-3b2f3ce328a0","executionInfo":{"status":"ok","timestamp":1588582706647,"user_tz":-60,"elapsed":27182,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["positive = task_b_train_df[task_b_train_df['sentiment'] == 'positive']['topic'].value_counts(sort=False).to_frame(name='positive')\n","negative = task_b_train_df[task_b_train_df['sentiment'] == 'negative']['topic'].value_counts(sort=False).to_frame(name='negative')\n","neutral = task_b_train_df[task_b_train_df['sentiment'] == 'neutral']['topic'].value_counts(sort=False).to_frame(name='neutral')\n","# negative = task_b_train_df['topic'][task_b_train_df['sentiment'] == 'negative'].value_counts(sort=False)\n","# neutral = task_b_train_df['topic'][task_b_train_df['sentiment'] == 'neutral'].value_counts(sort=False)\n","joined = positive.join(negative).join(neutral)\n","\n","joined = joined[(joined['positive'] > 0) & (joined['negative']>0)]\n","with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n","    print(joined)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                       positive  negative  neutral\n","frank gifford               151       5.0      NaN\n","bolton                        3       1.0      NaN\n","caitlyn jenner               24      69.0      NaN\n","james rodriguez              13       3.0      NaN\n","jan hooks                     3       2.0      NaN\n","the vamps                    55       5.0      NaN\n","justin bieber               117      27.0      NaN\n","ipod                         56       7.0      NaN\n","ventura                       8       2.0      NaN\n","david price                 114       9.0      NaN\n","t-mobile                     44      21.0      NaN\n","vampire diaries              15       1.0      NaN\n","moto g                       58       4.0      NaN\n","chris evans                  94      27.0      NaN\n","lady gaga                    71       5.0      NaN\n","janet jackson                95       3.0      NaN\n","kerry                        39      19.0      NaN\n","jessica                       9       2.0      NaN\n","tom brady                    77      33.0      NaN\n","kris jenner                   4       2.0      NaN\n","seth rollins                 74      32.0      NaN\n","cannavaro                     5       2.0      NaN\n","tvd                           9       2.0      NaN\n","rodgers                       3       5.0      NaN\n","rolling stone                39       8.0      NaN\n","planned parenthood           11      46.0      NaN\n","galatasaray                   3       8.0      NaN\n","gandhi                        5       1.0      NaN\n","dana white                   15      25.0      NaN\n","valentine                    55       4.0      NaN\n","iphone                       65       5.0      NaN\n","david beckham                76       3.0      NaN\n","xbox                         55       7.0      NaN\n","georgia dome                  7       1.0      NaN\n","swachh bharat                 3       2.0      NaN\n","ac/dc                        89       2.0      NaN\n","christian ponder              7       7.0      NaN\n","floyd mayweather             37      27.0      NaN\n","frank ocean                 110      38.0      NaN\n","justin                       69       9.0      NaN\n","randy orton                  36       7.0      NaN\n","charlie hebdo                12      43.0      NaN\n","venice beach                 88       3.0      NaN\n","galaxy note                  71       6.0      NaN\n","nike                         55       9.0      NaN\n","aaron samuels                 2       4.0      NaN\n","asiata                        1       4.0      NaN\n","netflix                      88       6.0      NaN\n","apple watch                  53       8.0      NaN\n","europa league                 3       2.0      NaN\n","careless world                6       1.0      NaN\n","chelsea                      59      10.0      NaN\n","kanye west                   70      71.0      NaN\n","a$ap rocky                    6       1.0      NaN\n","fleetwood mac                89       3.0      NaN\n","apple                        57      11.0      NaN\n","snoop dogg                   61       6.0      NaN\n","sam smith                   149      17.0      NaN\n","boko haram                    2     149.0      NaN\n","dustin johnson               91      31.0      NaN\n","packers                       6       2.0      NaN\n","barca                        69       6.0      NaN\n","jason aldean                138       2.0      NaN\n","lsu                           4       1.0      NaN\n","trump                        26      42.0      NaN\n","kyle                          9       4.0      NaN\n","serena                       73       1.0      NaN\n","islam                        68      69.0      NaN\n","curtis painter                5       7.0      NaN\n","colts                        10       1.0      NaN\n","aaron rodgers                13       3.0      NaN\n","brook lopez                   7       3.0      NaN\n","ducks                         8       1.0      NaN\n","chuck norris                 32       6.0      NaN\n","bernie sanders               76       9.0      NaN\n","beyonce                      84       2.0      NaN\n","foo fighters                228       3.0      NaN\n","carling cup final             3       1.0      NaN\n","miss usa                     41       6.0      NaN\n","fabian delph                  4       3.0      NaN\n","google+                      44      19.0      NaN\n","nokia                        59       9.0      NaN\n","super eagles                 38       2.0      NaN\n","tom cruise                   61      13.0      NaN\n","redskins                      4       2.0      NaN\n","donny                         6       3.0      NaN\n","cupid                         1       2.0      NaN\n","kesha                         6       2.0      NaN\n","zayn                        107      10.0      NaN\n","george harrison              48       1.0      NaN\n","denzel                        5       1.0      NaN\n","boehner                       1       7.0      NaN\n","xmas                          8       2.0      NaN\n","game of thrones              83       3.0      NaN\n","cfb                           2       1.0      NaN\n","independence day              6       1.0      NaN\n","ios                          47       5.0      NaN\n","messi                        74       3.0      NaN\n","taylor swift                 84       1.0      NaN\n","hulk hogan                   44      50.0      NaN\n","ice cube                    134      14.0      NaN\n","rick perry                   23      41.0      NaN\n","school                        3      10.0      NaN\n","john kasich                  48      24.0      NaN\n","watchman                     55       5.0      NaN\n","muslims                      30      96.0      NaN\n","eid                         123       4.0      NaN\n","ukip                         28      31.0      NaN\n","amazon                       57      16.0      NaN\n","delhi police                  2       2.0      NaN\n","icc                           5       1.0      NaN\n","kendrick                    147      32.0      NaN\n","brit award                    9       1.0      NaN\n","ruto                          3       3.0      NaN\n","pope                         58       5.0      NaN\n","kris bryant                  93       8.0      NaN\n","labor day                    61       7.0      NaN\n","academy awards                1       2.0      NaN\n","grateful dead                86       2.0      NaN\n","harper                       18      41.0      NaN\n","paul mccartney              115       1.0      NaN\n","ahs                          10       3.0      NaN\n","spx                           4       2.0      NaN\n","bill maher                    4       6.0      NaN\n","kershaw                       4       2.0      NaN\n","abc                           4       2.0      NaN\n","michael jackson              75       1.0      NaN\n","david taylor                  1       1.0      NaN\n","green day                     8       1.0      NaN\n","juventus                     35      11.0      NaN\n","cowboys                       6       3.0      NaN\n","boise state                   4       1.0      NaN\n","ira                          10     111.0      NaN\n","kane                         62      31.0      NaN\n","windows 10                   43      18.0      NaN\n","vikings                       5       3.0      NaN\n","ed sheeran                  163       5.0      NaN\n","twilight                    114      28.0      NaN\n","mariah carey                 77       3.0      NaN\n","atleti                        3       3.0      NaN\n","adidas                        1       1.0      NaN\n","gilmore girls                 7       4.0      NaN\n","bulls                         9       1.0      NaN\n","milan                       142      18.0      NaN\n","metlife                      69       5.0      NaN\n","digi                         72       2.0      NaN\n","gucci                        68      18.0      NaN\n","bobby jindal                 19      37.0      NaN\n","oracle                       46       8.0      NaN\n","mikasa                       11       1.0      NaN\n","ipad                         63       4.0      NaN\n","batman                       64       5.0      NaN\n","brian cushing                 3       6.0      NaN\n","jurassic park                86       2.0      NaN\n","josh hamilton                47       6.0      NaN\n","hannibal                     82       4.0      NaN\n","red bull                      5       3.0      NaN\n","eric church                  90       3.0      NaN\n","ps4                         105      10.0      NaN\n","white sox                    38      15.0      NaN\n","american horror story         8       5.0      NaN\n","bentley                      65       3.0      NaN\n","shawn                       155       5.0      NaN\n","green bay                     2       2.0      NaN\n","leeds united                  8       1.0      NaN\n","sting                        77       2.0      NaN\n","iran                         15      61.0      NaN\n","stoops                        3       5.0      NaN\n","iron maiden                 133       3.0      NaN\n","nirvana                     126       3.0      NaN\n","lee soo man                  12       2.0      NaN\n","alonso                        1       7.0      NaN\n","john cena                   121      24.0      NaN\n","chris brown                  76       6.0      NaN\n","friday night lights          12       1.0      NaN\n","cate blanchett               41       2.0      NaN\n","jeb bush                      4      55.0      NaN\n","russell wilson                3       4.0      NaN\n","rbi                           4       1.0      NaN\n","amazon prime                 70      18.0      NaN\n","ancelotti                    10       2.0      NaN\n","valentine 's day             54       7.0      NaN\n","lax                           5       1.0      NaN\n","sharknado                   132      39.0      NaN\n","atletico madrid               4       1.0      NaN\n","tegan                         9       2.0      NaN\n","modi                          6       4.0      NaN\n","nintendo                     54       8.0      NaN\n","scott walker                 18      54.0      NaN\n","star wars                    88       2.0      NaN\n","disneyland                   92       2.0      NaN\n","harry potter                 80       2.0      NaN\n","kurt cobain                  61       7.0      NaN\n","big brother                 135      38.0      NaN\n","conor mcgregor               80       4.0      NaN\n","ryan braun                   51       9.0      NaN\n","prince george               125       2.0      NaN\n","narendra modi                 4       1.0      NaN\n","rousey                       61      23.0      NaN\n","yakub                         5     105.0      NaN\n","brian kelly                   6       3.0      NaN\n","charles darwin                1       1.0      NaN\n","ted 2                        81       7.0      NaN\n","pride parade                 75       2.0      NaN\n","wwat                         13       1.0      NaN\n","dark souls                   73      14.0      NaN\n","naruto                      152      10.0      NaN\n","saudi arabia                 14      57.0      NaN\n","wednesday adams               7       3.0      NaN\n","arian foster                  8       3.0      NaN\n","white house                   2       1.0      NaN\n","dunkin                      156      18.0      NaN\n","demi lovato                  11       1.0      NaN\n","sarah palin                   7      53.0      NaN\n","bob marley                   89       3.0      NaN\n","federer                      75       1.0      NaN\n","kim kardashian               34      23.0      NaN\n","tgif                         84       2.0      NaN\n","gay                          50      45.0      NaN\n","democrats                     1       4.0      NaN\n","red sox                      73      22.0      NaN\n","niall                       130      14.0      NaN\n","kobe bryant                  13       4.0      NaN\n","kendrick lamar               89      16.0      NaN\n","bbc                          36      16.0      NaN\n","jennifer lawrence             7       1.0      NaN\n","bad blood                    49      17.0      NaN\n","david cameron                25      38.0      NaN\n","carlo ancelotti               9       3.0      NaN\n","ghoncheh ghavami              3       1.0      NaN\n","lexus                        70       5.0      NaN\n","arsenal                      45      15.0      NaN\n","paul dunne                  142      10.0      NaN\n","nicki                        99      32.0      NaN\n","zac brown band              110       3.0      NaN\n","carly fiorina               108      17.0      NaN\n","christians                   56      50.0      NaN\n","real madrid                  19      16.0      NaN\n","ihop                         81       3.0      NaN\n","jurassic world               84       5.0      NaN\n","ric flair                    61       1.0      NaN\n","yoga                         80       1.0      NaN\n","carling cup                   3       1.0      NaN\n","michelle obama               47      17.0      NaN\n","george osborne               29      39.0      NaN\n","tony blair                   10      65.0      NaN\n","murray                       90      24.0      NaN\n","zlatan                       60       4.0      NaN\n","ashton                       12       2.0      NaN\n","selena                        8       1.0      NaN\n","yougov                        5       3.0      NaN\n","jimmy fallon                  8       1.0      NaN\n","amy schumer                  47      13.0      NaN\n","afghans                       1       2.0      NaN\n","monster hunter                9       1.0      NaN\n","angela merkel                23      23.0      NaN\n","tiger woods                  46      19.0      NaN\n","sony                         49      14.0      NaN\n","national hot dog day        148       3.0      NaN\n","brock lesnar                108      14.0      NaN\n","paper towns                 167       8.0      NaN\n","ibm                          64       5.0      NaN\n","david wright                 92       7.0      NaN\n","ant-man                     221       8.0      NaN\n","amazon prime day             43      43.0      NaN\n","kpop                         67       9.0      NaN\n","joe biden                    40      18.0      NaN\n","nabeel rajab                  1       3.0      NaN\n","scotus                       18      31.0      NaN\n","bellusci                      4       1.0      NaN\n","the big bang theory           9       1.0      NaN\n","liam                         11       2.0      NaN\n","david bowie                  93       3.0      NaN\n","madonna                      77       2.0      NaN\n","disney                       12       1.0      NaN\n","dean ambrose                 21       4.0      NaN\n","thor                        138      18.0      NaN\n","seinfeld                     75       2.0      NaN\n","@microsoft                   34      46.0      NaN\n","katy perry                   57       8.0      NaN\n","act                           1       2.0      NaN\n","u2                           85       8.0      NaN\n","magic mike xxl               89       3.0      NaN\n","jay-z                        59      14.0      NaN\n","chris maguire                 2       1.0      NaN\n","hillary                      22      48.0      NaN\n","obama                        21      40.0      NaN\n","james franklin                2      10.0      NaN\n","donald trump                 16      44.0      NaN\n","rebecca black                 6       3.0      NaN\n","simmons                      13       1.0      NaN\n","tory                         14      46.0      NaN\n","minecraft                    82       1.0      NaN\n","google                       35      15.0      NaN\n","curtis                       86      13.0      NaN\n","tsipras                      25      21.0      NaN\n","briana                       48      44.0      NaN\n","monsanto                      7      54.0      NaN\n","vettel                        3       2.0      NaN\n","ucla                          2       1.0      NaN\n","israel                       50      63.0      NaN\n","rahul gandhi                 28      46.0      NaN\n","erdogan                       3      58.0      NaN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nm_M0CXUAsAn","colab_type":"code","outputId":"168a854d-8c39-4212-e45c-21d13d8a0467","executionInfo":{"status":"ok","timestamp":1588582707852,"user_tz":-60,"elapsed":28382,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["\n","plt.bar(range(joined.shape[0]), joined['positive'], label='positive', color='blue')\n","plt.bar(range(joined.shape[0]), joined['negative'], label='negative', color='red', bottom=joined['positive'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BarContainer object of 303 artists>"]},"metadata":{"tags":[]},"execution_count":18},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"vwJSV9cfh9Q6","colab_type":"text"},"source":["# Building Classifier"]},{"cell_type":"markdown","metadata":{"id":"D0KvskijQ9H9","colab_type":"text"},"source":["## Build Dataset\n"]},{"cell_type":"code","metadata":{"id":"XzWqeZzyvFKH","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader, Dataset\n","from torchtext.data import TabularDataset, Field, LabelField, BucketIterator\n","from sklearn.preprocessing import LabelEncoder\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","from ekphrasis.classes.preprocessor import TextPreProcessor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-_FIEeLyTFM","colab_type":"code","colab":{}},"source":["# read csv file as torchtext's TabularDataset\n","def csv_to_tabular_dataset(filepath, fields):\n","  tabular_daset = TabularDataset(\n","      path=filepath,\n","      fields = fields,\n","      format='tsv',\n","      skip_header=True\n","  )\n","  return tabular_daset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpIHw_3YdSAn","colab_type":"text"},"source":["## Neural Network"]},{"cell_type":"markdown","metadata":{"id":"2TsmpHiydeyr","colab_type":"text"},"source":["three layers:\n","1. embedding layer (transform one-hot encoding vector into a dense embedding vector)\n","2. RNN\n","3. linear layer (output)"]},{"cell_type":"code","metadata":{"id":"Xo_SPYGZdWuq","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n","               bidirectional, dropout, pad_idx):\n","    super().__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","    # self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","    self.rnn = nn.LSTM(embedding_dim,\n","                       hidden_dim,\n","                       num_layers=n_layers,\n","                       bidirectional=bidirectional,\n","                       dropout=dropout,\n","                       )\n","    self.fully_connected = nn.Linear(hidden_dim*2, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, text, text_length):\n","    # text dimention is [sentence len, batch size]\n","    \n","    # embedded dimention is [sentence len, batch size, embedding_dim]\n","    embedded = self.dropout(self.embedding(text))\n","\n","    # pack the sequence\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n","    # print('packed_embedded:')\n","    # print(packed_embedded)\n","    packet_output, (hidden, cell) = self.rnn(packed_embedded)\n","\n","    # unpack the sequence\n","    output, output_length = nn.utils.rnn.pad_packed_sequence(packet_output)\n","\n","    # output dimention is [sentence len, batch size, hidden dim * num directions]\n","    # output over padding tokens are zero tensors\n","\n","    # hidden dimension is [num layers*num directions, batch size, hidden dim]\n","    # cell dimension is also [num layers*num directions, batch size, hidden dim]\n","\n","    #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","    #and apply dropout\n","\n","    # hidden dimension is [batch size, hid dim*num directions]\n","    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n","    \n","    return self.fully_connected(hidden)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWbAXYOsr68C","colab_type":"code","colab":{}},"source":["def count_model_params(model):\n","  # number of parameters that are trainable\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkG8MGZMvTLi","colab_type":"text"},"source":["## Train the Model"]},{"cell_type":"code","metadata":{"id":"_5cEu9fAwixJ","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n","def calculate_performance(y, preds):\n","  # get the index of the max probability \n","  Y_pred = preds.argmax(dim = 1, keepdim = True).squeeze(1)\n","  Y_pred = Y_pred.detach().cpu().clone().numpy()\n","  \n","  Y_true = y.detach().cpu().clone().numpy()\n","\n","  acc = accuracy_score(Y_true, Y_pred)\n","  rec = recall_score(Y_true, Y_pred, average='macro')\n","  prec= precision_score(Y_true, Y_pred, average='macro')\n","  f1  = f1_score(Y_true, Y_pred, average='macro')\n","  return acc, rec, prec, f1, Y_true, Y_pred\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0wGJ4rww7Fj","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_rec = 0\n","    epoch_prec = 0\n","    epoch_f1 = 0\n","\n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","                \n","        text, text_length = batch.text\n","        predictions = model(text, text_length).squeeze(1)\n","        \n","        \n","        loss = criterion(predictions, batch.sentiment.long())\n","        \n","        acc, rec, prec, f1, Y_true, Y_pred = calculate_performance(batch.sentiment, predictions)\n","      \n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        epoch_rec += rec.item()\n","        epoch_prec += prec.item()\n","        epoch_f1 += f1.item()\n","\n","        avg_loss = epoch_loss / len(iterator)\n","        avg_acc = epoch_acc / len(iterator)\n","        avg_rec = epoch_rec / len(iterator)\n","        avg_prec = epoch_prec / len(iterator)\n","        avg_f1 = epoch_f1 / len(iterator)\n","\n","    return avg_loss, avg_acc, avg_rec, avg_prec, avg_f1, Y_true, Y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41SqIzKvJUdF","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, criterion):   \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_rec = 0\n","    epoch_prec = 0\n","    epoch_f1 = 0\n","    \n","    Y_true = []\n","    Y_pred = []\n","    model.eval()\n","\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            text, text_length = batch.text\n","            predictions = model(text, text_length).squeeze(1)\n","            actuals = batch.sentiment\n"," \n","            loss = criterion(predictions, batch.sentiment.long())\n","            \n","            acc, rec, prec, f1, y_true, y_pred = calculate_performance(batch.sentiment, predictions)\n","            \n","            Y_true = np.concatenate([Y_true,y_true])\n","            Y_pred = np.concatenate([Y_pred,y_pred])\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            epoch_rec += rec.item()\n","            epoch_prec += prec.item()\n","            epoch_f1 += f1.item()\n","\n","            avg_loss = epoch_loss / len(iterator)\n","            avg_acc = epoch_acc / len(iterator)\n","            avg_rec = epoch_rec / len(iterator)\n","            avg_prec = epoch_prec / len(iterator)\n","            avg_f1 = epoch_f1 / len(iterator)\n","    return avg_loss, avg_acc, avg_rec, avg_prec, avg_f1, Y_true, Y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0JpAUV-xYY2","colab_type":"code","colab":{}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xORR7LVxiro","colab_type":"code","colab":{}},"source":["import requests, zipfile, io\n","from pathlib import Path\n","from torchtext.vocab import Vectors\n","import torch.optim as optim\n","import dill\n","\n","def build_torch_dataset(csv_filepath, fields, vocab_params, is_training_data=True):\n","\tbatch_size = vocab_params['batch_size']\n","\tmax_vocab_size = vocab_params['max_vocab_size']\n","\tpretrained_embedding_filepath = vocab_params['pretrained_embedding']['filepath']\n","\tpretrained_embedding_url = vocab_params['pretrained_embedding']['url']\n","\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"," \n","\tdata = csv_to_tabular_dataset(\n","\t  csv_filepath,\n","\t  fields = fields,\n","\t)\n","\tif not is_training_data:\n","\t\t# split into batches\n","\t\tempty, empty, test_iterator = BucketIterator.splits(\n","\t\t\t\t(None, None, data), \n","\t\t\t\tbatch_sizes = (None, None, batch_size),\n","\t\t\t\tsort_key=lambda x: len(x.text),\n","\t\t\t\tsort_within_batch = True,\n","\t\t\t\tdevice = device)\n","\t\treturn test_iterator\n","\telse:\t\t\n","\t\t# split train data as traid:validation\n","\t\ttrain_data, valid_data = data.split(split_ratio=0.8)\n","\n","\t\t# split into batches\n","\t\ttrain_iterator, valid_iterator = BucketIterator.splits(\n","\t\t\t\t(train_data, valid_data), \n","\t\t\t\tbatch_sizes = (batch_size, batch_size),\n","\t\t\t\tsort_key=lambda x: len(x.text),\n","\t\t\t\tsort_within_batch = True,\n","\t\t\t\tdevice = device)\n","\n","\t\t# build vocabulary\n","\t\t# check if we need to download vector file\n","\t\tif not Path(pretrained_embedding_filepath).is_file():\n","\t\t\t# download vector file (in .zip)\n","\t\t\tr = requests.get(pretrained_embedding_url)\n","\t\t\tz = zipfile.ZipFile(io.BytesIO(r.content))\n","\t\t\t# unzip the file\n","\t\t\tz.extractall()\n","\n","\t\tROW_NUM.build_vocab(train_data)\n","\t\tSENTIMENT.build_vocab(train_data)\n","\t\tTEXT.build_vocab(train_data,\n","\t\t\t\t\t\t\t\t\t\tmax_size = max_vocab_size,\n","\t\t\t\t\t\t\t\t\t\tvectors = Vectors(pretrained_embedding_filepath),\n","\t\t\t\t\t\t\t\t\t\tunk_init = torch.Tensor.normal_\n","\t\t\t\t\t\t\t\t\t\t)\n","\n","\t\treturn ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator\n","\n","\n","def tune_hyperparams(config, ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator, model_filepath, best_valid_loss):\n","\tUNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\tPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","\t# build the RNN object\n","\tmodel = RNN(\n","\t\tlen(TEXT.vocab),\n","\t\tconfig['embedding_dim'],\n","\t\tconfig['hidden_dim'],\n","\t\tlen(SENTIMENT.vocab),\n","\t\tconfig['n_layers'],\n","\t\tconfig['is_bidirectional'],\n","\t\tconfig['dropout'],\n","\t\tPAD_IDX,\n","\t)\n","\n","\t# print the model and its number of params\n","\tprint(model.parameters)\n","\tprint(count_model_params(model))\n","\tprint(model)\n","\tfor p in model.parameters():\n","\t  print(p.numel())\n","\n","\t# replace the initial weights of the embedding layer with the pretrained embeddings\n","\tpretrained_embeddings = TEXT.vocab.vectors\n","\tmodel.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","  # initialise UNK and PAD tokens to zeros\n","\tmodel.embedding.weight.data[UNK_IDX] = torch.zeros(config['embedding_dim'])\n","\tmodel.embedding.weight.data[PAD_IDX] = torch.zeros(config['embedding_dim'])\n","\n","\toptimiser = optim.Adam(model.parameters())\n","\tcriterion = nn.CrossEntropyLoss()\n","\n","\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\tmodel = model.to(device)\n","\tcriterion = criterion.to(device)\n","\n","\t# --- EARLY STOPPING\n","\tuse_early_stopping = True\n","\tearly_stopping_patience = 2\n","\n","\tfor epoch in range(config['n_epochs']):\n","\t\tstart_time = time.time()\n","\n","\t\ttrain_loss, train_acc, train_rec, train_prec, train_f1, train_Y_true, train_Y_pred = train(model, train_iterator, optimiser, criterion)\n","\t\tvalid_loss, valid_acc, valid_rec, valid_prec, valid_f1, valid_Y_true, valid_Y_pred = evaluate(model, valid_iterator, criterion)\n","\n","\t\tend_time = time.time()\n","\n","\t\tepoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","\t\tif valid_loss < best_valid_loss:\n","\t\t\tbest_valid_loss = valid_loss\n","\t\t\t# save the model as a file\n","\t\t\tcheckpoint = {\n","\t\t\t\t\t'model' : model,\n","\t\t\t\t\t'state_dict' : model.state_dict(),\n","\t\t\t\t\t'optimiser' : optimiser.state_dict(),\n","\t\t\t}\n","\t\t\ttorch.save(checkpoint, model_filepath)\n","\t \t\t# save the Field(s)\n","\t\t\twith open(cfg['paths']['fields']['row_num'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(ROW_NUM,f)\n","\t\t\twith open(cfg['paths']['fields']['text'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(TEXT,f)\n","\t\t\twith open(cfg['paths']['fields']['sentiment'], \"wb\")as f:\n","\t\t\t\t\tdill.dump(SENTIMENT,f)\n","\t\telse:\n","\t\t\tearly_stopping_patience -= 1   \n","\t\tprint(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","\t\tprint(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n","\t\tprint(f'\\tTrain Acc : {train_acc*100:.2f}% | Val. Acc : {valid_acc*100:.2f}%')\n","\t\tprint(f'\\tTrain Rec : {train_rec*100:.2f}% | Val. Rec : {valid_rec*100:.2f}%')\n","\t\tprint(f'\\tTrain Prec: {train_prec*100:.2f}% | Val. Prec: {valid_prec*100:.2f}%')\n","\t\tprint(f'\\tTrain F1  : {train_f1*100:.2f}% | Val. F1  : {valid_f1*100:.2f}%')\n","\n","\t\t# check if early stopping is needed\n","\t\tif use_early_stopping:\n","\t\t\tif early_stopping_patience < 0:\n","\t\t\t\tprint('Early stopping!' )\n","\t\t\t\tbreak\n","\n","\t# after completing all epochs, visualise the word vectors\n","\tvectors = model.embedding.weight.data\n","\tlabels = [l for l in TEXT.vocab.itos]\n","\t\n","\tprint('best valid loss: ', best_valid_loss)\n","\treturn best_valid_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGU30dGY3t1Z","colab_type":"code","outputId":"23828ece-bcdb-43f0-d321-106a514839c6","executionInfo":{"status":"ok","timestamp":1588587547334,"user_tz":-60,"elapsed":4867526,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["params = {\n","    'vocab': {\n","        'batch_size' : [32, 64],\n","\t      'max_vocab_size' : [50000, 100000],\n","  \t    'pretrained_embedding': [{\n","            'url' : 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip',\n","            'filepath': './crawl-300d-2M.vec',\n","            'embedding_dim': 300,\n","        },{\n","            'url' : 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n","            'filepath': './glove.twitter.27B.200d.txt',\n","            'embedding_dim' : 200,\n","        },],\n","        'preprocessing' : [{\n","            'normalize' : ['url', 'email', 'percent', 'money', 'phone', 'user',\n","                'time', 'date', 'number'],\n","            'annotate' : ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'],\n","            'spell_correct_elong' : True, #previously False\n","            'to_lowercase': True,    \n","        },\n","        ]\n","    },\n","    'nn': { \n","          'hidden_dim': [128, 256],  #previously only 256\n","          'n_layers': [2, 3],\n","          'is_bidirectional': [True],\n","          'dropout': [0.5, 0.8],\n","          'n_epochs': [20],\n","    }\n","}\n","\n","def generate_text_processor(preprocessing_params):\n","  text_processor = TextPreProcessor(\n","    # normalized these terms (ex: \"google.com\" into \"<url>\")\n","    normalize = preprocessing_params['normalize'],\n","\n","    # annotate these terms (ex: \"#win\" into [\"<hashtag>\", \"win\", \"</hashtag>\"])\n","    annotate = preprocessing_params['annotate'],\n","    fix_html=True,\n","    segmenter = 'twitter',\n","    corrector='twitter',\n","    unpack_hashtags=True,\n","    unpack_contractions=True,\n","    spell_correct_elong=preprocessing_params['spell_correct_elong'],\n","    tokenizer = SocialTokenizer(lowercase=preprocessing_params['to_lowercase']).tokenize,\n","    dicts = [emoticons]\n","  )\n","  return text_processor\n","\n","def custom_tokenizer(example):\n","  return text_processor.pre_process_doc(example)\n","\n","from itertools import product\n","def get_cartesian_product(d):\n","  # get all possible combination of a dictionary containing lists\n","  return [dict(zip(d, v)) for v in product(*d.values())]\n","\n","\n","\n","# RUN EXPERIMENTS / MAIN\n","vocab_params_trials = get_cartesian_product(params['vocab'])\n","nn_params_trials = get_cartesian_product(params['nn'])\n","best_valid_loss = float('inf')\n","\n","for vocab_params in vocab_params_trials:\n","  # build torch dataset and build the vocab.\n","  # this should be done only once during hyperparam searching\n","  print('-'*25  )\n","  print(vocab_params)\n","\n","  # text_processor will be used inside the custom_tokenizer\n","  text_processor = generate_text_processor(vocab_params['preprocessing'])\n","  TEXT = Field(tokenize=custom_tokenizer,\n","              include_lengths=True)\n","  SENTIMENT = LabelField(dtype = torch.int)\n","  ROW_NUM = Field()\n"," \n","\n","  fields = [('row_num', ROW_NUM), ('text', TEXT), ('sentiment', SENTIMENT)]\n","  ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator = build_torch_dataset(train_csv_filepath, fields, vocab_params, True)\n","\n","  for nn_params in nn_params_trials:\n","    # execute the training process\n","      nn_params['embedding_dim'] = vocab_params['pretrained_embedding']['embedding_dim']\n","      print('#'*25)\n","      print(nn_params)\n","  \n","      best_valid_loss = tune_hyperparams(nn_params, ROW_NUM, TEXT, SENTIMENT, train_iterator, valid_iterator, saved_model_filepath, best_valid_loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-------------------------\n","{'batch_size': 32, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Word statistics files not found!\n","Downloading... done!\n","Unpacking... done!\n","Reading twitter - 1grams ...\n","generating cache file for faster loading...\n","reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n","Reading twitter - 2grams ...\n","generating cache file for faster loading...\n","reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n","Reading twitter - 1grams ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/1999995 [00:00<?, ?it/s]Skipping token b'1999995' with 1-dimensional vector [b'300']; likely a header\n","100%|█████████▉| 1999377/1999995 [03:25<00:00, 11685.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.786 | Val. Loss: 0.727\n","\tTrain Acc : 63.30% | Val. Acc : 67.44%\n","\tTrain Rec : 57.77% | Val. Rec : 60.54%\n","\tTrain Prec: 59.75% | Val. Prec: 68.94%\n","\tTrain F1  : 56.20% | Val. F1  : 60.87%\n","Epoch: 02 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.646 | Val. Loss: 0.675\n","\tTrain Acc : 71.67% | Val. Acc : 69.79%\n","\tTrain Rec : 68.43% | Val. Rec : 66.04%\n","\tTrain Prec: 69.85% | Val. Prec: 67.57%\n","\tTrain F1  : 67.50% | Val. F1  : 65.46%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.553 | Val. Loss: 0.703\n","\tTrain Acc : 76.39% | Val. Acc : 68.75%\n","\tTrain Rec : 74.43% | Val. Rec : 63.45%\n","\tTrain Prec: 75.57% | Val. Prec: 67.36%\n","\tTrain F1  : 73.42% | Val. F1  : 63.55%\n","Epoch: 04 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.463 | Val. Loss: 0.800\n","\tTrain Acc : 80.91% | Val. Acc : 67.88%\n","\tTrain Rec : 79.18% | Val. Rec : 65.29%\n","\tTrain Prec: 80.06% | Val. Prec: 65.61%\n","\tTrain F1  : 78.41% | Val. F1  : 64.22%\n","Epoch: 05 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.384 | Val. Loss: 0.846\n","\tTrain Acc : 84.48% | Val. Acc : 66.73%\n","\tTrain Rec : 83.43% | Val. Rec : 63.94%\n","\tTrain Prec: 83.82% | Val. Prec: 64.38%\n","\tTrain F1  : 82.52% | Val. F1  : 62.79%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.876 | Val. Loss: 0.753\n","\tTrain Acc : 57.57% | Val. Acc : 66.68%\n","\tTrain Rec : 49.85% | Val. Rec : 61.95%\n","\tTrain Prec: 51.79% | Val. Prec: 64.73%\n","\tTrain F1  : 47.89% | Val. F1  : 61.45%\n","Epoch: 02 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.757 | Val. Loss: 0.707\n","\tTrain Acc : 65.82% | Val. Acc : 69.18%\n","\tTrain Rec : 61.13% | Val. Rec : 63.82%\n","\tTrain Prec: 63.75% | Val. Prec: 67.33%\n","\tTrain F1  : 60.30% | Val. F1  : 63.85%\n","Epoch: 03 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.706 | Val. Loss: 0.689\n","\tTrain Acc : 68.50% | Val. Acc : 69.26%\n","\tTrain Rec : 64.46% | Val. Rec : 63.25%\n","\tTrain Prec: 67.22% | Val. Prec: 67.81%\n","\tTrain F1  : 63.91% | Val. F1  : 63.39%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.792 | Val. Loss: 0.705\n","\tTrain Acc : 63.10% | Val. Acc : 68.77%\n","\tTrain Rec : 57.51% | Val. Rec : 65.52%\n","\tTrain Prec: 59.89% | Val. Prec: 66.48%\n","\tTrain F1  : 55.98% | Val. F1  : 64.62%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.651 | Val. Loss: 0.692\n","\tTrain Acc : 71.29% | Val. Acc : 69.87%\n","\tTrain Rec : 68.32% | Val. Rec : 65.52%\n","\tTrain Prec: 70.14% | Val. Prec: 67.95%\n","\tTrain F1  : 67.34% | Val. F1  : 65.22%\n","Epoch: 03 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.563 | Val. Loss: 0.728\n","\tTrain Acc : 76.18% | Val. Acc : 68.62%\n","\tTrain Rec : 74.09% | Val. Rec : 63.02%\n","\tTrain Prec: 75.27% | Val. Prec: 68.01%\n","\tTrain F1  : 73.06% | Val. F1  : 63.33%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.877 | Val. Loss: 0.745\n","\tTrain Acc : 57.52% | Val. Acc : 66.14%\n","\tTrain Rec : 50.55% | Val. Rec : 58.65%\n","\tTrain Prec: 52.55% | Val. Prec: 65.90%\n","\tTrain F1  : 48.72% | Val. F1  : 58.75%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.764 | Val. Loss: 0.699\n","\tTrain Acc : 65.65% | Val. Acc : 68.55%\n","\tTrain Rec : 60.75% | Val. Rec : 65.72%\n","\tTrain Prec: 63.85% | Val. Prec: 66.39%\n","\tTrain F1  : 60.00% | Val. F1  : 64.62%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.712 | Val. Loss: 0.695\n","\tTrain Acc : 68.59% | Val. Acc : 69.09%\n","\tTrain Rec : 64.54% | Val. Rec : 66.52%\n","\tTrain Prec: 66.99% | Val. Prec: 66.63%\n","\tTrain F1  : 63.61% | Val. F1  : 65.26%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.788 | Val. Loss: 0.701\n","\tTrain Acc : 63.27% | Val. Acc : 67.91%\n","\tTrain Rec : 57.97% | Val. Rec : 66.07%\n","\tTrain Prec: 59.94% | Val. Prec: 66.33%\n","\tTrain F1  : 56.32% | Val. F1  : 64.49%\n","Epoch: 02 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.650 | Val. Loss: 0.691\n","\tTrain Acc : 71.36% | Val. Acc : 69.25%\n","\tTrain Rec : 68.60% | Val. Rec : 65.91%\n","\tTrain Prec: 70.26% | Val. Prec: 67.88%\n","\tTrain F1  : 67.44% | Val. F1  : 65.28%\n","Epoch: 03 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.557 | Val. Loss: 0.710\n","\tTrain Acc : 76.27% | Val. Acc : 69.01%\n","\tTrain Rec : 74.36% | Val. Rec : 65.41%\n","\tTrain Prec: 75.55% | Val. Prec: 67.17%\n","\tTrain F1  : 73.40% | Val. F1  : 64.75%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.871 | Val. Loss: 0.758\n","\tTrain Acc : 58.74% | Val. Acc : 65.64%\n","\tTrain Rec : 51.51% | Val. Rec : 57.72%\n","\tTrain Prec: 54.21% | Val. Prec: 65.16%\n","\tTrain F1  : 49.79% | Val. F1  : 57.70%\n","Epoch: 02 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.760 | Val. Loss: 0.710\n","\tTrain Acc : 65.44% | Val. Acc : 67.96%\n","\tTrain Rec : 60.54% | Val. Rec : 64.75%\n","\tTrain Prec: 63.34% | Val. Prec: 66.16%\n","\tTrain F1  : 59.60% | Val. F1  : 63.75%\n","Epoch: 03 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.707 | Val. Loss: 0.689\n","\tTrain Acc : 68.73% | Val. Acc : 69.60%\n","\tTrain Rec : 64.80% | Val. Rec : 64.28%\n","\tTrain Prec: 67.55% | Val. Prec: 68.44%\n","\tTrain F1  : 63.96% | Val. F1  : 64.22%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.791 | Val. Loss: 0.710\n","\tTrain Acc : 63.36% | Val. Acc : 67.99%\n","\tTrain Rec : 58.18% | Val. Rec : 63.60%\n","\tTrain Prec: 59.91% | Val. Prec: 67.13%\n","\tTrain F1  : 56.36% | Val. F1  : 63.24%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.662 | Val. Loss: 0.699\n","\tTrain Acc : 70.65% | Val. Acc : 69.14%\n","\tTrain Rec : 67.52% | Val. Rec : 65.93%\n","\tTrain Prec: 69.55% | Val. Prec: 67.21%\n","\tTrain F1  : 66.43% | Val. F1  : 65.11%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.571 | Val. Loss: 0.701\n","\tTrain Acc : 75.53% | Val. Acc : 68.82%\n","\tTrain Rec : 72.93% | Val. Rec : 64.83%\n","\tTrain Prec: 74.48% | Val. Prec: 67.45%\n","\tTrain F1  : 72.13% | Val. F1  : 64.50%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.888 | Val. Loss: 0.753\n","\tTrain Acc : 57.16% | Val. Acc : 66.23%\n","\tTrain Rec : 49.59% | Val. Rec : 58.21%\n","\tTrain Prec: 51.82% | Val. Prec: 65.48%\n","\tTrain F1  : 47.51% | Val. F1  : 58.38%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.769 | Val. Loss: 0.701\n","\tTrain Acc : 65.21% | Val. Acc : 68.86%\n","\tTrain Rec : 60.07% | Val. Rec : 63.20%\n","\tTrain Prec: 63.00% | Val. Prec: 66.98%\n","\tTrain F1  : 59.03% | Val. F1  : 63.10%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.713 | Val. Loss: 0.688\n","\tTrain Acc : 67.98% | Val. Acc : 69.07%\n","\tTrain Rec : 63.75% | Val. Rec : 63.54%\n","\tTrain Prec: 66.45% | Val. Prec: 68.82%\n","\tTrain F1  : 62.95% | Val. F1  : 63.72%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 0/1193514 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 1445/1193514 [00:00<01:22, 14444.16it/s]\u001b[A\n","  0%|          | 2881/1193514 [00:00<01:22, 14412.07it/s]\u001b[A\n","  0%|          | 4293/1193514 [00:00<01:23, 14322.86it/s]\u001b[A\n","  0%|          | 5775/1193514 [00:00<01:22, 14466.58it/s]\u001b[A\n","  1%|          | 7242/1193514 [00:00<01:21, 14526.21it/s]\u001b[A\n","  1%|          | 8648/1193514 [00:00<01:22, 14380.68it/s]\u001b[A\n","  1%|          | 10107/1193514 [00:00<01:21, 14440.24it/s]\u001b[A\n","  1%|          | 11524/1193514 [00:00<01:22, 14356.33it/s]\u001b[A\n","  1%|          | 13016/1193514 [00:00<01:21, 14518.47it/s]\u001b[A\n","  1%|          | 14490/1193514 [00:01<01:20, 14583.30it/s]\u001b[A\n","  1%|▏         | 15957/1193514 [00:01<01:20, 14607.07it/s]\u001b[A\n","  1%|▏         | 17390/1193514 [00:01<01:22, 14291.21it/s]\u001b[A\n","  2%|▏         | 18801/1193514 [00:01<01:23, 14004.24it/s]\u001b[A\n","  2%|▏         | 20247/1193514 [00:01<01:22, 14136.59it/s]\u001b[A\n","  2%|▏         | 21731/1193514 [00:01<01:21, 14338.19it/s]\u001b[A\n","  2%|▏         | 23184/1193514 [00:01<01:21, 14392.75it/s]\u001b[A\n","  2%|▏         | 24632/1193514 [00:01<01:21, 14416.95it/s]\u001b[A\n","  2%|▏         | 26084/1193514 [00:01<01:20, 14447.53it/s]\u001b[A\n","  2%|▏         | 27539/1193514 [00:01<01:20, 14475.45it/s]\u001b[A\n","  2%|▏         | 29021/1193514 [00:02<01:19, 14574.51it/s]\u001b[A\n","  3%|▎         | 30478/1193514 [00:02<01:19, 14569.68it/s]\u001b[A\n","  3%|▎         | 31950/1193514 [00:02<01:19, 14613.62it/s]\u001b[A\n","  3%|▎         | 33412/1193514 [00:02<01:20, 14328.13it/s]\u001b[A\n","  3%|▎         | 34846/1193514 [00:02<01:21, 14182.88it/s]\u001b[A\n","  3%|▎         | 36322/1193514 [00:02<01:20, 14348.73it/s]\u001b[A\n","  3%|▎         | 37780/1193514 [00:02<01:20, 14415.35it/s]\u001b[A\n","  3%|▎         | 39236/1193514 [00:02<01:19, 14457.87it/s]\u001b[A\n","  3%|▎         | 40683/1193514 [00:02<01:20, 14383.56it/s]\u001b[A\n","  4%|▎         | 42133/1193514 [00:02<01:19, 14415.82it/s]\u001b[A\n","  4%|▎         | 43593/1193514 [00:03<01:19, 14469.36it/s]\u001b[A\n","  4%|▍         | 45067/1193514 [00:03<01:18, 14548.37it/s]\u001b[A\n","  4%|▍         | 46562/1193514 [00:03<01:18, 14663.81it/s]\u001b[A\n","  4%|▍         | 48029/1193514 [00:03<01:24, 13609.89it/s]\u001b[A\n","  4%|▍         | 49424/1193514 [00:03<01:23, 13707.99it/s]\u001b[A\n","  4%|▍         | 50906/1193514 [00:03<01:21, 14023.24it/s]\u001b[A\n","  4%|▍         | 52318/1193514 [00:03<01:21, 13997.06it/s]\u001b[A\n","  5%|▍         | 53813/1193514 [00:03<01:19, 14265.69it/s]\u001b[A\n","  5%|▍         | 55246/1193514 [00:03<01:19, 14265.36it/s]\u001b[A\n","  5%|▍         | 56693/1193514 [00:03<01:19, 14324.29it/s]\u001b[A\n","  5%|▍         | 58130/1193514 [00:04<01:19, 14336.36it/s]\u001b[A\n","  5%|▍         | 59566/1193514 [00:04<01:19, 14309.73it/s]\u001b[A\n","  5%|▌         | 60999/1193514 [00:04<01:19, 14205.59it/s]\u001b[A\n","  5%|▌         | 62421/1193514 [00:04<01:21, 13797.39it/s]\u001b[A\n","  5%|▌         | 63819/1193514 [00:04<01:21, 13849.59it/s]\u001b[A\n","  5%|▌         | 65245/1193514 [00:04<01:20, 13969.84it/s]\u001b[A\n","  6%|▌         | 66713/1193514 [00:04<01:19, 14174.10it/s]\u001b[A\n","  6%|▌         | 68224/1193514 [00:04<01:17, 14441.55it/s]\u001b[A\n","  6%|▌         | 69671/1193514 [00:04<01:18, 14356.52it/s]\u001b[A\n","  6%|▌         | 71159/1193514 [00:04<01:17, 14508.09it/s]\u001b[A\n","  6%|▌         | 72612/1193514 [00:05<01:17, 14465.98it/s]\u001b[A\n","  6%|▌         | 74078/1193514 [00:05<01:17, 14518.71it/s]\u001b[A\n","  6%|▋         | 75556/1193514 [00:05<01:16, 14595.20it/s]\u001b[A\n","  6%|▋         | 77017/1193514 [00:05<01:16, 14564.52it/s]\u001b[A\n","  7%|▋         | 78474/1193514 [00:05<01:16, 14527.78it/s]\u001b[A\n","  7%|▋         | 79928/1193514 [00:05<01:16, 14466.59it/s]\u001b[A\n","  7%|▋         | 81375/1193514 [00:05<01:17, 14308.40it/s]\u001b[A\n","  7%|▋         | 82826/1193514 [00:05<01:17, 14367.44it/s]\u001b[A\n","  7%|▋         | 84270/1193514 [00:05<01:17, 14388.37it/s]\u001b[A\n","  7%|▋         | 85711/1193514 [00:05<01:16, 14393.28it/s]\u001b[A\n","  7%|▋         | 87151/1193514 [00:06<01:16, 14377.85it/s]\u001b[A\n","  7%|▋         | 88589/1193514 [00:06<01:17, 14344.79it/s]\u001b[A\n","  8%|▊         | 90047/1193514 [00:06<01:16, 14413.85it/s]\u001b[A\n","  8%|▊         | 91489/1193514 [00:06<01:17, 14302.90it/s]\u001b[A\n","  8%|▊         | 92920/1193514 [00:06<01:17, 14276.38it/s]\u001b[A\n","  8%|▊         | 94362/1193514 [00:06<01:16, 14318.24it/s]\u001b[A\n","  8%|▊         | 95795/1193514 [00:06<01:17, 14127.16it/s]\u001b[A\n","  8%|▊         | 97260/1193514 [00:06<01:16, 14279.29it/s]\u001b[A\n","  8%|▊         | 98722/1193514 [00:06<01:16, 14377.79it/s]\u001b[A\n","  8%|▊         | 100161/1193514 [00:06<01:16, 14375.22it/s]\u001b[A\n","  9%|▊         | 101600/1193514 [00:07<01:16, 14338.90it/s]\u001b[A\n","  9%|▊         | 103044/1193514 [00:07<01:15, 14366.49it/s]\u001b[A\n","  9%|▉         | 104524/1193514 [00:07<01:15, 14492.81it/s]\u001b[A\n","  9%|▉         | 105974/1193514 [00:07<01:15, 14485.83it/s]\u001b[A\n","  9%|▉         | 107423/1193514 [00:07<01:16, 14241.80it/s]\u001b[A\n","  9%|▉         | 108962/1193514 [00:07<01:14, 14566.15it/s]\u001b[A\n","  9%|▉         | 110430/1193514 [00:07<01:14, 14599.16it/s]\u001b[A\n","  9%|▉         | 111978/1193514 [00:07<01:12, 14851.60it/s]\u001b[A\n"," 10%|▉         | 113541/1193514 [00:07<01:11, 15074.97it/s]\u001b[A\n"," 10%|▉         | 115060/1193514 [00:07<01:11, 15108.77it/s]\u001b[A\n"," 10%|▉         | 116582/1193514 [00:08<01:11, 15141.60it/s]\u001b[A\n"," 10%|▉         | 118124/1193514 [00:08<01:10, 15221.93it/s]\u001b[A\n"," 10%|█         | 119649/1193514 [00:08<01:10, 15228.29it/s]\u001b[A\n"," 10%|█         | 121173/1193514 [00:08<01:11, 14926.32it/s]\u001b[A\n"," 10%|█         | 122668/1193514 [00:08<01:11, 14928.95it/s]\u001b[A\n"," 10%|█         | 124188/1193514 [00:08<01:11, 15007.29it/s]\u001b[A\n"," 11%|█         | 125690/1193514 [00:08<01:12, 14720.16it/s]\u001b[A\n"," 11%|█         | 127194/1193514 [00:08<01:11, 14811.55it/s]\u001b[A\n"," 11%|█         | 128693/1193514 [00:08<01:11, 14862.10it/s]\u001b[A\n"," 11%|█         | 130181/1193514 [00:09<01:11, 14859.57it/s]\u001b[A\n"," 11%|█         | 131723/1193514 [00:09<01:10, 15022.50it/s]\u001b[A\n"," 11%|█         | 133234/1193514 [00:09<01:10, 15046.87it/s]\u001b[A\n"," 11%|█▏        | 134770/1193514 [00:09<01:09, 15137.42it/s]\u001b[A\n"," 11%|█▏        | 136285/1193514 [00:09<01:10, 14996.76it/s]\u001b[A\n"," 12%|█▏        | 137786/1193514 [00:09<01:10, 14993.78it/s]\u001b[A\n"," 12%|█▏        | 139329/1193514 [00:09<01:09, 15119.92it/s]\u001b[A\n"," 12%|█▏        | 140842/1193514 [00:09<01:10, 15013.65it/s]\u001b[A\n"," 12%|█▏        | 142344/1193514 [00:09<01:10, 14949.15it/s]\u001b[A\n"," 12%|█▏        | 143868/1193514 [00:09<01:09, 15035.12it/s]\u001b[A\n"," 12%|█▏        | 145382/1193514 [00:10<01:09, 15064.90it/s]\u001b[A\n"," 12%|█▏        | 146926/1193514 [00:10<01:08, 15174.86it/s]\u001b[A\n"," 12%|█▏        | 148464/1193514 [00:10<01:08, 15234.02it/s]\u001b[A\n"," 13%|█▎        | 150011/1193514 [00:10<01:08, 15303.78it/s]\u001b[A\n"," 13%|█▎        | 151542/1193514 [00:10<01:09, 15032.49it/s]\u001b[A\n"," 13%|█▎        | 153073/1193514 [00:10<01:08, 15112.65it/s]\u001b[A\n"," 13%|█▎        | 154586/1193514 [00:10<01:09, 14961.84it/s]\u001b[A\n"," 13%|█▎        | 156084/1193514 [00:10<01:09, 14857.20it/s]\u001b[A\n"," 13%|█▎        | 157610/1193514 [00:10<01:09, 14975.54it/s]\u001b[A\n"," 13%|█▎        | 159153/1193514 [00:10<01:08, 15107.17it/s]\u001b[A\n"," 13%|█▎        | 160680/1193514 [00:11<01:08, 15153.12it/s]\u001b[A\n"," 14%|█▎        | 162196/1193514 [00:11<01:08, 15092.27it/s]\u001b[A\n"," 14%|█▎        | 163709/1193514 [00:11<01:08, 15103.19it/s]\u001b[A\n"," 14%|█▍        | 165251/1193514 [00:11<01:07, 15195.70it/s]\u001b[A\n"," 14%|█▍        | 166771/1193514 [00:11<01:08, 15028.09it/s]\u001b[A\n"," 14%|█▍        | 168275/1193514 [00:11<01:08, 15015.35it/s]\u001b[A\n"," 14%|█▍        | 169807/1193514 [00:11<01:07, 15103.39it/s]\u001b[A\n"," 14%|█▍        | 171318/1193514 [00:11<01:08, 14975.82it/s]\u001b[A\n"," 14%|█▍        | 172837/1193514 [00:11<01:07, 15038.16it/s]\u001b[A\n"," 15%|█▍        | 174358/1193514 [00:11<01:07, 15088.33it/s]\u001b[A\n"," 15%|█▍        | 175887/1193514 [00:12<01:07, 15146.10it/s]\u001b[A\n"," 15%|█▍        | 177437/1193514 [00:12<01:06, 15250.26it/s]\u001b[A\n"," 15%|█▍        | 178963/1193514 [00:12<01:07, 15043.85it/s]\u001b[A\n"," 15%|█▌        | 180536/1193514 [00:12<01:06, 15241.45it/s]\u001b[A\n"," 15%|█▌        | 182062/1193514 [00:12<01:07, 15020.46it/s]\u001b[A\n"," 15%|█▌        | 183569/1193514 [00:12<01:07, 15031.72it/s]\u001b[A\n"," 16%|█▌        | 185095/1193514 [00:12<01:06, 15099.22it/s]\u001b[A\n"," 16%|█▌        | 186606/1193514 [00:12<01:06, 15099.63it/s]\u001b[A\n"," 16%|█▌        | 188162/1193514 [00:12<01:06, 15232.28it/s]\u001b[A\n"," 16%|█▌        | 189731/1193514 [00:12<01:05, 15364.08it/s]\u001b[A\n"," 16%|█▌        | 191269/1193514 [00:13<01:05, 15307.21it/s]\u001b[A\n"," 16%|█▌        | 192801/1193514 [00:13<01:05, 15232.95it/s]\u001b[A\n"," 16%|█▋        | 194353/1193514 [00:13<01:05, 15316.12it/s]\u001b[A\n"," 16%|█▋        | 195898/1193514 [00:13<01:04, 15353.95it/s]\u001b[A\n"," 17%|█▋        | 197434/1193514 [00:13<01:05, 15115.99it/s]\u001b[A\n"," 17%|█▋        | 198949/1193514 [00:13<01:05, 15125.34it/s]\u001b[A\n"," 17%|█▋        | 200463/1193514 [00:13<01:06, 14844.08it/s]\u001b[A\n"," 17%|█▋        | 201950/1193514 [00:13<01:07, 14740.07it/s]\u001b[A\n"," 17%|█▋        | 203426/1193514 [00:13<01:07, 14732.94it/s]\u001b[A\n"," 17%|█▋        | 204951/1193514 [00:13<01:06, 14883.91it/s]\u001b[A\n"," 17%|█▋        | 206441/1193514 [00:14<01:06, 14878.34it/s]\u001b[A\n"," 17%|█▋        | 207952/1193514 [00:14<01:05, 14944.23it/s]\u001b[A\n"," 18%|█▊        | 209447/1193514 [00:14<01:07, 14675.01it/s]\u001b[A\n"," 18%|█▊        | 210956/1193514 [00:14<01:06, 14795.43it/s]\u001b[A\n"," 18%|█▊        | 212467/1193514 [00:14<01:05, 14888.11it/s]\u001b[A\n"," 18%|█▊        | 213985/1193514 [00:14<01:05, 14973.77it/s]\u001b[A\n"," 18%|█▊        | 215538/1193514 [00:14<01:04, 15133.52it/s]\u001b[A\n"," 18%|█▊        | 217053/1193514 [00:14<01:05, 14812.36it/s]\u001b[A\n"," 18%|█▊        | 218578/1193514 [00:14<01:05, 14940.08it/s]\u001b[A\n"," 18%|█▊        | 220125/1193514 [00:14<01:04, 15094.23it/s]\u001b[A\n"," 19%|█▊        | 221637/1193514 [00:15<01:04, 15078.25it/s]\u001b[A\n"," 19%|█▊        | 223183/1193514 [00:15<01:03, 15189.93it/s]\u001b[A\n"," 19%|█▉        | 224703/1193514 [00:15<01:04, 15119.31it/s]\u001b[A\n"," 19%|█▉        | 226216/1193514 [00:15<01:04, 15036.33it/s]\u001b[A\n"," 19%|█▉        | 227721/1193514 [00:15<01:04, 15008.54it/s]\u001b[A\n"," 19%|█▉        | 229254/1193514 [00:15<01:03, 15101.68it/s]\u001b[A\n"," 19%|█▉        | 230789/1193514 [00:15<01:03, 15174.46it/s]\u001b[A\n"," 19%|█▉        | 232307/1193514 [00:15<01:04, 14921.51it/s]\u001b[A\n"," 20%|█▉        | 233801/1193514 [00:15<01:04, 14907.14it/s]\u001b[A\n"," 20%|█▉        | 235293/1193514 [00:15<01:04, 14907.21it/s]\u001b[A\n"," 20%|█▉        | 236788/1193514 [00:16<01:04, 14919.54it/s]\u001b[A\n"," 20%|█▉        | 238304/1193514 [00:16<01:03, 14988.68it/s]\u001b[A\n"," 20%|██        | 239804/1193514 [00:16<01:04, 14832.97it/s]\u001b[A\n"," 20%|██        | 241288/1193514 [00:16<01:04, 14749.47it/s]\u001b[A\n"," 20%|██        | 242764/1193514 [00:16<01:06, 14211.95it/s]\u001b[A\n"," 20%|██        | 244242/1193514 [00:16<01:06, 14377.28it/s]\u001b[A\n"," 21%|██        | 245704/1193514 [00:16<01:05, 14448.36it/s]\u001b[A\n"," 21%|██        | 247152/1193514 [00:16<01:05, 14400.82it/s]\u001b[A\n"," 21%|██        | 248674/1193514 [00:16<01:04, 14633.16it/s]\u001b[A\n"," 21%|██        | 250172/1193514 [00:17<01:04, 14733.29it/s]\u001b[A\n"," 21%|██        | 251679/1193514 [00:17<01:03, 14832.26it/s]\u001b[A\n"," 21%|██        | 253188/1193514 [00:17<01:03, 14907.50it/s]\u001b[A\n"," 21%|██▏       | 254700/1193514 [00:17<01:02, 14969.01it/s]\u001b[A\n"," 21%|██▏       | 256198/1193514 [00:17<01:02, 14888.71it/s]\u001b[A\n"," 22%|██▏       | 257688/1193514 [00:17<01:02, 14855.01it/s]\u001b[A\n"," 22%|██▏       | 259174/1193514 [00:17<01:03, 14796.52it/s]\u001b[A\n"," 22%|██▏       | 260675/1193514 [00:17<01:02, 14858.56it/s]\u001b[A\n"," 22%|██▏       | 262162/1193514 [00:17<01:02, 14854.02it/s]\u001b[A\n"," 22%|██▏       | 263648/1193514 [00:17<01:02, 14843.67it/s]\u001b[A\n"," 22%|██▏       | 265133/1193514 [00:18<01:02, 14814.93it/s]\u001b[A\n"," 22%|██▏       | 266615/1193514 [00:18<01:02, 14810.60it/s]\u001b[A\n"," 22%|██▏       | 268127/1193514 [00:18<01:02, 14901.86it/s]\u001b[A\n"," 23%|██▎       | 269618/1193514 [00:18<01:02, 14898.43it/s]\u001b[A\n"," 23%|██▎       | 271108/1193514 [00:18<01:03, 14595.30it/s]\u001b[A\n"," 23%|██▎       | 272569/1193514 [00:18<01:04, 14365.27it/s]\u001b[A\n"," 23%|██▎       | 274094/1193514 [00:18<01:02, 14617.39it/s]\u001b[A\n"," 23%|██▎       | 275589/1193514 [00:18<01:02, 14714.17it/s]\u001b[A\n"," 23%|██▎       | 277072/1193514 [00:18<01:02, 14747.18it/s]\u001b[A\n"," 23%|██▎       | 278571/1193514 [00:18<01:01, 14817.62it/s]\u001b[A\n"," 23%|██▎       | 280064/1193514 [00:19<01:01, 14850.38it/s]\u001b[A\n"," 24%|██▎       | 281550/1193514 [00:19<01:01, 14851.83it/s]\u001b[A\n"," 24%|██▎       | 283048/1193514 [00:19<01:01, 14889.94it/s]\u001b[A\n"," 24%|██▍       | 284600/1193514 [00:19<01:00, 15071.00it/s]\u001b[A\n"," 24%|██▍       | 286108/1193514 [00:19<01:01, 14835.02it/s]\u001b[A\n"," 24%|██▍       | 287593/1193514 [00:19<01:01, 14677.15it/s]\u001b[A\n"," 24%|██▍       | 289125/1193514 [00:19<01:00, 14863.77it/s]\u001b[A\n"," 24%|██▍       | 290634/1193514 [00:19<01:00, 14930.92it/s]\u001b[A\n"," 24%|██▍       | 292168/1193514 [00:19<00:59, 15049.97it/s]\u001b[A\n"," 25%|██▍       | 293675/1193514 [00:19<01:00, 14951.93it/s]\u001b[A\n"," 25%|██▍       | 295172/1193514 [00:20<01:00, 14823.15it/s]\u001b[A\n"," 25%|██▍       | 296656/1193514 [00:20<01:00, 14769.67it/s]\u001b[A\n"," 25%|██▍       | 298192/1193514 [00:20<00:59, 14935.03it/s]\u001b[A\n"," 25%|██▌       | 299718/1193514 [00:20<00:59, 15029.19it/s]\u001b[A\n"," 25%|██▌       | 301222/1193514 [00:20<00:59, 14909.92it/s]\u001b[A\n"," 25%|██▌       | 302714/1193514 [00:20<00:59, 14871.88it/s]\u001b[A\n"," 25%|██▌       | 304202/1193514 [00:20<01:00, 14798.61it/s]\u001b[A\n"," 26%|██▌       | 305683/1193514 [00:20<01:00, 14683.04it/s]\u001b[A\n"," 26%|██▌       | 307205/1193514 [00:20<00:59, 14837.49it/s]\u001b[A\n"," 26%|██▌       | 308720/1193514 [00:20<00:59, 14928.72it/s]\u001b[A\n"," 26%|██▌       | 310214/1193514 [00:21<00:59, 14776.12it/s]\u001b[A\n"," 26%|██▌       | 311693/1193514 [00:21<00:59, 14776.72it/s]\u001b[A\n"," 26%|██▌       | 313172/1193514 [00:21<00:59, 14739.70it/s]\u001b[A\n"," 26%|██▋       | 314647/1193514 [00:21<00:59, 14727.83it/s]\u001b[A\n"," 26%|██▋       | 316152/1193514 [00:21<00:59, 14822.39it/s]\u001b[A\n"," 27%|██▋       | 317635/1193514 [00:21<00:59, 14755.82it/s]\u001b[A\n"," 27%|██▋       | 319111/1193514 [00:21<00:59, 14732.89it/s]\u001b[A\n"," 27%|██▋       | 320635/1193514 [00:21<00:58, 14880.53it/s]\u001b[A\n"," 27%|██▋       | 322155/1193514 [00:21<00:58, 14972.80it/s]\u001b[A\n"," 27%|██▋       | 323659/1193514 [00:21<00:58, 14992.79it/s]\u001b[A\n"," 27%|██▋       | 325174/1193514 [00:22<00:57, 15037.81it/s]\u001b[A\n"," 27%|██▋       | 326679/1193514 [00:22<00:58, 14724.17it/s]\u001b[A\n"," 27%|██▋       | 328183/1193514 [00:22<00:58, 14815.70it/s]\u001b[A\n"," 28%|██▊       | 329714/1193514 [00:22<00:57, 14958.29it/s]\u001b[A\n"," 28%|██▊       | 331227/1193514 [00:22<00:57, 15007.04it/s]\u001b[A\n"," 28%|██▊       | 332729/1193514 [00:22<00:57, 15007.39it/s]\u001b[A\n"," 28%|██▊       | 334262/1193514 [00:22<00:56, 15100.21it/s]\u001b[A\n"," 28%|██▊       | 335778/1193514 [00:22<00:56, 15116.97it/s]\u001b[A\n"," 28%|██▊       | 337311/1193514 [00:22<00:56, 15175.71it/s]\u001b[A\n"," 28%|██▊       | 338829/1193514 [00:22<00:56, 15144.85it/s]\u001b[A\n"," 29%|██▊       | 340344/1193514 [00:23<00:56, 15050.60it/s]\u001b[A\n"," 29%|██▊       | 341850/1193514 [00:23<00:57, 14889.04it/s]\u001b[A\n"," 29%|██▉       | 343340/1193514 [00:23<00:57, 14891.33it/s]\u001b[A\n"," 29%|██▉       | 344830/1193514 [00:23<00:57, 14855.97it/s]\u001b[A\n"," 29%|██▉       | 346316/1193514 [00:23<00:57, 14778.34it/s]\u001b[A\n"," 29%|██▉       | 347795/1193514 [00:23<00:57, 14621.43it/s]\u001b[A\n"," 29%|██▉       | 349258/1193514 [00:23<00:57, 14619.81it/s]\u001b[A\n"," 29%|██▉       | 350767/1193514 [00:23<00:57, 14757.18it/s]\u001b[A\n"," 30%|██▉       | 352244/1193514 [00:23<00:57, 14645.40it/s]\u001b[A\n"," 30%|██▉       | 353728/1193514 [00:23<00:57, 14701.05it/s]\u001b[A\n"," 30%|██▉       | 355289/1193514 [00:24<00:56, 14961.02it/s]\u001b[A\n"," 30%|██▉       | 356787/1193514 [00:24<00:56, 14884.94it/s]\u001b[A\n"," 30%|███       | 358277/1193514 [00:24<00:57, 14587.18it/s]\u001b[A\n"," 30%|███       | 359738/1193514 [00:24<00:57, 14584.50it/s]\u001b[A\n"," 30%|███       | 361227/1193514 [00:24<00:56, 14673.16it/s]\u001b[A\n"," 30%|███       | 362706/1193514 [00:24<00:56, 14707.55it/s]\u001b[A\n"," 31%|███       | 364197/1193514 [00:24<00:56, 14766.90it/s]\u001b[A\n"," 31%|███       | 365694/1193514 [00:24<00:55, 14825.04it/s]\u001b[A\n"," 31%|███       | 367178/1193514 [00:24<00:55, 14820.62it/s]\u001b[A\n"," 31%|███       | 368663/1193514 [00:24<00:55, 14827.24it/s]\u001b[A\n"," 31%|███       | 370156/1193514 [00:25<00:55, 14856.14it/s]\u001b[A\n"," 31%|███       | 371642/1193514 [00:25<00:56, 14597.65it/s]\u001b[A\n"," 31%|███▏      | 373103/1193514 [00:25<00:56, 14519.67it/s]\u001b[A\n"," 31%|███▏      | 374556/1193514 [00:25<00:56, 14435.48it/s]\u001b[A\n"," 32%|███▏      | 376001/1193514 [00:25<00:56, 14433.39it/s]\u001b[A\n"," 32%|███▏      | 377450/1193514 [00:25<00:56, 14449.35it/s]\u001b[A\n"," 32%|███▏      | 378925/1193514 [00:25<00:56, 14536.82it/s]\u001b[A\n"," 32%|███▏      | 380423/1193514 [00:25<00:55, 14666.17it/s]\u001b[A\n"," 32%|███▏      | 381912/1193514 [00:25<00:55, 14730.99it/s]\u001b[A\n"," 32%|███▏      | 383386/1193514 [00:26<00:55, 14627.50it/s]\u001b[A\n"," 32%|███▏      | 384904/1193514 [00:26<00:54, 14786.77it/s]\u001b[A\n"," 32%|███▏      | 386390/1193514 [00:26<00:54, 14807.14it/s]\u001b[A\n"," 32%|███▏      | 387872/1193514 [00:26<00:54, 14676.34it/s]\u001b[A\n"," 33%|███▎      | 389341/1193514 [00:26<00:55, 14558.11it/s]\u001b[A\n"," 33%|███▎      | 390798/1193514 [00:26<00:55, 14530.58it/s]\u001b[A\n"," 33%|███▎      | 392252/1193514 [00:26<00:55, 14458.95it/s]\u001b[A\n"," 33%|███▎      | 393722/1193514 [00:26<00:55, 14528.19it/s]\u001b[A\n"," 33%|███▎      | 395189/1193514 [00:26<00:54, 14570.02it/s]\u001b[A\n"," 33%|███▎      | 396695/1193514 [00:26<00:54, 14713.35it/s]\u001b[A\n"," 33%|███▎      | 398170/1193514 [00:27<00:54, 14724.02it/s]\u001b[A\n"," 33%|███▎      | 399668/1193514 [00:27<00:53, 14798.80it/s]\u001b[A\n"," 34%|███▎      | 401173/1193514 [00:27<00:53, 14872.54it/s]\u001b[A\n"," 34%|███▎      | 402671/1193514 [00:27<00:53, 14904.10it/s]\u001b[A\n"," 34%|███▍      | 404162/1193514 [00:27<00:54, 14496.40it/s]\u001b[A\n"," 34%|███▍      | 405652/1193514 [00:27<00:53, 14613.31it/s]\u001b[A\n"," 34%|███▍      | 407130/1193514 [00:27<00:53, 14662.81it/s]\u001b[A\n"," 34%|███▍      | 408672/1193514 [00:27<00:52, 14880.94it/s]\u001b[A\n"," 34%|███▍      | 410196/1193514 [00:27<00:52, 14985.40it/s]\u001b[A\n"," 34%|███▍      | 411696/1193514 [00:27<00:52, 14886.69it/s]\u001b[A\n"," 35%|███▍      | 413186/1193514 [00:28<00:52, 14840.64it/s]\u001b[A\n"," 35%|███▍      | 414671/1193514 [00:28<00:52, 14814.00it/s]\u001b[A\n"," 35%|███▍      | 416154/1193514 [00:28<00:52, 14804.61it/s]\u001b[A\n"," 35%|███▍      | 417637/1193514 [00:28<00:52, 14810.12it/s]\u001b[A\n"," 35%|███▌      | 419119/1193514 [00:28<00:53, 14608.47it/s]\u001b[A\n"," 35%|███▌      | 420626/1193514 [00:28<00:52, 14743.37it/s]\u001b[A\n"," 35%|███▌      | 422102/1193514 [00:28<00:54, 14230.67it/s]\u001b[A\n"," 35%|███▌      | 423591/1193514 [00:28<00:53, 14421.38it/s]\u001b[A\n"," 36%|███▌      | 425037/1193514 [00:28<00:53, 14402.81it/s]\u001b[A\n"," 36%|███▌      | 426519/1193514 [00:28<00:52, 14524.17it/s]\u001b[A\n"," 36%|███▌      | 427974/1193514 [00:29<00:53, 14283.81it/s]\u001b[A\n"," 36%|███▌      | 429448/1193514 [00:29<00:52, 14416.35it/s]\u001b[A\n"," 36%|███▌      | 430929/1193514 [00:29<00:52, 14531.31it/s]\u001b[A\n"," 36%|███▌      | 432402/1193514 [00:29<00:52, 14590.12it/s]\u001b[A\n"," 36%|███▋      | 433863/1193514 [00:29<00:52, 14517.35it/s]\u001b[A\n"," 36%|███▋      | 435325/1193514 [00:29<00:52, 14547.16it/s]\u001b[A\n"," 37%|███▋      | 436781/1193514 [00:29<00:52, 14510.55it/s]\u001b[A\n"," 37%|███▋      | 438250/1193514 [00:29<00:51, 14561.40it/s]\u001b[A\n"," 37%|███▋      | 439762/1193514 [00:29<00:51, 14722.61it/s]\u001b[A\n"," 37%|███▋      | 441282/1193514 [00:29<00:50, 14860.76it/s]\u001b[A\n"," 37%|███▋      | 442769/1193514 [00:30<00:51, 14521.79it/s]\u001b[A\n"," 37%|███▋      | 444224/1193514 [00:30<00:52, 14285.37it/s]\u001b[A\n"," 37%|███▋      | 445655/1193514 [00:30<00:53, 13958.07it/s]\u001b[A\n"," 37%|███▋      | 447055/1193514 [00:30<00:53, 13948.07it/s]\u001b[A\n"," 38%|███▊      | 448453/1193514 [00:30<00:54, 13612.99it/s]\u001b[A\n"," 38%|███▊      | 449859/1193514 [00:30<00:54, 13743.92it/s]\u001b[A\n"," 38%|███▊      | 451303/1193514 [00:30<00:53, 13943.60it/s]\u001b[A\n"," 38%|███▊      | 452794/1193514 [00:30<00:52, 14219.40it/s]\u001b[A\n"," 38%|███▊      | 454282/1193514 [00:30<00:51, 14409.87it/s]\u001b[A\n"," 38%|███▊      | 455749/1193514 [00:30<00:50, 14485.34it/s]\u001b[A\n"," 38%|███▊      | 457200/1193514 [00:31<00:51, 14408.54it/s]\u001b[A\n"," 38%|███▊      | 458643/1193514 [00:31<00:52, 13872.79it/s]\u001b[A\n"," 39%|███▊      | 460115/1193514 [00:31<00:51, 14114.76it/s]\u001b[A\n"," 39%|███▊      | 461554/1193514 [00:31<00:51, 14194.03it/s]\u001b[A\n"," 39%|███▉      | 462977/1193514 [00:31<00:51, 14195.71it/s]\u001b[A\n"," 39%|███▉      | 464413/1193514 [00:31<00:51, 14244.58it/s]\u001b[A\n"," 39%|███▉      | 465906/1193514 [00:31<00:50, 14443.31it/s]\u001b[A\n"," 39%|███▉      | 467396/1193514 [00:31<00:49, 14575.41it/s]\u001b[A\n"," 39%|███▉      | 468856/1193514 [00:31<00:49, 14522.91it/s]\u001b[A\n"," 39%|███▉      | 470323/1193514 [00:31<00:49, 14566.19it/s]\u001b[A\n"," 40%|███▉      | 471781/1193514 [00:32<00:49, 14516.37it/s]\u001b[A\n"," 40%|███▉      | 473234/1193514 [00:32<00:50, 14361.34it/s]\u001b[A\n"," 40%|███▉      | 474701/1193514 [00:32<00:49, 14450.73it/s]\u001b[A\n"," 40%|███▉      | 476147/1193514 [00:32<00:50, 14311.95it/s]\u001b[A\n"," 40%|████      | 477579/1193514 [00:32<00:50, 14114.48it/s]\u001b[A\n"," 40%|████      | 479023/1193514 [00:32<00:50, 14208.42it/s]\u001b[A\n"," 40%|████      | 480530/1193514 [00:32<00:49, 14455.91it/s]\u001b[A\n"," 40%|████      | 482003/1193514 [00:32<00:48, 14534.66it/s]\u001b[A\n"," 41%|████      | 483486/1193514 [00:32<00:48, 14620.47it/s]\u001b[A\n"," 41%|████      | 484961/1193514 [00:33<00:48, 14658.58it/s]\u001b[A\n"," 41%|████      | 486428/1193514 [00:33<00:49, 14191.84it/s]\u001b[A\n"," 41%|████      | 487880/1193514 [00:33<00:49, 14288.21it/s]\u001b[A\n"," 41%|████      | 489341/1193514 [00:33<00:48, 14381.32it/s]\u001b[A\n"," 41%|████      | 490782/1193514 [00:33<00:48, 14389.43it/s]\u001b[A\n"," 41%|████      | 492234/1193514 [00:33<00:48, 14426.62it/s]\u001b[A\n"," 41%|████▏     | 493678/1193514 [00:33<00:48, 14285.73it/s]\u001b[A\n"," 41%|████▏     | 495108/1193514 [00:33<00:49, 14187.42it/s]\u001b[A\n"," 42%|████▏     | 496617/1193514 [00:33<00:48, 14445.77it/s]\u001b[A\n"," 42%|████▏     | 498107/1193514 [00:33<00:47, 14578.04it/s]\u001b[A\n"," 42%|████▏     | 499585/1193514 [00:34<00:47, 14636.81it/s]\u001b[A\n"," 42%|████▏     | 501050/1193514 [00:34<00:47, 14541.40it/s]\u001b[A\n"," 42%|████▏     | 502521/1193514 [00:34<00:47, 14590.38it/s]\u001b[A\n"," 42%|████▏     | 503992/1193514 [00:34<00:47, 14625.06it/s]\u001b[A\n"," 42%|████▏     | 505462/1193514 [00:34<00:46, 14646.52it/s]\u001b[A\n"," 42%|████▏     | 506975/1193514 [00:34<00:46, 14788.15it/s]\u001b[A\n"," 43%|████▎     | 508455/1193514 [00:34<00:47, 14447.27it/s]\u001b[A\n"," 43%|████▎     | 509973/1193514 [00:34<00:46, 14658.84it/s]\u001b[A\n"," 43%|████▎     | 511503/1193514 [00:34<00:45, 14843.96it/s]\u001b[A\n"," 43%|████▎     | 513039/1193514 [00:34<00:45, 14994.87it/s]\u001b[A\n"," 43%|████▎     | 514541/1193514 [00:35<00:45, 14979.91it/s]\u001b[A\n"," 43%|████▎     | 516041/1193514 [00:35<00:46, 14674.92it/s]\u001b[A\n"," 43%|████▎     | 517566/1193514 [00:35<00:45, 14840.66it/s]\u001b[A\n"," 43%|████▎     | 519092/1193514 [00:35<00:45, 14961.55it/s]\u001b[A\n"," 44%|████▎     | 520590/1193514 [00:35<00:45, 14633.03it/s]\u001b[A\n"," 44%|████▎     | 522079/1193514 [00:35<00:45, 14707.35it/s]\u001b[A\n"," 44%|████▍     | 523552/1193514 [00:35<00:46, 14492.30it/s]\u001b[A\n"," 44%|████▍     | 525071/1193514 [00:35<00:45, 14694.33it/s]\u001b[A\n"," 44%|████▍     | 526584/1193514 [00:35<00:44, 14822.20it/s]\u001b[A\n"," 44%|████▍     | 528081/1193514 [00:35<00:44, 14865.57it/s]\u001b[A\n"," 44%|████▍     | 529569/1193514 [00:36<00:44, 14758.53it/s]\u001b[A\n"," 44%|████▍     | 531046/1193514 [00:36<00:45, 14550.06it/s]\u001b[A\n"," 45%|████▍     | 532515/1193514 [00:36<00:45, 14591.27it/s]\u001b[A\n"," 45%|████▍     | 533994/1193514 [00:36<00:45, 14649.46it/s]\u001b[A\n"," 45%|████▍     | 535495/1193514 [00:36<00:44, 14753.86it/s]\u001b[A\n"," 45%|████▍     | 536972/1193514 [00:36<00:44, 14731.32it/s]\u001b[A\n"," 45%|████▌     | 538451/1193514 [00:36<00:44, 14748.72it/s]\u001b[A\n"," 45%|████▌     | 539934/1193514 [00:36<00:44, 14771.76it/s]\u001b[A\n"," 45%|████▌     | 541457/1193514 [00:36<00:43, 14904.98it/s]\u001b[A\n"," 45%|████▌     | 542980/1193514 [00:36<00:43, 15000.35it/s]\u001b[A\n"," 46%|████▌     | 544501/1193514 [00:37<00:43, 15061.48it/s]\u001b[A\n"," 46%|████▌     | 546008/1193514 [00:37<00:43, 14910.78it/s]\u001b[A\n"," 46%|████▌     | 547500/1193514 [00:37<00:43, 14907.88it/s]\u001b[A\n"," 46%|████▌     | 548992/1193514 [00:37<00:44, 14609.62it/s]\u001b[A\n"," 46%|████▌     | 550455/1193514 [00:37<00:44, 14554.21it/s]\u001b[A\n"," 46%|████▌     | 551968/1193514 [00:37<00:43, 14720.21it/s]\u001b[A\n"," 46%|████▋     | 553442/1193514 [00:37<00:44, 14457.86it/s]\u001b[A\n"," 46%|████▋     | 554902/1193514 [00:37<00:44, 14499.26it/s]\u001b[A\n"," 47%|████▋     | 556371/1193514 [00:37<00:43, 14554.20it/s]\u001b[A\n"," 47%|████▋     | 557888/1193514 [00:37<00:43, 14731.64it/s]\u001b[A\n"," 47%|████▋     | 559408/1193514 [00:38<00:42, 14868.57it/s]\u001b[A\n"," 47%|████▋     | 560897/1193514 [00:38<00:42, 14795.91it/s]\u001b[A\n"," 47%|████▋     | 562431/1193514 [00:38<00:42, 14953.81it/s]\u001b[A\n"," 47%|████▋     | 563928/1193514 [00:38<00:42, 14760.20it/s]\u001b[A\n"," 47%|████▋     | 565424/1193514 [00:38<00:42, 14816.73it/s]\u001b[A\n"," 48%|████▊     | 566981/1193514 [00:38<00:41, 15034.36it/s]\u001b[A\n"," 48%|████▊     | 568486/1193514 [00:38<00:42, 14779.43it/s]\u001b[A\n"," 48%|████▊     | 570014/1193514 [00:38<00:41, 14924.63it/s]\u001b[A\n"," 48%|████▊     | 571509/1193514 [00:38<00:42, 14797.71it/s]\u001b[A\n"," 48%|████▊     | 573030/1193514 [00:39<00:41, 14917.57it/s]\u001b[A\n"," 48%|████▊     | 574562/1193514 [00:39<00:41, 15033.88it/s]\u001b[A\n"," 48%|████▊     | 576067/1193514 [00:39<00:41, 15027.58it/s]\u001b[A\n"," 48%|████▊     | 577571/1193514 [00:39<00:41, 14975.89it/s]\u001b[A\n"," 49%|████▊     | 579070/1193514 [00:39<00:41, 14734.11it/s]\u001b[A\n"," 49%|████▊     | 580545/1193514 [00:39<00:41, 14691.75it/s]\u001b[A\n"," 49%|████▉     | 582053/1193514 [00:39<00:41, 14802.75it/s]\u001b[A\n"," 49%|████▉     | 583535/1193514 [00:39<00:41, 14538.41it/s]\u001b[A\n"," 49%|████▉     | 585005/1193514 [00:39<00:41, 14586.00it/s]\u001b[A\n"," 49%|████▉     | 586477/1193514 [00:39<00:41, 14625.89it/s]\u001b[A\n"," 49%|████▉     | 587970/1193514 [00:40<00:41, 14713.29it/s]\u001b[A\n"," 49%|████▉     | 589505/1193514 [00:40<00:40, 14896.46it/s]\u001b[A\n"," 50%|████▉     | 590996/1193514 [00:40<00:40, 14830.04it/s]\u001b[A\n"," 50%|████▉     | 592522/1193514 [00:40<00:40, 14954.89it/s]\u001b[A\n"," 50%|████▉     | 594019/1193514 [00:40<00:40, 14857.04it/s]\u001b[A\n"," 50%|████▉     | 595512/1193514 [00:40<00:40, 14876.09it/s]\u001b[A\n"," 50%|█████     | 597001/1193514 [00:40<00:40, 14854.71it/s]\u001b[A\n"," 50%|█████     | 598502/1193514 [00:40<00:39, 14900.04it/s]\u001b[A\n"," 50%|█████     | 600021/1193514 [00:40<00:39, 14985.71it/s]\u001b[A\n"," 50%|█████     | 601544/1193514 [00:40<00:39, 15054.80it/s]\u001b[A\n"," 51%|█████     | 603050/1193514 [00:41<00:39, 14860.36it/s]\u001b[A\n"," 51%|█████     | 604567/1193514 [00:41<00:39, 14950.18it/s]\u001b[A\n"," 51%|█████     | 606063/1193514 [00:41<00:39, 14814.79it/s]\u001b[A\n"," 51%|█████     | 607608/1193514 [00:41<00:39, 14997.59it/s]\u001b[A\n"," 51%|█████     | 609109/1193514 [00:41<00:39, 14803.42it/s]\u001b[A\n"," 51%|█████     | 610612/1193514 [00:41<00:39, 14869.43it/s]\u001b[A\n"," 51%|█████▏    | 612100/1193514 [00:41<00:39, 14766.34it/s]\u001b[A\n"," 51%|█████▏    | 613608/1193514 [00:41<00:39, 14857.13it/s]\u001b[A\n"," 52%|█████▏    | 615111/1193514 [00:41<00:38, 14907.01it/s]\u001b[A\n"," 52%|█████▏    | 616613/1193514 [00:41<00:38, 14939.62it/s]\u001b[A\n"," 52%|█████▏    | 618108/1193514 [00:42<00:38, 14906.50it/s]\u001b[A\n"," 52%|█████▏    | 619599/1193514 [00:42<00:38, 14860.82it/s]\u001b[A\n"," 52%|█████▏    | 621087/1193514 [00:42<00:38, 14864.13it/s]\u001b[A\n"," 52%|█████▏    | 622637/1193514 [00:42<00:37, 15048.44it/s]\u001b[A\n"," 52%|█████▏    | 624143/1193514 [00:42<00:37, 15044.10it/s]\u001b[A\n"," 52%|█████▏    | 625648/1193514 [00:42<00:38, 14938.65it/s]\u001b[A\n"," 53%|█████▎    | 627143/1193514 [00:42<00:37, 14927.08it/s]\u001b[A\n"," 53%|█████▎    | 628666/1193514 [00:42<00:37, 15016.67it/s]\u001b[A\n"," 53%|█████▎    | 630206/1193514 [00:42<00:37, 15128.17it/s]\u001b[A\n"," 53%|█████▎    | 631720/1193514 [00:42<00:37, 15110.23it/s]\u001b[A\n"," 53%|█████▎    | 633232/1193514 [00:43<00:37, 15033.26it/s]\u001b[A\n"," 53%|█████▎    | 634741/1193514 [00:43<00:37, 15048.50it/s]\u001b[A\n"," 53%|█████▎    | 636247/1193514 [00:43<00:37, 14999.49it/s]\u001b[A\n"," 53%|█████▎    | 637794/1193514 [00:43<00:36, 15135.11it/s]\u001b[A\n"," 54%|█████▎    | 639308/1193514 [00:43<00:36, 15025.11it/s]\u001b[A\n"," 54%|█████▎    | 640811/1193514 [00:43<00:37, 14801.45it/s]\u001b[A\n"," 54%|█████▍    | 642300/1193514 [00:43<00:37, 14825.70it/s]\u001b[A\n"," 54%|█████▍    | 643784/1193514 [00:43<00:37, 14485.85it/s]\u001b[A\n"," 54%|█████▍    | 645272/1193514 [00:43<00:37, 14599.41it/s]\u001b[A\n"," 54%|█████▍    | 646758/1193514 [00:43<00:37, 14676.48it/s]\u001b[A\n"," 54%|█████▍    | 648252/1193514 [00:44<00:36, 14753.35it/s]\u001b[A\n"," 54%|█████▍    | 649729/1193514 [00:44<00:37, 14482.10it/s]\u001b[A\n"," 55%|█████▍    | 651180/1193514 [00:44<00:37, 14463.91it/s]\u001b[A\n"," 55%|█████▍    | 652713/1193514 [00:44<00:36, 14711.28it/s]\u001b[A\n"," 55%|█████▍    | 654214/1193514 [00:44<00:36, 14798.76it/s]\u001b[A\n"," 55%|█████▍    | 655721/1193514 [00:44<00:36, 14876.44it/s]\u001b[A\n"," 55%|█████▌    | 657241/1193514 [00:44<00:35, 14971.05it/s]\u001b[A\n"," 55%|█████▌    | 658740/1193514 [00:44<00:36, 14836.25it/s]\u001b[A\n"," 55%|█████▌    | 660262/1193514 [00:44<00:35, 14947.78it/s]\u001b[A\n"," 55%|█████▌    | 661808/1193514 [00:44<00:35, 15096.43it/s]\u001b[A\n"," 56%|█████▌    | 663319/1193514 [00:45<00:35, 15075.39it/s]\u001b[A\n"," 56%|█████▌    | 664828/1193514 [00:45<00:35, 15038.85it/s]\u001b[A\n"," 56%|█████▌    | 666333/1193514 [00:45<00:35, 14797.91it/s]\u001b[A\n"," 56%|█████▌    | 667839/1193514 [00:45<00:35, 14874.99it/s]\u001b[A\n"," 56%|█████▌    | 669338/1193514 [00:45<00:35, 14908.87it/s]\u001b[A\n"," 56%|█████▌    | 670837/1193514 [00:45<00:35, 14930.92it/s]\u001b[A\n"," 56%|█████▋    | 672331/1193514 [00:45<00:35, 14882.31it/s]\u001b[A\n"," 56%|█████▋    | 673820/1193514 [00:45<00:34, 14883.97it/s]\u001b[A\n"," 57%|█████▋    | 675340/1193514 [00:45<00:34, 14977.18it/s]\u001b[A\n"," 57%|█████▋    | 676889/1193514 [00:45<00:34, 15125.04it/s]\u001b[A\n"," 57%|█████▋    | 678426/1193514 [00:46<00:33, 15195.41it/s]\u001b[A\n"," 57%|█████▋    | 679947/1193514 [00:46<00:34, 14712.68it/s]\u001b[A\n"," 57%|█████▋    | 681469/1193514 [00:46<00:34, 14859.18it/s]\u001b[A\n"," 57%|█████▋    | 682958/1193514 [00:46<00:34, 14842.94it/s]\u001b[A\n"," 57%|█████▋    | 684463/1193514 [00:46<00:34, 14904.07it/s]\u001b[A\n"," 57%|█████▋    | 685964/1193514 [00:46<00:33, 14934.33it/s]\u001b[A\n"," 58%|█████▊    | 687459/1193514 [00:46<00:33, 14893.85it/s]\u001b[A\n"," 58%|█████▊    | 688969/1193514 [00:46<00:33, 14953.80it/s]\u001b[A\n"," 58%|█████▊    | 690507/1193514 [00:46<00:33, 15077.18it/s]\u001b[A\n"," 58%|█████▊    | 692016/1193514 [00:46<00:33, 15042.89it/s]\u001b[A\n"," 58%|█████▊    | 693546/1193514 [00:47<00:33, 15117.79it/s]\u001b[A\n"," 58%|█████▊    | 695078/1193514 [00:47<00:32, 15176.57it/s]\u001b[A\n"," 58%|█████▊    | 696602/1193514 [00:47<00:32, 15193.56it/s]\u001b[A\n"," 58%|█████▊    | 698122/1193514 [00:47<00:33, 14971.88it/s]\u001b[A\n"," 59%|█████▊    | 699621/1193514 [00:47<00:33, 14953.09it/s]\u001b[A\n"," 59%|█████▊    | 701171/1193514 [00:47<00:32, 15110.21it/s]\u001b[A\n"," 59%|█████▉    | 702694/1193514 [00:47<00:32, 15143.80it/s]\u001b[A\n"," 59%|█████▉    | 704209/1193514 [00:47<00:32, 15023.10it/s]\u001b[A\n"," 59%|█████▉    | 705748/1193514 [00:47<00:32, 15127.54it/s]\u001b[A\n"," 59%|█████▉    | 707271/1193514 [00:48<00:32, 15155.58it/s]\u001b[A\n"," 59%|█████▉    | 708788/1193514 [00:48<00:31, 15151.99it/s]\u001b[A\n"," 60%|█████▉    | 710304/1193514 [00:48<00:32, 15088.85it/s]\u001b[A\n"," 60%|█████▉    | 711814/1193514 [00:48<00:32, 14850.85it/s]\u001b[A\n"," 60%|█████▉    | 713301/1193514 [00:48<00:32, 14706.17it/s]\u001b[A\n"," 60%|█████▉    | 714781/1193514 [00:48<00:32, 14732.87it/s]\u001b[A\n"," 60%|██████    | 716278/1193514 [00:48<00:32, 14802.54it/s]\u001b[A\n"," 60%|██████    | 717802/1193514 [00:48<00:31, 14930.48it/s]\u001b[A\n"," 60%|██████    | 719317/1193514 [00:48<00:31, 14993.96it/s]\u001b[A\n"," 60%|██████    | 720847/1193514 [00:48<00:31, 15083.04it/s]\u001b[A\n"," 61%|██████    | 722392/1193514 [00:49<00:31, 15189.55it/s]\u001b[A\n"," 61%|██████    | 723912/1193514 [00:49<00:31, 15091.16it/s]\u001b[A\n"," 61%|██████    | 725430/1193514 [00:49<00:30, 15115.88it/s]\u001b[A\n"," 61%|██████    | 726942/1193514 [00:49<00:30, 15053.53it/s]\u001b[A\n"," 61%|██████    | 728448/1193514 [00:49<00:31, 14861.88it/s]\u001b[A\n"," 61%|██████    | 729935/1193514 [00:49<00:31, 14770.79it/s]\u001b[A\n"," 61%|██████▏   | 731429/1193514 [00:49<00:31, 14818.79it/s]\u001b[A\n"," 61%|██████▏   | 732942/1193514 [00:49<00:30, 14908.97it/s]\u001b[A\n"," 62%|██████▏   | 734434/1193514 [00:49<00:30, 14829.29it/s]\u001b[A\n"," 62%|██████▏   | 735951/1193514 [00:49<00:30, 14927.70it/s]\u001b[A\n"," 62%|██████▏   | 737479/1193514 [00:50<00:30, 15030.75it/s]\u001b[A\n"," 62%|██████▏   | 738989/1193514 [00:50<00:30, 15048.68it/s]\u001b[A\n"," 62%|██████▏   | 740495/1193514 [00:50<00:30, 14935.27it/s]\u001b[A\n"," 62%|██████▏   | 741989/1193514 [00:50<00:30, 14855.52it/s]\u001b[A\n"," 62%|██████▏   | 743475/1193514 [00:50<00:30, 14758.82it/s]\u001b[A\n"," 62%|██████▏   | 744987/1193514 [00:50<00:30, 14864.30it/s]\u001b[A\n"," 63%|██████▎   | 746521/1193514 [00:50<00:29, 15002.75it/s]\u001b[A\n"," 63%|██████▎   | 748022/1193514 [00:50<00:29, 14985.03it/s]\u001b[A\n"," 63%|██████▎   | 749538/1193514 [00:50<00:29, 15035.05it/s]\u001b[A\n"," 63%|██████▎   | 751042/1193514 [00:50<00:29, 14931.50it/s]\u001b[A\n"," 63%|██████▎   | 752587/1193514 [00:51<00:29, 15080.56it/s]\u001b[A\n"," 63%|██████▎   | 754122/1193514 [00:51<00:28, 15158.25it/s]\u001b[A\n"," 63%|██████▎   | 755639/1193514 [00:51<00:29, 15093.65it/s]\u001b[A\n"," 63%|██████▎   | 757173/1193514 [00:51<00:28, 15166.07it/s]\u001b[A\n"," 64%|██████▎   | 758691/1193514 [00:51<00:29, 14886.76it/s]\u001b[A\n"," 64%|██████▎   | 760221/1193514 [00:51<00:28, 15006.10it/s]\u001b[A\n"," 64%|██████▍   | 761755/1193514 [00:51<00:28, 15103.59it/s]\u001b[A\n"," 64%|██████▍   | 763267/1193514 [00:51<00:28, 15017.62it/s]\u001b[A\n"," 64%|██████▍   | 764770/1193514 [00:51<00:28, 14888.93it/s]\u001b[A\n"," 64%|██████▍   | 766260/1193514 [00:51<00:28, 14840.30it/s]\u001b[A\n"," 64%|██████▍   | 767789/1193514 [00:52<00:28, 14972.38it/s]\u001b[A\n"," 64%|██████▍   | 769287/1193514 [00:52<00:28, 14961.38it/s]\u001b[A\n"," 65%|██████▍   | 770799/1193514 [00:52<00:28, 15006.38it/s]\u001b[A\n"," 65%|██████▍   | 772301/1193514 [00:52<00:28, 14876.72it/s]\u001b[A\n"," 65%|██████▍   | 773790/1193514 [00:52<00:28, 14552.91it/s]\u001b[A\n"," 65%|██████▍   | 775303/1193514 [00:52<00:28, 14719.34it/s]\u001b[A\n"," 65%|██████▌   | 776787/1193514 [00:52<00:28, 14753.92it/s]\u001b[A\n"," 65%|██████▌   | 778289/1193514 [00:52<00:27, 14832.45it/s]\u001b[A\n"," 65%|██████▌   | 779774/1193514 [00:52<00:28, 14768.05it/s]\u001b[A\n"," 65%|██████▌   | 781266/1193514 [00:52<00:27, 14811.58it/s]\u001b[A\n"," 66%|██████▌   | 782774/1193514 [00:53<00:27, 14890.27it/s]\u001b[A\n"," 66%|██████▌   | 784264/1193514 [00:53<00:27, 14788.29it/s]\u001b[A\n"," 66%|██████▌   | 785760/1193514 [00:53<00:27, 14837.06it/s]\u001b[A\n"," 66%|██████▌   | 787253/1193514 [00:53<00:27, 14863.02it/s]\u001b[A\n"," 66%|██████▌   | 788740/1193514 [00:53<00:27, 14588.51it/s]\u001b[A\n"," 66%|██████▌   | 790256/1193514 [00:53<00:27, 14755.07it/s]\u001b[A\n"," 66%|██████▋   | 791779/1193514 [00:53<00:26, 14890.29it/s]\u001b[A\n"," 66%|██████▋   | 793281/1193514 [00:53<00:26, 14928.66it/s]\u001b[A\n"," 67%|██████▋   | 794787/1193514 [00:53<00:26, 14965.12it/s]\u001b[A\n"," 67%|██████▋   | 796285/1193514 [00:53<00:26, 14869.47it/s]\u001b[A\n"," 67%|██████▋   | 797773/1193514 [00:54<00:26, 14760.87it/s]\u001b[A\n"," 67%|██████▋   | 799250/1193514 [00:54<00:26, 14734.06it/s]\u001b[A\n"," 67%|██████▋   | 800724/1193514 [00:54<00:27, 14490.33it/s]\u001b[A\n"," 67%|██████▋   | 802224/1193514 [00:54<00:26, 14636.63it/s]\u001b[A\n"," 67%|██████▋   | 803689/1193514 [00:54<00:26, 14454.31it/s]\u001b[A\n"," 67%|██████▋   | 805175/1193514 [00:54<00:26, 14572.87it/s]\u001b[A\n"," 68%|██████▊   | 806701/1193514 [00:54<00:26, 14772.12it/s]\u001b[A\n"," 68%|██████▊   | 808251/1193514 [00:54<00:25, 14981.56it/s]\u001b[A\n"," 68%|██████▊   | 809751/1193514 [00:54<00:25, 14883.01it/s]\u001b[A\n"," 68%|██████▊   | 811245/1193514 [00:54<00:25, 14894.96it/s]\u001b[A\n"," 68%|██████▊   | 812744/1193514 [00:55<00:25, 14922.13it/s]\u001b[A\n"," 68%|██████▊   | 814237/1193514 [00:55<00:25, 14836.96it/s]\u001b[A\n"," 68%|██████▊   | 815763/1193514 [00:55<00:25, 14959.21it/s]\u001b[A\n"," 68%|██████▊   | 817260/1193514 [00:55<00:25, 14941.12it/s]\u001b[A\n"," 69%|██████▊   | 818755/1193514 [00:55<00:25, 14646.54it/s]\u001b[A\n"," 69%|██████▊   | 820225/1193514 [00:55<00:25, 14659.91it/s]\u001b[A\n"," 69%|██████▉   | 821714/1193514 [00:55<00:25, 14726.07it/s]\u001b[A\n"," 69%|██████▉   | 823259/1193514 [00:55<00:24, 14936.00it/s]\u001b[A\n"," 69%|██████▉   | 824754/1193514 [00:55<00:24, 14926.61it/s]\u001b[A\n"," 69%|██████▉   | 826282/1193514 [00:55<00:24, 15030.29it/s]\u001b[A\n"," 69%|██████▉   | 827786/1193514 [00:56<00:24, 14978.86it/s]\u001b[A\n"," 69%|██████▉   | 829285/1193514 [00:56<00:24, 14924.87it/s]\u001b[A\n"," 70%|██████▉   | 830822/1193514 [00:56<00:24, 15054.98it/s]\u001b[A\n"," 70%|██████▉   | 832329/1193514 [00:56<00:24, 14930.67it/s]\u001b[A\n"," 70%|██████▉   | 833825/1193514 [00:56<00:24, 14935.19it/s]\u001b[A\n"," 70%|██████▉   | 835319/1193514 [00:56<00:24, 14826.35it/s]\u001b[A\n"," 70%|███████   | 836833/1193514 [00:56<00:23, 14918.53it/s]\u001b[A\n"," 70%|███████   | 838356/1193514 [00:56<00:23, 15010.40it/s]\u001b[A\n"," 70%|███████   | 839859/1193514 [00:56<00:23, 15014.84it/s]\u001b[A\n"," 70%|███████   | 841361/1193514 [00:57<00:23, 14930.52it/s]\u001b[A\n"," 71%|███████   | 842876/1193514 [00:57<00:23, 14995.34it/s]\u001b[A\n"," 71%|███████   | 844376/1193514 [00:57<00:23, 14664.64it/s]\u001b[A\n"," 71%|███████   | 845896/1193514 [00:57<00:23, 14819.51it/s]\u001b[A\n"," 71%|███████   | 847420/1193514 [00:57<00:23, 14940.60it/s]\u001b[A\n"," 71%|███████   | 848916/1193514 [00:57<00:23, 14919.24it/s]\u001b[A\n"," 71%|███████▏  | 850415/1193514 [00:57<00:22, 14939.76it/s]\u001b[A\n"," 71%|███████▏  | 851910/1193514 [00:57<00:23, 14806.34it/s]\u001b[A\n"," 72%|███████▏  | 853438/1193514 [00:57<00:22, 14945.04it/s]\u001b[A\n"," 72%|███████▏  | 854944/1193514 [00:57<00:22, 14978.25it/s]\u001b[A\n"," 72%|███████▏  | 856491/1193514 [00:58<00:22, 15120.30it/s]\u001b[A\n"," 72%|███████▏  | 858023/1193514 [00:58<00:22, 15177.28it/s]\u001b[A\n"," 72%|███████▏  | 859542/1193514 [00:58<00:22, 14729.47it/s]\u001b[A\n"," 72%|███████▏  | 861066/1193514 [00:58<00:22, 14877.22it/s]\u001b[A\n"," 72%|███████▏  | 862557/1193514 [00:58<00:22, 14869.38it/s]\u001b[A\n"," 72%|███████▏  | 864054/1193514 [00:58<00:22, 14896.41it/s]\u001b[A\n"," 73%|███████▎  | 865586/1193514 [00:58<00:21, 15020.77it/s]\u001b[A\n"," 73%|███████▎  | 867090/1193514 [00:58<00:22, 14830.71it/s]\u001b[A\n"," 73%|███████▎  | 868604/1193514 [00:58<00:21, 14920.03it/s]\u001b[A\n"," 73%|███████▎  | 870098/1193514 [00:58<00:21, 14715.35it/s]\u001b[A\n"," 73%|███████▎  | 871614/1193514 [00:59<00:21, 14843.76it/s]\u001b[A\n"," 73%|███████▎  | 873136/1193514 [00:59<00:21, 14953.77it/s]\u001b[A\n"," 73%|███████▎  | 874658/1193514 [00:59<00:21, 15030.78it/s]\u001b[A\n"," 73%|███████▎  | 876162/1193514 [00:59<00:21, 14815.45it/s]\u001b[A\n"," 74%|███████▎  | 877655/1193514 [00:59<00:21, 14847.33it/s]\u001b[A\n"," 74%|███████▎  | 879141/1193514 [00:59<00:21, 14757.76it/s]\u001b[A\n"," 74%|███████▍  | 880654/1193514 [00:59<00:21, 14865.10it/s]\u001b[A\n"," 74%|███████▍  | 882185/1193514 [00:59<00:20, 14995.30it/s]\u001b[A\n"," 74%|███████▍  | 883686/1193514 [00:59<00:20, 14956.21it/s]\u001b[A\n"," 74%|███████▍  | 885183/1193514 [00:59<00:20, 14891.05it/s]\u001b[A\n"," 74%|███████▍  | 886724/1193514 [01:00<00:20, 15042.44it/s]\u001b[A\n"," 74%|███████▍  | 888255/1193514 [01:00<00:20, 15119.63it/s]\u001b[A\n"," 75%|███████▍  | 889788/1193514 [01:00<00:20, 15179.68it/s]\u001b[A\n"," 75%|███████▍  | 891307/1193514 [01:00<00:20, 14931.74it/s]\u001b[A\n"," 75%|███████▍  | 892802/1193514 [01:00<00:20, 14917.75it/s]\u001b[A\n"," 75%|███████▍  | 894295/1193514 [01:00<00:20, 14908.31it/s]\u001b[A\n"," 75%|███████▌  | 895833/1193514 [01:00<00:19, 15044.50it/s]\u001b[A\n"," 75%|███████▌  | 897339/1193514 [01:00<00:19, 15019.30it/s]\u001b[A\n"," 75%|███████▌  | 898842/1193514 [01:00<00:19, 14995.22it/s]\u001b[A\n"," 75%|███████▌  | 900342/1193514 [01:00<00:19, 14862.64it/s]\u001b[A\n"," 76%|███████▌  | 901893/1193514 [01:01<00:19, 15049.85it/s]\u001b[A\n"," 76%|███████▌  | 903424/1193514 [01:01<00:19, 15125.69it/s]\u001b[A\n"," 76%|███████▌  | 904942/1193514 [01:01<00:19, 15141.29it/s]\u001b[A\n"," 76%|███████▌  | 906457/1193514 [01:01<00:19, 15050.18it/s]\u001b[A\n"," 76%|███████▌  | 907963/1193514 [01:01<00:19, 14972.38it/s]\u001b[A\n"," 76%|███████▌  | 909461/1193514 [01:01<00:19, 14829.54it/s]\u001b[A\n"," 76%|███████▋  | 910945/1193514 [01:01<00:19, 14566.83it/s]\u001b[A\n"," 76%|███████▋  | 912419/1193514 [01:01<00:19, 14614.65it/s]\u001b[A\n"," 77%|███████▋  | 913916/1193514 [01:01<00:18, 14718.95it/s]\u001b[A\n"," 77%|███████▋  | 915389/1193514 [01:01<00:19, 14079.48it/s]\u001b[A\n"," 77%|███████▋  | 916918/1193514 [01:02<00:19, 14421.54it/s]\u001b[A\n"," 77%|███████▋  | 918432/1193514 [01:02<00:18, 14629.01it/s]\u001b[A\n"," 77%|███████▋  | 919901/1193514 [01:02<00:18, 14576.08it/s]\u001b[A\n"," 77%|███████▋  | 921389/1193514 [01:02<00:18, 14665.46it/s]\u001b[A\n"," 77%|███████▋  | 922876/1193514 [01:02<00:18, 14724.99it/s]\u001b[A\n"," 77%|███████▋  | 924351/1193514 [01:02<00:18, 14669.07it/s]\u001b[A\n"," 78%|███████▊  | 925851/1193514 [01:02<00:18, 14766.62it/s]\u001b[A\n"," 78%|███████▊  | 927372/1193514 [01:02<00:17, 14894.38it/s]\u001b[A\n"," 78%|███████▊  | 928880/1193514 [01:02<00:17, 14947.15it/s]\u001b[A\n"," 78%|███████▊  | 930394/1193514 [01:02<00:17, 15002.84it/s]\u001b[A\n"," 78%|███████▊  | 931910/1193514 [01:03<00:17, 15048.45it/s]\u001b[A\n"," 78%|███████▊  | 933453/1193514 [01:03<00:17, 15160.91it/s]\u001b[A\n"," 78%|███████▊  | 934979/1193514 [01:03<00:17, 15188.28it/s]\u001b[A\n"," 78%|███████▊  | 936499/1193514 [01:03<00:16, 15180.95it/s]\u001b[A\n"," 79%|███████▊  | 938018/1193514 [01:03<00:16, 15041.96it/s]\u001b[A\n"," 79%|███████▊  | 939523/1193514 [01:03<00:17, 14901.27it/s]\u001b[A\n"," 79%|███████▉  | 941029/1193514 [01:03<00:16, 14947.08it/s]\u001b[A\n"," 79%|███████▉  | 942578/1193514 [01:03<00:16, 15103.96it/s]\u001b[A\n"," 79%|███████▉  | 944090/1193514 [01:03<00:16, 15078.69it/s]\u001b[A\n"," 79%|███████▉  | 945599/1193514 [01:03<00:16, 15040.68it/s]\u001b[A\n"," 79%|███████▉  | 947104/1193514 [01:04<00:16, 14724.67it/s]\u001b[A\n"," 79%|███████▉  | 948579/1193514 [01:04<00:16, 14536.45it/s]\u001b[A\n"," 80%|███████▉  | 950035/1193514 [01:04<00:16, 14449.28it/s]\u001b[A\n"," 80%|███████▉  | 951482/1193514 [01:04<00:16, 14378.75it/s]\u001b[A\n"," 80%|███████▉  | 952921/1193514 [01:04<00:16, 14291.65it/s]\u001b[A\n"," 80%|███████▉  | 954371/1193514 [01:04<00:16, 14352.60it/s]\u001b[A\n"," 80%|████████  | 955807/1193514 [01:04<00:16, 14339.38it/s]\u001b[A\n"," 80%|████████  | 957344/1193514 [01:04<00:16, 14631.41it/s]\u001b[A\n"," 80%|████████  | 958814/1193514 [01:04<00:16, 14650.00it/s]\u001b[A\n"," 80%|████████  | 960328/1193514 [01:05<00:15, 14793.31it/s]\u001b[A\n"," 81%|████████  | 961809/1193514 [01:05<00:15, 14778.36it/s]\u001b[A\n"," 81%|████████  | 963310/1193514 [01:05<00:15, 14845.17it/s]\u001b[A\n"," 81%|████████  | 964834/1193514 [01:05<00:15, 14960.56it/s]\u001b[A\n"," 81%|████████  | 966340/1193514 [01:05<00:15, 14990.02it/s]\u001b[A\n"," 81%|████████  | 967854/1193514 [01:05<00:15, 15031.94it/s]\u001b[A\n"," 81%|████████  | 969358/1193514 [01:05<00:14, 14971.00it/s]\u001b[A\n"," 81%|████████▏ | 970856/1193514 [01:05<00:15, 14637.97it/s]\u001b[A\n"," 81%|████████▏ | 972402/1193514 [01:05<00:14, 14873.99it/s]\u001b[A\n"," 82%|████████▏ | 973929/1193514 [01:05<00:14, 14989.98it/s]\u001b[A\n"," 82%|████████▏ | 975431/1193514 [01:06<00:14, 14996.88it/s]\u001b[A\n"," 82%|████████▏ | 976935/1193514 [01:06<00:14, 15007.49it/s]\u001b[A\n"," 82%|████████▏ | 978437/1193514 [01:06<00:14, 14967.42it/s]\u001b[A\n"," 82%|████████▏ | 979964/1193514 [01:06<00:14, 15054.65it/s]\u001b[A\n"," 82%|████████▏ | 981471/1193514 [01:06<00:14, 14965.71it/s]\u001b[A\n"," 82%|████████▏ | 982993/1193514 [01:06<00:13, 15039.41it/s]\u001b[A\n"," 82%|████████▏ | 984498/1193514 [01:06<00:14, 14910.30it/s]\u001b[A\n"," 83%|████████▎ | 985990/1193514 [01:06<00:13, 14897.79it/s]\u001b[A\n"," 83%|████████▎ | 987503/1193514 [01:06<00:13, 14965.11it/s]\u001b[A\n"," 83%|████████▎ | 989000/1193514 [01:06<00:13, 14898.67it/s]\u001b[A\n"," 83%|████████▎ | 990530/1193514 [01:07<00:13, 15014.28it/s]\u001b[A\n"," 83%|████████▎ | 992032/1193514 [01:07<00:13, 14895.29it/s]\u001b[A\n"," 83%|████████▎ | 993548/1193514 [01:07<00:13, 14972.38it/s]\u001b[A\n"," 83%|████████▎ | 995046/1193514 [01:07<00:13, 14968.54it/s]\u001b[A\n"," 83%|████████▎ | 996544/1193514 [01:07<00:13, 14873.98it/s]\u001b[A\n"," 84%|████████▎ | 998067/1193514 [01:07<00:13, 14977.03it/s]\u001b[A\n"," 84%|████████▍ | 999578/1193514 [01:07<00:12, 15015.68it/s]\u001b[A\n"," 84%|████████▍ | 1001080/1193514 [01:07<00:12, 14869.42it/s]\u001b[A\n"," 84%|████████▍ | 1002605/1193514 [01:07<00:12, 14980.90it/s]\u001b[A\n"," 84%|████████▍ | 1004104/1193514 [01:07<00:12, 14917.65it/s]\u001b[A\n"," 84%|████████▍ | 1005670/1193514 [01:08<00:12, 15130.21it/s]\u001b[A\n"," 84%|████████▍ | 1007213/1193514 [01:08<00:12, 15216.94it/s]\u001b[A\n"," 85%|████████▍ | 1008736/1193514 [01:08<00:12, 14931.35it/s]\u001b[A\n"," 85%|████████▍ | 1010256/1193514 [01:08<00:12, 15010.87it/s]\u001b[A\n"," 85%|████████▍ | 1011765/1193514 [01:08<00:12, 15033.32it/s]\u001b[A\n"," 85%|████████▍ | 1013349/1193514 [01:08<00:11, 15263.97it/s]\u001b[A\n"," 85%|████████▌ | 1014877/1193514 [01:08<00:11, 15206.40it/s]\u001b[A\n"," 85%|████████▌ | 1016399/1193514 [01:08<00:11, 14956.48it/s]\u001b[A\n"," 85%|████████▌ | 1017930/1193514 [01:08<00:11, 15059.80it/s]\u001b[A\n"," 85%|████████▌ | 1019439/1193514 [01:08<00:11, 15068.20it/s]\u001b[A\n"," 86%|████████▌ | 1020983/1193514 [01:09<00:11, 15177.39it/s]\u001b[A\n"," 86%|████████▌ | 1022548/1193514 [01:09<00:11, 15315.45it/s]\u001b[A\n"," 86%|████████▌ | 1024081/1193514 [01:09<00:11, 15153.46it/s]\u001b[A\n"," 86%|████████▌ | 1025613/1193514 [01:09<00:11, 15202.08it/s]\u001b[A\n"," 86%|████████▌ | 1027134/1193514 [01:09<00:10, 15131.73it/s]\u001b[A\n"," 86%|████████▌ | 1028651/1193514 [01:09<00:10, 15142.01it/s]\u001b[A\n"," 86%|████████▋ | 1030188/1193514 [01:09<00:10, 15209.66it/s]\u001b[A\n"," 86%|████████▋ | 1031710/1193514 [01:09<00:10, 15038.70it/s]\u001b[A\n"," 87%|████████▋ | 1033231/1193514 [01:09<00:10, 15086.63it/s]\u001b[A\n"," 87%|████████▋ | 1034741/1193514 [01:09<00:10, 15017.42it/s]\u001b[A\n"," 87%|████████▋ | 1036260/1193514 [01:10<00:10, 15066.69it/s]\u001b[A\n"," 87%|████████▋ | 1037781/1193514 [01:10<00:10, 15108.01it/s]\u001b[A\n"," 87%|████████▋ | 1039297/1193514 [01:10<00:10, 15121.01it/s]\u001b[A\n"," 87%|████████▋ | 1040820/1193514 [01:10<00:10, 15152.48it/s]\u001b[A\n"," 87%|████████▋ | 1042336/1193514 [01:10<00:10, 14987.05it/s]\u001b[A\n"," 87%|████████▋ | 1043868/1193514 [01:10<00:09, 15083.23it/s]\u001b[A\n"," 88%|████████▊ | 1045380/1193514 [01:10<00:09, 15094.14it/s]\u001b[A\n"," 88%|████████▊ | 1046890/1193514 [01:10<00:09, 15051.16it/s]\u001b[A\n"," 88%|████████▊ | 1048396/1193514 [01:10<00:09, 15041.35it/s]\u001b[A\n"," 88%|████████▊ | 1049901/1193514 [01:10<00:09, 14705.25it/s]\u001b[A\n"," 88%|████████▊ | 1051404/1193514 [01:11<00:09, 14799.64it/s]\u001b[A\n"," 88%|████████▊ | 1052930/1193514 [01:11<00:09, 14932.95it/s]\u001b[A\n"," 88%|████████▊ | 1054479/1193514 [01:11<00:09, 15094.79it/s]\u001b[A\n"," 88%|████████▊ | 1055990/1193514 [01:11<00:09, 15050.09it/s]\u001b[A\n"," 89%|████████▊ | 1057496/1193514 [01:11<00:09, 14996.24it/s]\u001b[A\n"," 89%|████████▊ | 1059031/1193514 [01:11<00:08, 15099.88it/s]\u001b[A\n"," 89%|████████▉ | 1060543/1193514 [01:11<00:08, 15105.84it/s]\u001b[A\n"," 89%|████████▉ | 1062055/1193514 [01:11<00:08, 14739.61it/s]\u001b[A\n"," 89%|████████▉ | 1063542/1193514 [01:11<00:08, 14777.76it/s]\u001b[A\n"," 89%|████████▉ | 1065035/1193514 [01:11<00:08, 14820.85it/s]\u001b[A\n"," 89%|████████▉ | 1066570/1193514 [01:12<00:08, 14974.34it/s]\u001b[A\n"," 89%|████████▉ | 1068073/1193514 [01:12<00:08, 14988.74it/s]\u001b[A\n"," 90%|████████▉ | 1069617/1193514 [01:12<00:08, 15119.60it/s]\u001b[A\n"," 90%|████████▉ | 1071130/1193514 [01:12<00:08, 15037.66it/s]\u001b[A\n"," 90%|████████▉ | 1072635/1193514 [01:12<00:08, 14890.02it/s]\u001b[A\n"," 90%|█████████ | 1074190/1193514 [01:12<00:07, 15080.01it/s]\u001b[A\n"," 90%|█████████ | 1075700/1193514 [01:12<00:07, 15043.55it/s]\u001b[A\n"," 90%|█████████ | 1077206/1193514 [01:12<00:07, 14967.74it/s]\u001b[A\n"," 90%|█████████ | 1078762/1193514 [01:12<00:07, 15140.11it/s]\u001b[A\n"," 91%|█████████ | 1080277/1193514 [01:13<00:07, 14810.11it/s]\u001b[A\n"," 91%|█████████ | 1081820/1193514 [01:13<00:07, 14989.11it/s]\u001b[A\n"," 91%|█████████ | 1083328/1193514 [01:13<00:07, 15014.50it/s]\u001b[A\n"," 91%|█████████ | 1084856/1193514 [01:13<00:07, 15091.83it/s]\u001b[A\n"," 91%|█████████ | 1086367/1193514 [01:13<00:07, 14980.45it/s]\u001b[A\n"," 91%|█████████ | 1087867/1193514 [01:13<00:07, 14720.98it/s]\u001b[A\n"," 91%|█████████▏| 1089390/1193514 [01:13<00:07, 14868.14it/s]\u001b[A\n"," 91%|█████████▏| 1090895/1193514 [01:13<00:06, 14920.39it/s]\u001b[A\n"," 92%|█████████▏| 1092389/1193514 [01:13<00:06, 14773.75it/s]\u001b[A\n"," 92%|█████████▏| 1093911/1193514 [01:13<00:06, 14901.10it/s]\u001b[A\n"," 92%|█████████▏| 1095403/1193514 [01:14<00:06, 14637.59it/s]\u001b[A\n"," 92%|█████████▏| 1096904/1193514 [01:14<00:06, 14745.36it/s]\u001b[A\n"," 92%|█████████▏| 1098401/1193514 [01:14<00:06, 14809.40it/s]\u001b[A\n"," 92%|█████████▏| 1099910/1193514 [01:14<00:06, 14891.88it/s]\u001b[A\n"," 92%|█████████▏| 1101401/1193514 [01:14<00:06, 14892.91it/s]\u001b[A\n"," 92%|█████████▏| 1102891/1193514 [01:14<00:06, 14855.95it/s]\u001b[A\n"," 93%|█████████▎| 1104451/1193514 [01:14<00:05, 15070.63it/s]\u001b[A\n"," 93%|█████████▎| 1105981/1193514 [01:14<00:05, 15136.90it/s]\u001b[A\n"," 93%|█████████▎| 1107496/1193514 [01:14<00:05, 15098.01it/s]\u001b[A\n"," 93%|█████████▎| 1109058/1193514 [01:14<00:05, 15249.26it/s]\u001b[A\n"," 93%|█████████▎| 1110584/1193514 [01:15<00:05, 15069.81it/s]\u001b[A\n"," 93%|█████████▎| 1112101/1193514 [01:15<00:05, 15097.90it/s]\u001b[A\n"," 93%|█████████▎| 1113628/1193514 [01:15<00:05, 15146.81it/s]\u001b[A\n"," 93%|█████████▎| 1115154/1193514 [01:15<00:05, 15179.74it/s]\u001b[A\n"," 94%|█████████▎| 1116673/1193514 [01:15<00:05, 15105.72it/s]\u001b[A\n"," 94%|█████████▎| 1118184/1193514 [01:15<00:05, 14935.99it/s]\u001b[A\n"," 94%|█████████▍| 1119679/1193514 [01:15<00:04, 14923.08it/s]\u001b[A\n"," 94%|█████████▍| 1121208/1193514 [01:15<00:04, 15029.45it/s]\u001b[A\n"," 94%|█████████▍| 1122712/1193514 [01:15<00:04, 14987.33it/s]\u001b[A\n"," 94%|█████████▍| 1124212/1193514 [01:15<00:04, 14866.77it/s]\u001b[A\n"," 94%|█████████▍| 1125700/1193514 [01:16<00:04, 14750.44it/s]\u001b[A\n"," 94%|█████████▍| 1127186/1193514 [01:16<00:04, 14781.60it/s]\u001b[A\n"," 95%|█████████▍| 1128668/1193514 [01:16<00:04, 14791.40it/s]\u001b[A\n"," 95%|█████████▍| 1130164/1193514 [01:16<00:04, 14838.99it/s]\u001b[A\n"," 95%|█████████▍| 1131649/1193514 [01:16<00:04, 14681.29it/s]\u001b[A\n"," 95%|█████████▍| 1133136/1193514 [01:16<00:04, 14735.37it/s]\u001b[A\n"," 95%|█████████▌| 1134610/1193514 [01:16<00:04, 14324.00it/s]\u001b[A\n"," 95%|█████████▌| 1136100/1193514 [01:16<00:03, 14492.00it/s]\u001b[A\n"," 95%|█████████▌| 1137552/1193514 [01:16<00:03, 14468.92it/s]\u001b[A\n"," 95%|█████████▌| 1139083/1193514 [01:16<00:03, 14709.68it/s]\u001b[A\n"," 96%|█████████▌| 1140576/1193514 [01:17<00:03, 14772.94it/s]\u001b[A\n"," 96%|█████████▌| 1142110/1193514 [01:17<00:03, 14936.40it/s]\u001b[A\n"," 96%|█████████▌| 1143650/1193514 [01:17<00:03, 15072.07it/s]\u001b[A\n"," 96%|█████████▌| 1145199/1193514 [01:17<00:03, 15193.80it/s]\u001b[A\n"," 96%|█████████▌| 1146720/1193514 [01:17<00:03, 15040.07it/s]\u001b[A\n"," 96%|█████████▌| 1148226/1193514 [01:17<00:03, 14973.50it/s]\u001b[A\n"," 96%|█████████▋| 1149725/1193514 [01:17<00:02, 14924.39it/s]\u001b[A\n"," 96%|█████████▋| 1151241/1193514 [01:17<00:02, 14991.95it/s]\u001b[A\n"," 97%|█████████▋| 1152741/1193514 [01:17<00:02, 14963.03it/s]\u001b[A\n"," 97%|█████████▋| 1154272/1193514 [01:17<00:02, 15063.53it/s]\u001b[A\n"," 97%|█████████▋| 1155779/1193514 [01:18<00:02, 14904.67it/s]\u001b[A\n"," 97%|█████████▋| 1157271/1193514 [01:18<00:02, 14829.53it/s]\u001b[A\n"," 97%|█████████▋| 1158794/1193514 [01:18<00:02, 14946.76it/s]\u001b[A\n"," 97%|█████████▋| 1160324/1193514 [01:18<00:02, 15050.55it/s]\u001b[A\n"," 97%|█████████▋| 1161830/1193514 [01:18<00:02, 14964.07it/s]\u001b[A\n"," 97%|█████████▋| 1163337/1193514 [01:18<00:02, 14995.30it/s]\u001b[A\n"," 98%|█████████▊| 1164846/1193514 [01:18<00:01, 15022.16it/s]\u001b[A\n"," 98%|█████████▊| 1166410/1193514 [01:18<00:01, 15200.71it/s]\u001b[A\n"," 98%|█████████▊| 1167931/1193514 [01:18<00:01, 15091.31it/s]\u001b[A\n"," 98%|█████████▊| 1169443/1193514 [01:18<00:01, 15099.59it/s]\u001b[A\n"," 98%|█████████▊| 1170954/1193514 [01:19<00:01, 15047.49it/s]\u001b[A\n"," 98%|█████████▊| 1172460/1193514 [01:19<00:01, 14924.50it/s]\u001b[A\n"," 98%|█████████▊| 1173980/1193514 [01:19<00:01, 15003.05it/s]\u001b[A\n"," 98%|█████████▊| 1175485/1193514 [01:19<00:01, 15014.56it/s]\u001b[A\n"," 99%|█████████▊| 1176996/1193514 [01:19<00:01, 15039.42it/s]\u001b[A\n"," 99%|█████████▊| 1178528/1193514 [01:19<00:00, 15120.69it/s]\u001b[A\n"," 99%|█████████▉| 1180041/1193514 [01:19<00:00, 14893.10it/s]\u001b[A\n"," 99%|█████████▉| 1181532/1193514 [01:19<00:00, 14680.58it/s]\u001b[A\n"," 99%|█████████▉| 1183012/1193514 [01:19<00:00, 14716.14it/s]\u001b[A\n"," 99%|█████████▉| 1184536/1193514 [01:19<00:00, 14867.30it/s]\u001b[A\n"," 99%|█████████▉| 1186024/1193514 [01:20<00:00, 14673.43it/s]\u001b[A\n"," 99%|█████████▉| 1187519/1193514 [01:20<00:00, 14753.91it/s]\u001b[A\n","100%|█████████▉| 1189030/1193514 [01:20<00:00, 14856.96it/s]\u001b[A\n","100%|█████████▉| 1190520/1193514 [01:20<00:00, 14868.97it/s]\u001b[A\n","100%|█████████▉| 1192050/1193514 [01:20<00:00, 14994.76it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|█████████▉| 1192050/1193514 [01:32<00:00, 14994.76it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.803 | Val. Loss: 0.704\n","\tTrain Acc : 62.18% | Val. Acc : 68.74%\n","\tTrain Rec : 55.92% | Val. Rec : 62.71%\n","\tTrain Prec: 58.57% | Val. Prec: 68.67%\n","\tTrain F1  : 54.62% | Val. F1  : 63.05%\n","Epoch: 02 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.678 | Val. Loss: 0.682\n","\tTrain Acc : 69.66% | Val. Acc : 69.12%\n","\tTrain Rec : 65.94% | Val. Rec : 66.52%\n","\tTrain Prec: 68.07% | Val. Prec: 67.03%\n","\tTrain F1  : 65.11% | Val. F1  : 65.32%\n","Epoch: 03 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.603 | Val. Loss: 0.685\n","\tTrain Acc : 73.78% | Val. Acc : 69.64%\n","\tTrain Rec : 71.17% | Val. Rec : 64.48%\n","\tTrain Prec: 72.59% | Val. Prec: 67.67%\n","\tTrain F1  : 70.26% | Val. F1  : 64.40%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.908 | Val. Loss: 0.782\n","\tTrain Acc : 55.38% | Val. Acc : 64.84%\n","\tTrain Rec : 46.85% | Val. Rec : 60.60%\n","\tTrain Prec: 48.37% | Val. Prec: 61.99%\n","\tTrain F1  : 44.76% | Val. F1  : 59.66%\n","Epoch: 02 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.810 | Val. Loss: 0.804\n","\tTrain Acc : 62.58% | Val. Acc : 66.43%\n","\tTrain Rec : 56.12% | Val. Rec : 63.45%\n","\tTrain Prec: 59.47% | Val. Prec: 63.60%\n","\tTrain F1  : 55.33% | Val. F1  : 61.96%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.761 | Val. Loss: 0.728\n","\tTrain Acc : 65.55% | Val. Acc : 67.42%\n","\tTrain Rec : 60.01% | Val. Rec : 66.53%\n","\tTrain Prec: 63.31% | Val. Prec: 64.67%\n","\tTrain F1  : 59.43% | Val. F1  : 63.88%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.806 | Val. Loss: 0.723\n","\tTrain Acc : 62.04% | Val. Acc : 67.90%\n","\tTrain Rec : 56.27% | Val. Rec : 64.34%\n","\tTrain Prec: 58.18% | Val. Prec: 66.05%\n","\tTrain F1  : 54.47% | Val. F1  : 63.47%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.684 | Val. Loss: 0.679\n","\tTrain Acc : 69.12% | Val. Acc : 69.56%\n","\tTrain Rec : 65.69% | Val. Rec : 64.57%\n","\tTrain Prec: 67.85% | Val. Prec: 67.91%\n","\tTrain F1  : 64.78% | Val. F1  : 64.54%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.612 | Val. Loss: 0.707\n","\tTrain Acc : 73.36% | Val. Acc : 68.44%\n","\tTrain Rec : 70.64% | Val. Rec : 67.28%\n","\tTrain Prec: 71.93% | Val. Prec: 65.71%\n","\tTrain F1  : 69.66% | Val. F1  : 65.05%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.908 | Val. Loss: 0.787\n","\tTrain Acc : 55.07% | Val. Acc : 63.40%\n","\tTrain Rec : 46.75% | Val. Rec : 61.36%\n","\tTrain Prec: 48.06% | Val. Prec: 61.00%\n","\tTrain F1  : 44.56% | Val. F1  : 59.50%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.812 | Val. Loss: 0.729\n","\tTrain Acc : 62.42% | Val. Acc : 67.45%\n","\tTrain Rec : 56.22% | Val. Rec : 63.84%\n","\tTrain Prec: 59.49% | Val. Prec: 64.46%\n","\tTrain F1  : 55.42% | Val. F1  : 62.85%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.764 | Val. Loss: 0.735\n","\tTrain Acc : 65.38% | Val. Acc : 67.92%\n","\tTrain Rec : 60.13% | Val. Rec : 66.15%\n","\tTrain Prec: 63.23% | Val. Prec: 64.74%\n","\tTrain F1  : 59.49% | Val. F1  : 64.16%\n","Early stopping!\n","best valid loss:  0.6747388335447463\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.807 | Val. Loss: 0.705\n","\tTrain Acc : 61.94% | Val. Acc : 68.18%\n","\tTrain Rec : 55.72% | Val. Rec : 66.69%\n","\tTrain Prec: 58.93% | Val. Prec: 65.40%\n","\tTrain F1  : 54.50% | Val. F1  : 64.80%\n","Epoch: 02 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.684 | Val. Loss: 0.673\n","\tTrain Acc : 69.37% | Val. Acc : 69.77%\n","\tTrain Rec : 65.75% | Val. Rec : 66.43%\n","\tTrain Prec: 68.02% | Val. Prec: 67.87%\n","\tTrain F1  : 64.77% | Val. F1  : 65.66%\n","Epoch: 03 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.606 | Val. Loss: 0.691\n","\tTrain Acc : 73.59% | Val. Acc : 69.77%\n","\tTrain Rec : 70.72% | Val. Rec : 68.12%\n","\tTrain Prec: 72.44% | Val. Prec: 67.48%\n","\tTrain F1  : 69.94% | Val. F1  : 66.42%\n","Epoch: 04 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.530 | Val. Loss: 0.721\n","\tTrain Acc : 77.47% | Val. Acc : 69.21%\n","\tTrain Rec : 75.16% | Val. Rec : 67.26%\n","\tTrain Prec: 76.55% | Val. Prec: 66.75%\n","\tTrain F1  : 74.39% | Val. F1  : 65.74%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.900 | Val. Loss: 0.774\n","\tTrain Acc : 56.10% | Val. Acc : 64.39%\n","\tTrain Rec : 48.08% | Val. Rec : 62.10%\n","\tTrain Prec: 50.36% | Val. Prec: 61.42%\n","\tTrain F1  : 46.20% | Val. F1  : 60.23%\n","Epoch: 02 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.808 | Val. Loss: 0.718\n","\tTrain Acc : 62.46% | Val. Acc : 67.73%\n","\tTrain Rec : 56.42% | Val. Rec : 64.07%\n","\tTrain Prec: 59.42% | Val. Prec: 65.34%\n","\tTrain F1  : 55.42% | Val. F1  : 63.31%\n","Epoch: 03 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.752 | Val. Loss: 0.726\n","\tTrain Acc : 65.44% | Val. Acc : 67.90%\n","\tTrain Rec : 60.30% | Val. Rec : 64.84%\n","\tTrain Prec: 63.19% | Val. Prec: 65.32%\n","\tTrain F1  : 59.54% | Val. F1  : 63.65%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.812 | Val. Loss: 0.716\n","\tTrain Acc : 62.00% | Val. Acc : 67.38%\n","\tTrain Rec : 55.56% | Val. Rec : 61.81%\n","\tTrain Prec: 57.98% | Val. Prec: 68.22%\n","\tTrain F1  : 53.73% | Val. F1  : 61.96%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.686 | Val. Loss: 0.690\n","\tTrain Acc : 69.34% | Val. Acc : 68.84%\n","\tTrain Rec : 65.77% | Val. Rec : 65.46%\n","\tTrain Prec: 68.24% | Val. Prec: 68.04%\n","\tTrain F1  : 64.94% | Val. F1  : 64.89%\n","Epoch: 03 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.613 | Val. Loss: 0.721\n","\tTrain Acc : 73.27% | Val. Acc : 69.04%\n","\tTrain Rec : 70.34% | Val. Rec : 66.77%\n","\tTrain Prec: 72.13% | Val. Prec: 66.65%\n","\tTrain F1  : 69.50% | Val. F1  : 65.42%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.907 | Val. Loss: 0.791\n","\tTrain Acc : 55.27% | Val. Acc : 64.00%\n","\tTrain Rec : 46.79% | Val. Rec : 56.76%\n","\tTrain Prec: 48.66% | Val. Prec: 62.52%\n","\tTrain F1  : 44.68% | Val. F1  : 56.47%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.810 | Val. Loss: 0.722\n","\tTrain Acc : 62.50% | Val. Acc : 67.77%\n","\tTrain Rec : 56.27% | Val. Rec : 65.76%\n","\tTrain Prec: 60.03% | Val. Prec: 65.40%\n","\tTrain F1  : 55.35% | Val. F1  : 64.20%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.764 | Val. Loss: 0.722\n","\tTrain Acc : 65.32% | Val. Acc : 68.38%\n","\tTrain Rec : 60.00% | Val. Rec : 65.10%\n","\tTrain Prec: 63.52% | Val. Prec: 66.09%\n","\tTrain F1  : 59.32% | Val. F1  : 64.29%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.787 | Val. Loss: 0.710\n","\tTrain Acc : 63.55% | Val. Acc : 68.34%\n","\tTrain Rec : 58.31% | Val. Rec : 62.02%\n","\tTrain Prec: 60.65% | Val. Prec: 68.66%\n","\tTrain F1  : 56.80% | Val. F1  : 62.21%\n","Epoch: 02 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.647 | Val. Loss: 0.686\n","\tTrain Acc : 71.55% | Val. Acc : 68.77%\n","\tTrain Rec : 68.49% | Val. Rec : 66.44%\n","\tTrain Prec: 70.12% | Val. Prec: 66.35%\n","\tTrain F1  : 67.48% | Val. F1  : 65.02%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.552 | Val. Loss: 0.714\n","\tTrain Acc : 76.54% | Val. Acc : 69.23%\n","\tTrain Rec : 74.51% | Val. Rec : 64.56%\n","\tTrain Prec: 75.60% | Val. Prec: 67.57%\n","\tTrain F1  : 73.56% | Val. F1  : 64.43%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.880 | Val. Loss: 0.743\n","\tTrain Acc : 57.38% | Val. Acc : 66.80%\n","\tTrain Rec : 49.56% | Val. Rec : 60.65%\n","\tTrain Prec: 51.36% | Val. Prec: 64.38%\n","\tTrain F1  : 47.72% | Val. F1  : 60.35%\n","Epoch: 02 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.758 | Val. Loss: 0.717\n","\tTrain Acc : 65.65% | Val. Acc : 68.16%\n","\tTrain Rec : 61.00% | Val. Rec : 62.93%\n","\tTrain Prec: 63.78% | Val. Prec: 66.27%\n","\tTrain F1  : 60.07% | Val. F1  : 62.60%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.707 | Val. Loss: 0.700\n","\tTrain Acc : 68.88% | Val. Acc : 69.57%\n","\tTrain Rec : 64.92% | Val. Rec : 66.34%\n","\tTrain Prec: 67.26% | Val. Prec: 67.13%\n","\tTrain F1  : 64.20% | Val. F1  : 65.41%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.786 | Val. Loss: 0.713\n","\tTrain Acc : 63.41% | Val. Acc : 68.13%\n","\tTrain Rec : 58.19% | Val. Rec : 65.14%\n","\tTrain Prec: 59.97% | Val. Prec: 66.48%\n","\tTrain F1  : 56.43% | Val. F1  : 64.01%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.651 | Val. Loss: 0.697\n","\tTrain Acc : 71.21% | Val. Acc : 69.24%\n","\tTrain Rec : 68.47% | Val. Rec : 65.48%\n","\tTrain Prec: 70.14% | Val. Prec: 67.57%\n","\tTrain F1  : 67.46% | Val. F1  : 65.01%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.560 | Val. Loss: 0.728\n","\tTrain Acc : 76.09% | Val. Acc : 68.64%\n","\tTrain Rec : 73.75% | Val. Rec : 65.94%\n","\tTrain Prec: 75.04% | Val. Prec: 66.43%\n","\tTrain F1  : 72.89% | Val. F1  : 64.73%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.883 | Val. Loss: 0.725\n","\tTrain Acc : 57.16% | Val. Acc : 66.63%\n","\tTrain Rec : 49.64% | Val. Rec : 63.40%\n","\tTrain Prec: 51.28% | Val. Prec: 64.30%\n","\tTrain F1  : 47.52% | Val. F1  : 62.36%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.763 | Val. Loss: 0.727\n","\tTrain Acc : 65.54% | Val. Acc : 67.93%\n","\tTrain Rec : 60.74% | Val. Rec : 62.52%\n","\tTrain Prec: 63.58% | Val. Prec: 67.40%\n","\tTrain F1  : 59.86% | Val. F1  : 62.50%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.711 | Val. Loss: 0.693\n","\tTrain Acc : 68.24% | Val. Acc : 69.79%\n","\tTrain Rec : 63.89% | Val. Rec : 65.46%\n","\tTrain Prec: 66.68% | Val. Prec: 68.23%\n","\tTrain F1  : 63.20% | Val. F1  : 65.12%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.786 | Val. Loss: 0.691\n","\tTrain Acc : 63.11% | Val. Acc : 68.45%\n","\tTrain Rec : 57.31% | Val. Rec : 66.34%\n","\tTrain Prec: 59.80% | Val. Prec: 66.42%\n","\tTrain F1  : 55.97% | Val. F1  : 64.84%\n","Epoch: 02 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.651 | Val. Loss: 0.681\n","\tTrain Acc : 71.23% | Val. Acc : 69.06%\n","\tTrain Rec : 68.13% | Val. Rec : 64.90%\n","\tTrain Prec: 69.86% | Val. Prec: 67.17%\n","\tTrain F1  : 67.03% | Val. F1  : 64.47%\n","Epoch: 03 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.555 | Val. Loss: 0.714\n","\tTrain Acc : 76.30% | Val. Acc : 68.53%\n","\tTrain Rec : 74.25% | Val. Rec : 66.56%\n","\tTrain Prec: 75.41% | Val. Prec: 66.07%\n","\tTrain F1  : 73.32% | Val. F1  : 64.95%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.869 | Val. Loss: 0.748\n","\tTrain Acc : 58.52% | Val. Acc : 65.70%\n","\tTrain Rec : 50.92% | Val. Rec : 63.24%\n","\tTrain Prec: 53.48% | Val. Prec: 65.49%\n","\tTrain F1  : 49.18% | Val. F1  : 61.67%\n","Epoch: 02 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.757 | Val. Loss: 0.703\n","\tTrain Acc : 65.89% | Val. Acc : 69.19%\n","\tTrain Rec : 61.30% | Val. Rec : 64.05%\n","\tTrain Prec: 64.22% | Val. Prec: 67.74%\n","\tTrain F1  : 60.58% | Val. F1  : 64.05%\n","Epoch: 03 | Epoch Time: 0m 19s\n","\tTrain Loss: 0.701 | Val. Loss: 0.688\n","\tTrain Acc : 68.81% | Val. Acc : 69.13%\n","\tTrain Rec : 64.88% | Val. Rec : 63.22%\n","\tTrain Prec: 67.22% | Val. Prec: 67.59%\n","\tTrain F1  : 64.02% | Val. F1  : 63.35%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.840 | Val. Loss: 0.776\n","\tTrain Acc : 60.43% | Val. Acc : 65.28%\n","\tTrain Rec : 54.15% | Val. Rec : 64.65%\n","\tTrain Prec: 55.79% | Val. Prec: 62.87%\n","\tTrain F1  : 51.80% | Val. F1  : 62.12%\n","Epoch: 02 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.699 | Val. Loss: 0.688\n","\tTrain Acc : 68.89% | Val. Acc : 69.44%\n","\tTrain Rec : 65.17% | Val. Rec : 63.55%\n","\tTrain Prec: 67.19% | Val. Prec: 68.17%\n","\tTrain F1  : 63.96% | Val. F1  : 63.58%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.600 | Val. Loss: 0.708\n","\tTrain Acc : 74.14% | Val. Acc : 69.02%\n","\tTrain Rec : 71.51% | Val. Rec : 65.06%\n","\tTrain Prec: 73.16% | Val. Prec: 67.31%\n","\tTrain F1  : 70.61% | Val. F1  : 64.69%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.878 | Val. Loss: 0.753\n","\tTrain Acc : 57.17% | Val. Acc : 65.19%\n","\tTrain Rec : 50.11% | Val. Rec : 62.82%\n","\tTrain Prec: 52.81% | Val. Prec: 63.37%\n","\tTrain F1  : 48.13% | Val. F1  : 61.15%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.769 | Val. Loss: 0.702\n","\tTrain Acc : 65.21% | Val. Acc : 69.16%\n","\tTrain Rec : 60.10% | Val. Rec : 64.75%\n","\tTrain Prec: 63.30% | Val. Prec: 67.63%\n","\tTrain F1  : 59.37% | Val. F1  : 64.49%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.716 | Val. Loss: 0.684\n","\tTrain Acc : 68.17% | Val. Acc : 69.69%\n","\tTrain Rec : 64.02% | Val. Rec : 64.94%\n","\tTrain Prec: 66.64% | Val. Prec: 68.09%\n","\tTrain F1  : 63.22% | Val. F1  : 64.80%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 32, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.803 | Val. Loss: 0.715\n","\tTrain Acc : 62.13% | Val. Acc : 67.65%\n","\tTrain Rec : 55.91% | Val. Rec : 60.95%\n","\tTrain Prec: 58.44% | Val. Prec: 68.45%\n","\tTrain F1  : 54.43% | Val. F1  : 61.27%\n","Epoch: 02 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.677 | Val. Loss: 0.679\n","\tTrain Acc : 69.64% | Val. Acc : 69.79%\n","\tTrain Rec : 66.20% | Val. Rec : 66.88%\n","\tTrain Prec: 68.15% | Val. Prec: 67.51%\n","\tTrain F1  : 65.30% | Val. F1  : 65.86%\n","Epoch: 03 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.604 | Val. Loss: 0.683\n","\tTrain Acc : 73.85% | Val. Acc : 69.79%\n","\tTrain Rec : 71.22% | Val. Rec : 65.69%\n","\tTrain Prec: 72.86% | Val. Prec: 67.74%\n","\tTrain F1  : 70.36% | Val. F1  : 65.34%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.908 | Val. Loss: 0.806\n","\tTrain Acc : 55.25% | Val. Acc : 63.08%\n","\tTrain Rec : 46.68% | Val. Rec : 61.24%\n","\tTrain Prec: 47.52% | Val. Prec: 60.22%\n","\tTrain F1  : 44.49% | Val. F1  : 59.14%\n","Epoch: 02 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.815 | Val. Loss: 0.729\n","\tTrain Acc : 62.33% | Val. Acc : 66.90%\n","\tTrain Rec : 56.12% | Val. Rec : 65.01%\n","\tTrain Prec: 59.28% | Val. Prec: 64.09%\n","\tTrain F1  : 55.23% | Val. F1  : 63.15%\n","Epoch: 03 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.764 | Val. Loss: 0.714\n","\tTrain Acc : 64.98% | Val. Acc : 68.19%\n","\tTrain Rec : 59.63% | Val. Rec : 65.12%\n","\tTrain Prec: 63.04% | Val. Prec: 65.79%\n","\tTrain F1  : 59.11% | Val. F1  : 63.98%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.805 | Val. Loss: 0.732\n","\tTrain Acc : 62.42% | Val. Acc : 67.63%\n","\tTrain Rec : 56.37% | Val. Rec : 62.30%\n","\tTrain Prec: 57.93% | Val. Prec: 66.63%\n","\tTrain F1  : 54.60% | Val. F1  : 61.99%\n","Epoch: 02 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.685 | Val. Loss: 0.677\n","\tTrain Acc : 69.23% | Val. Acc : 69.76%\n","\tTrain Rec : 65.86% | Val. Rec : 65.90%\n","\tTrain Prec: 67.77% | Val. Prec: 67.83%\n","\tTrain F1  : 64.81% | Val. F1  : 65.45%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.609 | Val. Loss: 0.724\n","\tTrain Acc : 73.47% | Val. Acc : 68.22%\n","\tTrain Rec : 70.47% | Val. Rec : 68.07%\n","\tTrain Prec: 71.89% | Val. Prec: 65.97%\n","\tTrain F1  : 69.65% | Val. F1  : 64.98%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.915 | Val. Loss: 0.776\n","\tTrain Acc : 54.66% | Val. Acc : 63.94%\n","\tTrain Rec : 46.41% | Val. Rec : 60.81%\n","\tTrain Prec: 47.41% | Val. Prec: 62.10%\n","\tTrain F1  : 44.10% | Val. F1  : 59.41%\n","Epoch: 02 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.810 | Val. Loss: 0.772\n","\tTrain Acc : 62.44% | Val. Acc : 67.00%\n","\tTrain Rec : 56.01% | Val. Rec : 59.12%\n","\tTrain Prec: 59.10% | Val. Prec: 65.94%\n","\tTrain F1  : 55.11% | Val. F1  : 59.28%\n","Epoch: 03 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.760 | Val. Loss: 0.742\n","\tTrain Acc : 65.27% | Val. Acc : 68.44%\n","\tTrain Rec : 60.01% | Val. Rec : 65.41%\n","\tTrain Prec: 62.89% | Val. Prec: 66.20%\n","\tTrain F1  : 59.34% | Val. F1  : 64.46%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.807 | Val. Loss: 0.698\n","\tTrain Acc : 62.00% | Val. Acc : 68.65%\n","\tTrain Rec : 55.47% | Val. Rec : 65.37%\n","\tTrain Prec: 58.26% | Val. Prec: 66.38%\n","\tTrain F1  : 54.07% | Val. F1  : 64.55%\n","Epoch: 02 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.682 | Val. Loss: 0.681\n","\tTrain Acc : 69.76% | Val. Acc : 69.26%\n","\tTrain Rec : 66.09% | Val. Rec : 65.29%\n","\tTrain Prec: 68.31% | Val. Prec: 67.37%\n","\tTrain F1  : 65.19% | Val. F1  : 64.81%\n","Epoch: 03 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.602 | Val. Loss: 0.681\n","\tTrain Acc : 73.73% | Val. Acc : 69.55%\n","\tTrain Rec : 71.17% | Val. Rec : 66.34%\n","\tTrain Prec: 72.84% | Val. Prec: 67.83%\n","\tTrain F1  : 70.38% | Val. F1  : 65.60%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.901 | Val. Loss: 0.765\n","\tTrain Acc : 56.09% | Val. Acc : 64.64%\n","\tTrain Rec : 47.88% | Val. Rec : 61.80%\n","\tTrain Prec: 49.64% | Val. Prec: 62.97%\n","\tTrain F1  : 45.81% | Val. F1  : 60.35%\n","Epoch: 02 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.803 | Val. Loss: 0.732\n","\tTrain Acc : 62.47% | Val. Acc : 67.69%\n","\tTrain Rec : 56.41% | Val. Rec : 63.68%\n","\tTrain Prec: 59.47% | Val. Prec: 65.77%\n","\tTrain F1  : 55.40% | Val. F1  : 63.09%\n","Epoch: 03 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.756 | Val. Loss: 0.696\n","\tTrain Acc : 65.63% | Val. Acc : 68.88%\n","\tTrain Rec : 60.51% | Val. Rec : 64.69%\n","\tTrain Prec: 63.28% | Val. Prec: 66.40%\n","\tTrain F1  : 59.68% | Val. F1  : 64.17%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.811 | Val. Loss: 0.708\n","\tTrain Acc : 61.80% | Val. Acc : 68.10%\n","\tTrain Rec : 55.46% | Val. Rec : 63.96%\n","\tTrain Prec: 57.86% | Val. Prec: 66.24%\n","\tTrain F1  : 53.79% | Val. F1  : 63.61%\n","Epoch: 02 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.682 | Val. Loss: 0.675\n","\tTrain Acc : 69.66% | Val. Acc : 69.60%\n","\tTrain Rec : 65.93% | Val. Rec : 65.62%\n","\tTrain Prec: 68.02% | Val. Prec: 67.63%\n","\tTrain F1  : 64.94% | Val. F1  : 65.13%\n","Epoch: 03 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.607 | Val. Loss: 0.679\n","\tTrain Acc : 73.66% | Val. Acc : 69.45%\n","\tTrain Rec : 71.27% | Val. Rec : 66.97%\n","\tTrain Prec: 73.10% | Val. Prec: 67.24%\n","\tTrain F1  : 70.39% | Val. F1  : 65.83%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.909 | Val. Loss: 0.787\n","\tTrain Acc : 54.66% | Val. Acc : 64.82%\n","\tTrain Rec : 46.35% | Val. Rec : 62.44%\n","\tTrain Prec: 47.53% | Val. Prec: 61.78%\n","\tTrain F1  : 44.03% | Val. F1  : 60.71%\n","Epoch: 02 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.807 | Val. Loss: 0.743\n","\tTrain Acc : 62.79% | Val. Acc : 67.70%\n","\tTrain Rec : 56.40% | Val. Rec : 64.57%\n","\tTrain Prec: 59.97% | Val. Prec: 65.20%\n","\tTrain F1  : 55.47% | Val. F1  : 63.49%\n","Epoch: 03 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.765 | Val. Loss: 0.711\n","\tTrain Acc : 64.79% | Val. Acc : 69.24%\n","\tTrain Rec : 59.38% | Val. Rec : 63.88%\n","\tTrain Prec: 62.48% | Val. Prec: 67.61%\n","\tTrain F1  : 58.53% | Val. F1  : 63.97%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.796 | Val. Loss: 0.699\n","\tTrain Acc : 62.28% | Val. Acc : 69.20%\n","\tTrain Rec : 57.04% | Val. Rec : 64.51%\n","\tTrain Prec: 59.59% | Val. Prec: 68.28%\n","\tTrain F1  : 56.00% | Val. F1  : 65.08%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.656 | Val. Loss: 0.688\n","\tTrain Acc : 71.06% | Val. Acc : 69.57%\n","\tTrain Rec : 68.05% | Val. Rec : 63.12%\n","\tTrain Prec: 69.75% | Val. Prec: 70.28%\n","\tTrain F1  : 67.72% | Val. F1  : 64.34%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.576 | Val. Loss: 0.728\n","\tTrain Acc : 75.42% | Val. Acc : 69.33%\n","\tTrain Rec : 73.25% | Val. Rec : 64.30%\n","\tTrain Prec: 74.68% | Val. Prec: 68.87%\n","\tTrain F1  : 73.00% | Val. F1  : 65.20%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.894 | Val. Loss: 0.753\n","\tTrain Acc : 55.95% | Val. Acc : 66.43%\n","\tTrain Rec : 48.25% | Val. Rec : 62.03%\n","\tTrain Prec: 50.95% | Val. Prec: 63.65%\n","\tTrain F1  : 46.93% | Val. F1  : 61.83%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.768 | Val. Loss: 0.720\n","\tTrain Acc : 64.94% | Val. Acc : 68.28%\n","\tTrain Rec : 59.99% | Val. Rec : 62.95%\n","\tTrain Prec: 63.11% | Val. Prec: 67.15%\n","\tTrain F1  : 60.01% | Val. F1  : 63.46%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.718 | Val. Loss: 0.701\n","\tTrain Acc : 68.00% | Val. Acc : 69.20%\n","\tTrain Rec : 63.87% | Val. Rec : 64.51%\n","\tTrain Prec: 66.75% | Val. Prec: 67.69%\n","\tTrain F1  : 63.97% | Val. F1  : 65.03%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.805 | Val. Loss: 0.739\n","\tTrain Acc : 62.08% | Val. Acc : 67.87%\n","\tTrain Rec : 56.17% | Val. Rec : 60.65%\n","\tTrain Prec: 58.56% | Val. Prec: 68.24%\n","\tTrain F1  : 54.84% | Val. F1  : 61.43%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.667 | Val. Loss: 0.696\n","\tTrain Acc : 70.48% | Val. Acc : 69.34%\n","\tTrain Rec : 67.69% | Val. Rec : 63.62%\n","\tTrain Prec: 69.38% | Val. Prec: 69.19%\n","\tTrain F1  : 67.37% | Val. F1  : 64.58%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.586 | Val. Loss: 0.700\n","\tTrain Acc : 74.67% | Val. Acc : 68.84%\n","\tTrain Rec : 72.30% | Val. Rec : 67.14%\n","\tTrain Prec: 73.96% | Val. Prec: 66.71%\n","\tTrain F1  : 72.03% | Val. F1  : 66.12%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.897 | Val. Loss: 0.754\n","\tTrain Acc : 55.57% | Val. Acc : 65.27%\n","\tTrain Rec : 47.76% | Val. Rec : 55.53%\n","\tTrain Prec: 50.60% | Val. Prec: 63.25%\n","\tTrain F1  : 46.34% | Val. F1  : 54.98%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.769 | Val. Loss: 0.710\n","\tTrain Acc : 64.85% | Val. Acc : 68.37%\n","\tTrain Rec : 60.28% | Val. Rec : 66.12%\n","\tTrain Prec: 63.04% | Val. Prec: 65.72%\n","\tTrain F1  : 60.00% | Val. F1  : 65.21%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.722 | Val. Loss: 0.697\n","\tTrain Acc : 67.80% | Val. Acc : 68.98%\n","\tTrain Rec : 63.61% | Val. Rec : 65.70%\n","\tTrain Prec: 66.42% | Val. Prec: 66.83%\n","\tTrain F1  : 63.59% | Val. F1  : 65.35%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.797 | Val. Loss: 0.705\n","\tTrain Acc : 62.57% | Val. Acc : 68.97%\n","\tTrain Rec : 57.00% | Val. Rec : 65.64%\n","\tTrain Prec: 59.58% | Val. Prec: 67.17%\n","\tTrain F1  : 55.94% | Val. F1  : 65.39%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.663 | Val. Loss: 0.686\n","\tTrain Acc : 70.71% | Val. Acc : 69.14%\n","\tTrain Rec : 67.85% | Val. Rec : 63.42%\n","\tTrain Prec: 69.89% | Val. Prec: 69.64%\n","\tTrain F1  : 67.45% | Val. F1  : 64.52%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.576 | Val. Loss: 0.707\n","\tTrain Acc : 75.34% | Val. Acc : 69.18%\n","\tTrain Rec : 73.22% | Val. Rec : 65.44%\n","\tTrain Prec: 74.63% | Val. Prec: 68.42%\n","\tTrain F1  : 72.94% | Val. F1  : 65.85%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.879 | Val. Loss: 0.749\n","\tTrain Acc : 57.78% | Val. Acc : 65.83%\n","\tTrain Rec : 49.77% | Val. Rec : 57.31%\n","\tTrain Prec: 53.46% | Val. Prec: 66.99%\n","\tTrain F1  : 48.71% | Val. F1  : 57.98%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.763 | Val. Loss: 0.714\n","\tTrain Acc : 65.43% | Val. Acc : 68.45%\n","\tTrain Rec : 60.61% | Val. Rec : 63.22%\n","\tTrain Prec: 63.88% | Val. Prec: 67.84%\n","\tTrain F1  : 60.54% | Val. F1  : 63.90%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.711 | Val. Loss: 0.697\n","\tTrain Acc : 68.24% | Val. Acc : 69.45%\n","\tTrain Rec : 64.13% | Val. Rec : 64.14%\n","\tTrain Prec: 67.20% | Val. Prec: 68.72%\n","\tTrain F1  : 64.19% | Val. F1  : 64.90%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.799 | Val. Loss: 0.730\n","\tTrain Acc : 62.34% | Val. Acc : 67.72%\n","\tTrain Rec : 56.76% | Val. Rec : 59.71%\n","\tTrain Prec: 59.62% | Val. Prec: 69.46%\n","\tTrain F1  : 55.62% | Val. F1  : 60.50%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.664 | Val. Loss: 0.697\n","\tTrain Acc : 70.68% | Val. Acc : 69.70%\n","\tTrain Rec : 67.82% | Val. Rec : 64.09%\n","\tTrain Prec: 69.99% | Val. Prec: 69.84%\n","\tTrain F1  : 67.48% | Val. F1  : 65.13%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.583 | Val. Loss: 0.700\n","\tTrain Acc : 74.99% | Val. Acc : 69.48%\n","\tTrain Rec : 72.70% | Val. Rec : 64.58%\n","\tTrain Prec: 74.48% | Val. Prec: 69.57%\n","\tTrain F1  : 72.45% | Val. F1  : 65.54%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.883 | Val. Loss: 0.754\n","\tTrain Acc : 57.34% | Val. Acc : 65.79%\n","\tTrain Rec : 49.43% | Val. Rec : 60.86%\n","\tTrain Prec: 51.83% | Val. Prec: 64.54%\n","\tTrain F1  : 47.53% | Val. F1  : 61.09%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.769 | Val. Loss: 0.713\n","\tTrain Acc : 65.01% | Val. Acc : 68.78%\n","\tTrain Rec : 59.70% | Val. Rec : 63.87%\n","\tTrain Prec: 63.54% | Val. Prec: 68.28%\n","\tTrain F1  : 59.72% | Val. F1  : 64.61%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.716 | Val. Loss: 0.716\n","\tTrain Acc : 68.01% | Val. Acc : 68.43%\n","\tTrain Rec : 63.99% | Val. Rec : 62.28%\n","\tTrain Prec: 66.98% | Val. Prec: 69.13%\n","\tTrain F1  : 64.00% | Val. F1  : 63.11%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 50000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.817 | Val. Loss: 0.709\n","\tTrain Acc : 60.81% | Val. Acc : 68.52%\n","\tTrain Rec : 54.67% | Val. Rec : 63.00%\n","\tTrain Prec: 57.74% | Val. Prec: 67.48%\n","\tTrain F1  : 53.66% | Val. F1  : 63.71%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.691 | Val. Loss: 0.680\n","\tTrain Acc : 68.98% | Val. Acc : 69.21%\n","\tTrain Rec : 65.42% | Val. Rec : 64.80%\n","\tTrain Prec: 67.77% | Val. Prec: 67.60%\n","\tTrain F1  : 65.23% | Val. F1  : 65.15%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.625 | Val. Loss: 0.718\n","\tTrain Acc : 72.55% | Val. Acc : 69.57%\n","\tTrain Rec : 69.69% | Val. Rec : 64.24%\n","\tTrain Prec: 71.56% | Val. Prec: 69.55%\n","\tTrain F1  : 69.56% | Val. F1  : 65.24%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.921 | Val. Loss: 0.809\n","\tTrain Acc : 53.92% | Val. Acc : 64.10%\n","\tTrain Rec : 45.51% | Val. Rec : 60.16%\n","\tTrain Prec: 48.24% | Val. Prec: 60.81%\n","\tTrain F1  : 43.97% | Val. F1  : 59.26%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.823 | Val. Loss: 0.740\n","\tTrain Acc : 61.89% | Val. Acc : 67.35%\n","\tTrain Rec : 55.33% | Val. Rec : 63.25%\n","\tTrain Prec: 59.50% | Val. Prec: 65.09%\n","\tTrain F1  : 55.43% | Val. F1  : 63.17%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.769 | Val. Loss: 0.714\n","\tTrain Acc : 64.69% | Val. Acc : 68.50%\n","\tTrain Rec : 59.62% | Val. Rec : 64.40%\n","\tTrain Prec: 62.43% | Val. Prec: 66.78%\n","\tTrain F1  : 59.55% | Val. F1  : 64.53%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.820 | Val. Loss: 0.719\n","\tTrain Acc : 60.92% | Val. Acc : 67.46%\n","\tTrain Rec : 54.33% | Val. Rec : 59.70%\n","\tTrain Prec: 57.89% | Val. Prec: 69.26%\n","\tTrain F1  : 53.34% | Val. F1  : 60.76%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.694 | Val. Loss: 0.692\n","\tTrain Acc : 69.00% | Val. Acc : 68.79%\n","\tTrain Rec : 64.98% | Val. Rec : 62.31%\n","\tTrain Prec: 67.83% | Val. Prec: 68.15%\n","\tTrain F1  : 64.94% | Val. F1  : 63.19%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.630 | Val. Loss: 0.678\n","\tTrain Acc : 72.20% | Val. Acc : 69.63%\n","\tTrain Rec : 69.51% | Val. Rec : 65.87%\n","\tTrain Prec: 71.39% | Val. Prec: 67.84%\n","\tTrain F1  : 69.29% | Val. F1  : 66.01%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.927 | Val. Loss: 0.807\n","\tTrain Acc : 52.69% | Val. Acc : 63.81%\n","\tTrain Rec : 44.85% | Val. Rec : 61.37%\n","\tTrain Prec: 47.32% | Val. Prec: 61.93%\n","\tTrain F1  : 42.95% | Val. F1  : 60.30%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.822 | Val. Loss: 0.760\n","\tTrain Acc : 61.56% | Val. Acc : 66.04%\n","\tTrain Rec : 55.02% | Val. Rec : 65.33%\n","\tTrain Prec: 58.90% | Val. Prec: 63.23%\n","\tTrain F1  : 54.80% | Val. F1  : 63.27%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.775 | Val. Loss: 0.721\n","\tTrain Acc : 64.74% | Val. Acc : 68.17%\n","\tTrain Rec : 59.56% | Val. Rec : 63.89%\n","\tTrain Prec: 63.07% | Val. Prec: 66.02%\n","\tTrain F1  : 59.52% | Val. F1  : 64.02%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.815 | Val. Loss: 0.726\n","\tTrain Acc : 61.43% | Val. Acc : 66.80%\n","\tTrain Rec : 54.97% | Val. Rec : 61.78%\n","\tTrain Prec: 58.96% | Val. Prec: 67.84%\n","\tTrain F1  : 54.08% | Val. F1  : 62.42%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.694 | Val. Loss: 0.696\n","\tTrain Acc : 68.65% | Val. Acc : 68.64%\n","\tTrain Rec : 65.12% | Val. Rec : 61.77%\n","\tTrain Prec: 67.81% | Val. Prec: 70.23%\n","\tTrain F1  : 64.90% | Val. F1  : 63.03%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.626 | Val. Loss: 0.679\n","\tTrain Acc : 72.57% | Val. Acc : 69.42%\n","\tTrain Rec : 69.69% | Val. Rec : 65.25%\n","\tTrain Prec: 71.73% | Val. Prec: 68.43%\n","\tTrain F1  : 69.57% | Val. F1  : 65.77%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.907 | Val. Loss: 0.779\n","\tTrain Acc : 55.39% | Val. Acc : 64.45%\n","\tTrain Rec : 47.14% | Val. Rec : 55.07%\n","\tTrain Prec: 50.18% | Val. Prec: 62.77%\n","\tTrain F1  : 45.81% | Val. F1  : 55.03%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.816 | Val. Loss: 0.743\n","\tTrain Acc : 61.90% | Val. Acc : 66.58%\n","\tTrain Rec : 55.47% | Val. Rec : 60.35%\n","\tTrain Prec: 59.87% | Val. Prec: 65.76%\n","\tTrain F1  : 55.40% | Val. F1  : 61.07%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.770 | Val. Loss: 0.697\n","\tTrain Acc : 64.55% | Val. Acc : 68.68%\n","\tTrain Rec : 59.02% | Val. Rec : 62.69%\n","\tTrain Prec: 62.90% | Val. Prec: 69.00%\n","\tTrain F1  : 59.15% | Val. F1  : 63.75%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.826 | Val. Loss: 0.731\n","\tTrain Acc : 60.30% | Val. Acc : 65.92%\n","\tTrain Rec : 54.57% | Val. Rec : 57.21%\n","\tTrain Prec: 57.97% | Val. Prec: 69.82%\n","\tTrain F1  : 53.28% | Val. F1  : 57.89%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.702 | Val. Loss: 0.704\n","\tTrain Acc : 68.37% | Val. Acc : 68.77%\n","\tTrain Rec : 64.68% | Val. Rec : 61.51%\n","\tTrain Prec: 67.66% | Val. Prec: 70.75%\n","\tTrain F1  : 64.55% | Val. F1  : 62.66%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.632 | Val. Loss: 0.697\n","\tTrain Acc : 72.48% | Val. Acc : 69.25%\n","\tTrain Rec : 69.50% | Val. Rec : 62.70%\n","\tTrain Prec: 71.61% | Val. Prec: 69.60%\n","\tTrain F1  : 69.33% | Val. F1  : 63.86%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.912 | Val. Loss: 0.776\n","\tTrain Acc : 55.00% | Val. Acc : 64.26%\n","\tTrain Rec : 46.11% | Val. Rec : 57.08%\n","\tTrain Prec: 48.14% | Val. Prec: 63.93%\n","\tTrain F1  : 44.09% | Val. F1  : 57.54%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.818 | Val. Loss: 0.732\n","\tTrain Acc : 62.03% | Val. Acc : 66.94%\n","\tTrain Rec : 55.33% | Val. Rec : 60.57%\n","\tTrain Prec: 59.75% | Val. Prec: 66.48%\n","\tTrain F1  : 55.17% | Val. F1  : 61.41%\n","Epoch: 03 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.771 | Val. Loss: 0.704\n","\tTrain Acc : 64.58% | Val. Acc : 68.26%\n","\tTrain Rec : 59.22% | Val. Rec : 62.32%\n","\tTrain Prec: 63.26% | Val. Prec: 67.56%\n","\tTrain F1  : 59.40% | Val. F1  : 63.08%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', 'filepath': './crawl-300d-2M.vec', 'embedding_dim': 300}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.796 | Val. Loss: 0.699\n","\tTrain Acc : 62.43% | Val. Acc : 68.78%\n","\tTrain Rec : 56.64% | Val. Rec : 63.95%\n","\tTrain Prec: 59.40% | Val. Prec: 67.33%\n","\tTrain F1  : 55.64% | Val. F1  : 64.36%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.652 | Val. Loss: 0.691\n","\tTrain Acc : 71.29% | Val. Acc : 69.35%\n","\tTrain Rec : 68.21% | Val. Rec : 63.42%\n","\tTrain Prec: 70.33% | Val. Prec: 70.00%\n","\tTrain F1  : 68.07% | Val. F1  : 64.54%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.571 | Val. Loss: 0.732\n","\tTrain Acc : 75.47% | Val. Acc : 69.19%\n","\tTrain Rec : 73.31% | Val. Rec : 63.49%\n","\tTrain Prec: 74.83% | Val. Prec: 69.86%\n","\tTrain F1  : 73.11% | Val. F1  : 64.60%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11514255\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.892 | Val. Loss: 0.764\n","\tTrain Acc : 56.23% | Val. Acc : 65.96%\n","\tTrain Rec : 48.47% | Val. Rec : 58.03%\n","\tTrain Prec: 51.43% | Val. Prec: 65.25%\n","\tTrain F1  : 47.26% | Val. F1  : 58.36%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.768 | Val. Loss: 0.712\n","\tTrain Acc : 65.11% | Val. Acc : 68.47%\n","\tTrain Rec : 60.28% | Val. Rec : 62.69%\n","\tTrain Prec: 63.19% | Val. Prec: 67.94%\n","\tTrain F1  : 60.28% | Val. F1  : 63.46%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.719 | Val. Loss: 0.702\n","\tTrain Acc : 67.96% | Val. Acc : 69.09%\n","\tTrain Rec : 63.88% | Val. Rec : 63.57%\n","\tTrain Prec: 66.70% | Val. Prec: 69.34%\n","\tTrain F1  : 63.96% | Val. F1  : 64.44%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.797 | Val. Loss: 0.701\n","\tTrain Acc : 62.33% | Val. Acc : 68.44%\n","\tTrain Rec : 56.63% | Val. Rec : 61.36%\n","\tTrain Prec: 59.09% | Val. Prec: 68.81%\n","\tTrain F1  : 55.56% | Val. F1  : 62.33%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.658 | Val. Loss: 0.680\n","\tTrain Acc : 70.97% | Val. Acc : 69.49%\n","\tTrain Rec : 67.92% | Val. Rec : 63.75%\n","\tTrain Prec: 69.93% | Val. Prec: 69.12%\n","\tTrain F1  : 67.71% | Val. F1  : 64.68%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.577 | Val. Loss: 0.685\n","\tTrain Acc : 75.15% | Val. Acc : 69.26%\n","\tTrain Rec : 72.89% | Val. Rec : 65.97%\n","\tTrain Prec: 74.34% | Val. Prec: 67.68%\n","\tTrain F1  : 72.58% | Val. F1  : 65.97%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11909519\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","153600\n","65536\n","512\n","512\n","153600\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.895 | Val. Loss: 0.755\n","\tTrain Acc : 55.79% | Val. Acc : 65.51%\n","\tTrain Rec : 48.42% | Val. Rec : 57.09%\n","\tTrain Prec: 51.06% | Val. Prec: 64.43%\n","\tTrain F1  : 46.77% | Val. F1  : 57.38%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.774 | Val. Loss: 0.718\n","\tTrain Acc : 64.65% | Val. Acc : 68.40%\n","\tTrain Rec : 59.85% | Val. Rec : 65.20%\n","\tTrain Prec: 62.78% | Val. Prec: 66.30%\n","\tTrain F1  : 59.62% | Val. F1  : 64.93%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.719 | Val. Loss: 0.691\n","\tTrain Acc : 67.97% | Val. Acc : 69.08%\n","\tTrain Rec : 64.07% | Val. Rec : 64.82%\n","\tTrain Prec: 66.99% | Val. Prec: 67.54%\n","\tTrain F1  : 64.07% | Val. F1  : 65.17%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.789 | Val. Loss: 0.718\n","\tTrain Acc : 63.18% | Val. Acc : 68.04%\n","\tTrain Rec : 57.68% | Val. Rec : 61.98%\n","\tTrain Prec: 60.63% | Val. Prec: 68.21%\n","\tTrain F1  : 56.80% | Val. F1  : 62.77%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.658 | Val. Loss: 0.694\n","\tTrain Acc : 70.71% | Val. Acc : 69.38%\n","\tTrain Rec : 67.86% | Val. Rec : 64.26%\n","\tTrain Prec: 69.62% | Val. Prec: 69.55%\n","\tTrain F1  : 67.41% | Val. F1  : 65.17%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.575 | Val. Loss: 0.725\n","\tTrain Acc : 75.48% | Val. Acc : 69.01%\n","\tTrain Rec : 73.43% | Val. Rec : 64.81%\n","\tTrain Prec: 74.63% | Val. Prec: 68.07%\n","\tTrain F1  : 73.03% | Val. F1  : 65.27%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","13399183\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.876 | Val. Loss: 0.738\n","\tTrain Acc : 57.77% | Val. Acc : 66.32%\n","\tTrain Rec : 50.02% | Val. Rec : 58.74%\n","\tTrain Prec: 53.87% | Val. Prec: 66.69%\n","\tTrain F1  : 48.83% | Val. F1  : 59.53%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.765 | Val. Loss: 0.718\n","\tTrain Acc : 65.28% | Val. Acc : 68.03%\n","\tTrain Rec : 60.56% | Val. Rec : 60.72%\n","\tTrain Prec: 63.84% | Val. Prec: 69.07%\n","\tTrain F1  : 60.53% | Val. F1  : 61.62%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.713 | Val. Loss: 0.701\n","\tTrain Acc : 68.37% | Val. Acc : 69.08%\n","\tTrain Rec : 64.22% | Val. Rec : 62.71%\n","\tTrain Prec: 67.25% | Val. Prec: 69.58%\n","\tTrain F1  : 64.38% | Val. F1  : 63.85%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.811 | Val. Loss: 0.727\n","\tTrain Acc : 61.41% | Val. Acc : 67.70%\n","\tTrain Rec : 55.99% | Val. Rec : 60.11%\n","\tTrain Prec: 58.94% | Val. Prec: 68.01%\n","\tTrain F1  : 54.77% | Val. F1  : 60.83%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.674 | Val. Loss: 0.685\n","\tTrain Acc : 70.05% | Val. Acc : 69.16%\n","\tTrain Rec : 67.26% | Val. Rec : 63.95%\n","\tTrain Prec: 68.91% | Val. Prec: 68.65%\n","\tTrain F1  : 66.82% | Val. F1  : 64.70%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.593 | Val. Loss: 0.704\n","\tTrain Acc : 74.59% | Val. Acc : 69.42%\n","\tTrain Rec : 72.35% | Val. Rec : 65.45%\n","\tTrain Prec: 73.88% | Val. Prec: 68.36%\n","\tTrain F1  : 72.05% | Val. F1  : 65.89%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 300}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","14976143\n","RNN(\n","  (embedding): Embedding(35593, 300, padding_idx=1)\n","  (rnn): LSTM(300, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","10677900\n","307200\n","262144\n","1024\n","1024\n","307200\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.884 | Val. Loss: 0.760\n","\tTrain Acc : 57.32% | Val. Acc : 66.37%\n","\tTrain Rec : 49.11% | Val. Rec : 59.51%\n","\tTrain Prec: 52.44% | Val. Prec: 65.29%\n","\tTrain F1  : 47.47% | Val. F1  : 60.10%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.768 | Val. Loss: 0.718\n","\tTrain Acc : 65.18% | Val. Acc : 68.91%\n","\tTrain Rec : 60.20% | Val. Rec : 64.18%\n","\tTrain Prec: 63.34% | Val. Prec: 67.84%\n","\tTrain F1  : 60.14% | Val. F1  : 64.59%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.722 | Val. Loss: 0.714\n","\tTrain Acc : 67.64% | Val. Acc : 68.95%\n","\tTrain Rec : 63.42% | Val. Rec : 62.09%\n","\tTrain Prec: 66.44% | Val. Prec: 70.05%\n","\tTrain F1  : 63.46% | Val. F1  : 63.15%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","-------------------------\n","{'batch_size': 64, 'max_vocab_size': 100000, 'pretrained_embedding': {'url': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip', 'filepath': './glove.twitter.27B.200d.txt', 'embedding_dim': 200}, 'preprocessing': {'normalize': ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'], 'annotate': ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored'], 'spell_correct_elong': True, 'to_lowercase': True}}\n","Reading twitter - 1grams ...\n","Reading twitter - 2grams ...\n","Reading twitter - 1grams ...\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.818 | Val. Loss: 0.715\n","\tTrain Acc : 61.07% | Val. Acc : 68.52%\n","\tTrain Rec : 54.48% | Val. Rec : 64.88%\n","\tTrain Prec: 57.81% | Val. Prec: 66.62%\n","\tTrain F1  : 53.55% | Val. F1  : 64.84%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.693 | Val. Loss: 0.687\n","\tTrain Acc : 68.88% | Val. Acc : 69.19%\n","\tTrain Rec : 65.39% | Val. Rec : 63.20%\n","\tTrain Prec: 67.83% | Val. Prec: 68.66%\n","\tTrain F1  : 65.25% | Val. F1  : 64.14%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.625 | Val. Loss: 0.705\n","\tTrain Acc : 72.60% | Val. Acc : 69.40%\n","\tTrain Rec : 69.65% | Val. Rec : 63.99%\n","\tTrain Prec: 71.51% | Val. Prec: 68.78%\n","\tTrain F1  : 69.52% | Val. F1  : 64.92%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","7852555\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.918 | Val. Loss: 0.851\n","\tTrain Acc : 54.10% | Val. Acc : 60.77%\n","\tTrain Rec : 45.67% | Val. Rec : 57.65%\n","\tTrain Prec: 48.37% | Val. Prec: 59.99%\n","\tTrain F1  : 44.13% | Val. F1  : 55.81%\n","Epoch: 02 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.820 | Val. Loss: 0.764\n","\tTrain Acc : 61.59% | Val. Acc : 66.56%\n","\tTrain Rec : 55.27% | Val. Rec : 61.59%\n","\tTrain Prec: 59.11% | Val. Prec: 65.29%\n","\tTrain F1  : 55.28% | Val. F1  : 61.69%\n","Epoch: 03 | Epoch Time: 0m 8s\n","\tTrain Loss: 0.768 | Val. Loss: 0.741\n","\tTrain Acc : 64.74% | Val. Acc : 68.31%\n","\tTrain Rec : 59.50% | Val. Rec : 64.52%\n","\tTrain Prec: 63.28% | Val. Prec: 65.98%\n","\tTrain F1  : 59.70% | Val. F1  : 64.45%\n","Early stopping!\n","best valid loss:  0.6734540395320407\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.818 | Val. Loss: 0.722\n","\tTrain Acc : 61.12% | Val. Acc : 68.09%\n","\tTrain Rec : 54.91% | Val. Rec : 62.20%\n","\tTrain Prec: 57.92% | Val. Prec: 67.66%\n","\tTrain F1  : 53.98% | Val. F1  : 63.03%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.698 | Val. Loss: 0.698\n","\tTrain Acc : 68.43% | Val. Acc : 69.05%\n","\tTrain Rec : 64.48% | Val. Rec : 62.70%\n","\tTrain Prec: 67.28% | Val. Prec: 69.20%\n","\tTrain F1  : 64.49% | Val. F1  : 63.79%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.629 | Val. Loss: 0.670\n","\tTrain Acc : 72.38% | Val. Acc : 69.90%\n","\tTrain Rec : 69.81% | Val. Rec : 67.22%\n","\tTrain Prec: 71.59% | Val. Prec: 67.84%\n","\tTrain F1  : 69.58% | Val. F1  : 66.80%\n","Epoch: 04 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.571 | Val. Loss: 0.699\n","\tTrain Acc : 75.50% | Val. Acc : 69.15%\n","\tTrain Rec : 73.31% | Val. Rec : 65.38%\n","\tTrain Prec: 74.80% | Val. Prec: 67.70%\n","\tTrain F1  : 73.13% | Val. F1  : 65.57%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n","#########################\n","{'hidden_dim': 128, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","8247819\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 128, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=256, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","102400\n","65536\n","512\n","512\n","102400\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","131072\n","65536\n","512\n","512\n","768\n","3\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.924 | Val. Loss: 0.848\n","\tTrain Acc : 53.25% | Val. Acc : 62.70%\n","\tTrain Rec : 44.80% | Val. Rec : 58.45%\n","\tTrain Prec: 47.13% | Val. Prec: 59.42%\n","\tTrain F1  : 42.67% | Val. F1  : 57.64%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.822 | Val. Loss: 0.751\n","\tTrain Acc : 61.60% | Val. Acc : 66.46%\n","\tTrain Rec : 54.95% | Val. Rec : 64.79%\n","\tTrain Prec: 59.12% | Val. Prec: 63.57%\n","\tTrain F1  : 54.83% | Val. F1  : 63.23%\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.775 | Val. Loss: 0.737\n","\tTrain Acc : 65.00% | Val. Acc : 67.59%\n","\tTrain Rec : 59.62% | Val. Rec : 66.89%\n","\tTrain Prec: 63.27% | Val. Prec: 64.83%\n","\tTrain F1  : 59.77% | Val. F1  : 64.87%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.817 | Val. Loss: 0.722\n","\tTrain Acc : 61.26% | Val. Acc : 67.93%\n","\tTrain Rec : 54.56% | Val. Rec : 63.02%\n","\tTrain Prec: 57.66% | Val. Prec: 67.23%\n","\tTrain F1  : 53.38% | Val. F1  : 63.56%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.693 | Val. Loss: 0.685\n","\tTrain Acc : 68.99% | Val. Acc : 69.31%\n","\tTrain Rec : 65.38% | Val. Rec : 64.90%\n","\tTrain Prec: 68.00% | Val. Prec: 68.53%\n","\tTrain F1  : 65.24% | Val. F1  : 65.47%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.623 | Val. Loss: 0.704\n","\tTrain Acc : 72.41% | Val. Acc : 69.64%\n","\tTrain Rec : 69.48% | Val. Rec : 64.42%\n","\tTrain Prec: 71.41% | Val. Prec: 69.39%\n","\tTrain F1  : 69.38% | Val. F1  : 65.40%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n","#########################\n","{'hidden_dim': 256, 'n_layers': 2, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","9635083\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=2, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.908 | Val. Loss: 0.818\n","\tTrain Acc : 55.69% | Val. Acc : 60.26%\n","\tTrain Rec : 47.21% | Val. Rec : 60.82%\n","\tTrain Prec: 50.24% | Val. Prec: 61.08%\n","\tTrain F1  : 45.90% | Val. F1  : 57.67%\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.816 | Val. Loss: 0.731\n","\tTrain Acc : 62.18% | Val. Acc : 67.54%\n","\tTrain Rec : 55.89% | Val. Rec : 62.97%\n","\tTrain Prec: 60.21% | Val. Prec: 65.94%\n","\tTrain F1  : 55.82% | Val. F1  : 63.20%\n","Epoch: 03 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.767 | Val. Loss: 0.730\n","\tTrain Acc : 64.96% | Val. Acc : 66.16%\n","\tTrain Rec : 59.58% | Val. Rec : 56.36%\n","\tTrain Prec: 63.07% | Val. Prec: 68.91%\n","\tTrain F1  : 59.65% | Val. F1  : 56.67%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.5, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.5, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.819 | Val. Loss: 0.717\n","\tTrain Acc : 61.34% | Val. Acc : 67.94%\n","\tTrain Rec : 55.21% | Val. Rec : 61.32%\n","\tTrain Prec: 58.24% | Val. Prec: 67.93%\n","\tTrain F1  : 54.17% | Val. F1  : 62.22%\n","Epoch: 02 | Epoch Time: 0m 16s\n","\tTrain Loss: 0.697 | Val. Loss: 0.720\n","\tTrain Acc : 68.46% | Val. Acc : 68.30%\n","\tTrain Rec : 64.72% | Val. Rec : 60.34%\n","\tTrain Prec: 67.46% | Val. Prec: 70.05%\n","\tTrain F1  : 64.44% | Val. F1  : 61.07%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.632 | Val. Loss: 0.683\n","\tTrain Acc : 72.10% | Val. Acc : 68.93%\n","\tTrain Rec : 69.15% | Val. Rec : 67.90%\n","\tTrain Prec: 71.07% | Val. Prec: 66.53%\n","\tTrain F1  : 68.94% | Val. F1  : 66.29%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n","#########################\n","{'hidden_dim': 256, 'n_layers': 3, 'is_bidirectional': True, 'dropout': 0.8, 'n_epochs': 20, 'embedding_dim': 200}\n","<bound method Module.parameters of RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")>\n","11212043\n","RNN(\n","  (embedding): Embedding(35593, 200, padding_idx=1)\n","  (rnn): LSTM(200, 256, num_layers=3, dropout=0.8, bidirectional=True)\n","  (fully_connected): Linear(in_features=512, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.8, inplace=False)\n",")\n","7118600\n","204800\n","262144\n","1024\n","1024\n","204800\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","524288\n","262144\n","1024\n","1024\n","1536\n","3\n","Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.910 | Val. Loss: 0.787\n","\tTrain Acc : 55.57% | Val. Acc : 64.25%\n","\tTrain Rec : 46.77% | Val. Rec : 55.52%\n","\tTrain Prec: 49.98% | Val. Prec: 62.52%\n","\tTrain F1  : 45.20% | Val. F1  : 55.60%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.817 | Val. Loss: 0.760\n","\tTrain Acc : 61.91% | Val. Acc : 65.78%\n","\tTrain Rec : 55.48% | Val. Rec : 55.38%\n","\tTrain Prec: 60.09% | Val. Prec: 65.84%\n","\tTrain F1  : 55.27% | Val. F1  : 55.13%\n","Epoch: 03 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.773 | Val. Loss: 0.718\n","\tTrain Acc : 64.60% | Val. Acc : 68.27%\n","\tTrain Rec : 59.30% | Val. Rec : 65.64%\n","\tTrain Prec: 62.92% | Val. Prec: 66.06%\n","\tTrain F1  : 59.27% | Val. F1  : 64.93%\n","Early stopping!\n","best valid loss:  0.6695021189089063\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdHaQTiSxsrH","colab_type":"code","outputId":"4491f9d9-0f1c-4986-8a0f-dde092b0763d","executionInfo":{"status":"ok","timestamp":1588587554333,"user_tz":-60,"elapsed":4874494,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"source":["def load_checkpoint(cfg, filepath):\n","  # load the model and params\n","  checkpoint = torch.load(filepath)\n","\n","  model = checkpoint['model']\n","  model.load_state_dict(checkpoint['state_dict'])\n","\n","\t# save the Field(s)\n","  with open(cfg['paths']['fields']['row_num'],\"rb\")as f:\n","      ROW_NUM = dill.load(f)\n","  with open(cfg['paths']['fields']['text'],\"rb\")as f:\n","      TEXT = dill.load(f)\n","  with open(cfg['paths']['fields']['sentiment'],\"rb\")as f:\n","      SENTIMENT = dill.load(f)\n","  fields = [('row_num', ROW_NUM), ('text', TEXT), ('sentiment', SENTIMENT)]\n","\n","  for parameter in model.parameters():\n","    parameter.requires_grad = False\n","  return model, test_iterator, fields\n","\n","\n","import seaborn as sn\n","\n","def generate_confusion_matrix(Y_true, Y_pred, labels, normalize=None):\n","  cm = confusion_matrix(Y_true, Y_pred, normalize=normalize)\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111)\n","  cax = ax.matshow(cm, cmap=plt.cm.get_cmap('Blues', 6))\n","  fig.colorbar(cax)\n","  ax.set_xticklabels([''] + labels, rotation=45)\n","  ax.set_yticklabels([''] + labels)\n","  plt.xlabel('Predicted')\n","  plt.ylabel('True')\n","  plt.show()\n","  \n","def execute_testing_process(model_filepath, test_iterator):\n","  # load the best model architecture, params and weights gained from training process\n","  model, test_iterator, fields = load_checkpoint(cfg, model_filepath)\n","  criterion = nn.CrossEntropyLoss()\n","\n","  test_loss, test_acc, test_rec, test_prec, test_f1, test_Y_true, test_Y_pred = evaluate(model, test_iterator, criterion)\n","  print(f'Test Loss: {test_loss:.3f}')\n","  print(f'Test Acc : {test_acc*100:.2f}%')\n","  print(f'Test Rec : {test_rec*100:.2f}%')\n","  print(f'Test Prec: {test_prec*100:.2f}%')\n","  print(f'Test F1  : {test_f1*100:.2f}%')\n","\n","  num_classes = len(set(test_Y_true))\n","  print(num_classes)\n","  labels = []\n","  for i in range(num_classes):\n","    labels.append(SENTIMENT.vocab.itos[i])\n","    \n","  generate_confusion_matrix(test_Y_true, test_Y_pred, labels, normalize='true')\n","\n","test_iterator = build_torch_dataset(test_csv_filepath, fields, vocab_params, is_training_data=False)\n","execute_testing_process(saved_model_filepath, test_iterator)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["Test Loss: 0.759\n","Test Acc : 65.20%\n","Test Rec : 65.30%\n","Test Prec: 62.61%\n","Test F1  : 62.72%\n","3\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}